<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>pytorch学习 | Roger-Lv's space</title><meta name="author" content="Roger-Lv"><meta name="copyright" content="Roger-Lv"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch学习 简单例子（包含数据集加载、训练、模型保存和测试） 一个基本的例子：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;508721527 加载数据 12torchvision.datasets.CIFAR10torch.utils.data.DataLoader() 训练网络 123456789101112131415161718192021222324252627282">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch学习">
<meta property="og:url" content="http://example.com/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Roger-Lv&#39;s space">
<meta property="og:description" content="pytorch学习 简单例子（包含数据集加载、训练、模型保存和测试） 一个基本的例子：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;508721527 加载数据 12torchvision.datasets.CIFAR10torch.utils.data.DataLoader() 训练网络 123456789101112131415161718192021222324252627282">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cover/pytorch.webp">
<meta property="article:published_time" content="2025-12-18T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-19T10:03:53.202Z">
<meta property="article:author" content="Roger-Lv">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cover/pytorch.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "pytorch学习",
  "url": "http://example.com/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/",
  "image": "http://example.com/img/cover/pytorch.webp",
  "datePublished": "2025-12-18T16:00:00.000Z",
  "dateModified": "2025-12-19T10:03:53.202Z",
  "author": [
    {
      "@type": "Person",
      "name": "Roger-Lv",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="http://example.com/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.4.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">150</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">49</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Roger-Lv's space</span></a><a class="nav-page-title" href="/"><span class="site-name">pytorch学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div><!-- 添加搜索按钮 ↓--><span class="search-button"><i class="fas fa-search" aria-hidden="true"></i></span></div></nav><div id="post-info"><h1 class="post-title">pytorch学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-18T16:00:00.000Z" title="发表于 2025-12-19 00:00:00">2025-12-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-19T10:03:53.202Z" title="更新于 2025-12-19 18:03:53">2025-12-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/llm/">llm</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/" data-flag-title="pytorch学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>pytorch学习</h1>
<h2 id="简单例子（包含数据集加载、训练、模型保存和测试）">简单例子（包含数据集加载、训练、模型保存和测试）</h2>
<p>一个基本的例子：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/508721527">https://zhuanlan.zhihu.com/p/508721527</a></p>
<h3 id="加载数据">加载数据</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torchvision.datasets.CIFAR10</span><br><span class="line">torch.utils.data.DataLoader()</span><br></pre></td></tr></table></figure>
<h3 id="训练网络">训练网络</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层卷积：输入通道3（RGB图像），输出通道6，卷积核大小5×5</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 池化层：2×2最大池化，步长为2</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层卷积：输入通道6，输出通道16，卷积核大小5×5</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层（线性层）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  <span class="comment"># 输入维度400，输出120</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)         <span class="comment"># 输入120，输出84</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)          <span class="comment"># 输入84，输出10（对应10个类别）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))  <span class="comment">#relu是一种常见的激活函数之一</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x))) <span class="comment">#卷积之后池化，然后拉平的一维向量传递给线性全连接层</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 这样一个网络就定义好了</span></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<p>这里的维度变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">输入: [batch_size, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]  <span class="comment"># 3通道RGB图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1后</span></span><br><span class="line">conv1: [batch_size, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>]  <span class="comment"># (32-5+1)=28，6个通道</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pool1后</span></span><br><span class="line">pool1: [batch_size, <span class="number">6</span>, <span class="number">14</span>, <span class="number">14</span>]  <span class="comment"># 2×2池化，尺寸减半</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># conv2后</span></span><br><span class="line">conv2: [batch_size, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>]  <span class="comment"># (14-5+1)=10，16个通道</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pool2后</span></span><br><span class="line">pool2: [batch_size, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>]    <span class="comment"># 2×2池化，尺寸减半</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展平后</span></span><br><span class="line">flatten: [batch_size, <span class="number">16</span>×<span class="number">5</span>×<span class="number">5</span>=<span class="number">400</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">fc1: [batch_size, <span class="number">120</span>]</span><br><span class="line">fc2: [batch_size, <span class="number">84</span>]</span><br><span class="line">fc3: [batch_size, <span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<h3 id="训练">训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开训</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">  	runnning_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader,<span class="number">0</span>):</span><br><span class="line">      	<span class="built_in">input</span>,labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = net(<span class="built_in">input</span>)</span><br><span class="line">        loss =criterion(output,labels)</span><br><span class="line">        optimizer.backward()</span><br><span class="line">        optimizer.step() <span class="comment"># 优化器优化，反向传播后直接更新参数</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">1999</span>:</span><br><span class="line">          	<span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">           	running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;finish&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="保存模型">保存模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;./model.path&#x27;</span></span><br><span class="line">torch.save(net.state_dict(),PATH)</span><br></pre></td></tr></table></figure>
<h3 id="测试">测试</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testdataloader = torch.utils.data.Dataloader(testset,batch_size=<span class="number">4</span>,shuffle=<span class="literal">False</span>)</span><br><span class="line">dataiter = <span class="built_in">iter</span>(testdataloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="分布式训练-torch-distributed">分布式训练:torch.distributed</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1982568110990067100">https://zhuanlan.zhihu.com/p/1982568110990067100</a></p>
<p><strong>torch.distributed</strong></p>
<p>是 PyTorch 生态系统中分布式训练的核心模块，为现代深度学习提供了强大的多进程、多节点并行计算能力</p>
<h3 id="分布式进程间通信"><strong>分布式进程间通信</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.all_reduces() <span class="comment"># 集合通信</span></span><br><span class="line">torch.distributed.send() <span class="comment"># 点对点通信：发送</span></span><br><span class="line">torch.dsitributed.recv() <span class="comment"># 点对点通信：接收</span></span><br></pre></td></tr></table></figure>
<h3 id="分布式多进程"><strong>分布式多进程</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>) // NCCL后端(GPU)</span><br><span class="line">torch.distributed.init_process_group(backend= <span class="string">&#x27;gloo&#x27;</span>) // gloohouduan (CPU/GPU)</span><br><span class="line">torch.distributed.init_process_group(backend= <span class="string">&#x27;mpi&#x27;</span>) // MPI后端（HPC集群）</span><br><span class="line"><span class="comment"># 检查可用后端</span></span><br><span class="line"><span class="built_in">print</span>(torch.distributed.is_nccl_available())  <span class="comment"># 检查NCCL</span></span><br><span class="line"><span class="built_in">print</span>(torch.distributed.is_gloo_available())  <span class="comment"># 检查Gloo</span></span><br><span class="line"><span class="built_in">print</span>(torch.distributed.is_mpi_available())   <span class="comment"># 检查MPI</span></span><br></pre></td></tr></table></figure>
<ol>
<li>NCCL (NVIDIA Collective Communications Library)</li>
</ol>
<p><strong>—— GPU 训练的绝对首选</strong></p>
<p>NCCL 是 NVIDIA 专门为自家 GPU 开发的高性能通信库。</p>
<ul>
<li><strong>硬件支持：</strong> 仅限 NVIDIA GPU。</li>
<li><strong>传输效率：</strong> 极高。它能感知拓扑结构（如 PCIe, NVLink），并自动选择最快的路径。在多机多卡环境下，它能充分利用 <strong>InfiniBand (IB)</strong> 网络。</li>
<li><strong>通信模式：</strong> 支持所有常见的集体通信算子（All-Reduce, All-Gather, Broadcast 等）。</li>
<li><strong>缺点：</strong> 不支持 CPU 之间的通信；报错信息相对晦涩（通常显示为简单的 <code>NCCL Error</code>）。</li>
</ul>
<hr>
<ol start="2">
<li>Gloo</li>
</ol>
<p><strong>—— 兼容性之王，CPU 训练的首选</strong></p>
<p>Gloo 是由 Facebook 开发的跨平台通信库。</p>
<ul>
<li><strong>硬件支持：</strong> 同时支持 CPU 和 GPU（但在 GPU 上比 NCCL 慢得多）。</li>
<li><strong>传输效率：</strong> 中等。它主要基于 TCP 协议进行网络传输，没有针对 GPU 硬件链路（如 NVLink）做极致优化。</li>
<li><strong>优势：</strong> * <strong>极其稳定：</strong> 很少出现超时崩溃，是调试分布式代码时的“安全网”。
<ul>
<li><strong>全能型：</strong> 如果你的模型部分在 CPU 运行，部分在 GPU 运行，Gloo 是唯一的选择。</li>
</ul>
</li>
<li><strong>应用场景：</strong> CPU 集群训练、小规模测试、或者网络环境不支持 RDMA/IB 的普通以太网环境。</li>
</ul>
<hr>
<ol start="3">
<li>MPI (Message Passing Interface)</li>
</ol>
<p><strong>—— 高性能计算（HPC）的遗产</strong></p>
<p>MPI 是分布式计算领域的老牌标准，在深度学习兴起前就已统治学术界和超级计算机多年。</p>
<ul>
<li><strong>硬件支持：</strong> 取决于具体的 MPI 实现（如 OpenMPI, Intel MPI）。</li>
<li><strong>使用门槛：</strong> 比较复杂。在 PyTorch 中使用 MPI 后端，通常需要你手动编译支持 MPI 的 PyTorch 版本，且必须在系统层安装相应的库。</li>
<li><strong>优势：</strong> 在传统的大规模超算集群上，MPI 拥有极佳的作业调度和容错能力。</li>
<li><strong>现状：</strong> 除非你的训练任务运行在特定的国家实验室超算中心，或者有非常特殊的集群管理需求，否则<strong>不推荐</strong>在现代深度学习任务中使用它（因为 NCCL 已经足够强大）。</li>
</ul>
<h3 id="分布式数据并行">分布式数据并行</h3>
<p>DDP FSDP等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.parallel.DistributedDataParallel - DDP类</span><br><span class="line">torch.distributed.fsdp.FullyShardedDataParallel - FSDP类</span><br><span class="line">torch.utils.data.distributed.DistributedSampler - 分布式采样器</span><br></pre></td></tr></table></figure>
<h4 id="1-torch-nn-parallel-DistributedDataParallel-DDP">1. <strong>torch.nn.parallel.DistributedDataParallel (DDP)</strong></h4>
<p><strong>作用</strong></p>
<p>数据并行训练的主要实现，将模型复制到每个GPU上，通过梯度同步实现并行训练。</p>
<p><strong>核心原理</strong></p>
<ol>
<li>每个GPU有独立的模型副本</li>
<li>前向传播时，每个GPU处理不同的数据</li>
<li>反向传播时，通过All-Reduce操作同步梯度</li>
<li>每个GPU使用同步后的梯度更新模型</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例代码</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化分布式环境&quot;&quot;&quot;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;12355&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化进程组</span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;nccl&quot;</span>, rank=rank, world_size=world_size)</span><br><span class="line">    torch.cuda.set_device(rank)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cleanup</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;清理分布式环境&quot;&quot;&quot;</span></span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ddp</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;DDP训练函数&quot;&quot;&quot;</span></span><br><span class="line">    setup(rank, world_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 创建模型并移动到当前GPU</span></span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">    ).to(rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 用DDP包装模型</span></span><br><span class="line">    ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 准备数据</span></span><br><span class="line">    <span class="comment"># 假设每个GPU的batch_size=32</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    inputs = torch.randn(batch_size, <span class="number">10</span>).to(rank)</span><br><span class="line">    labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (batch_size,)).to(rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 定义损失函数和优化器</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = ddp_model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()  <span class="comment"># DDP自动同步梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数（所有GPU参数保持同步）</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> rank == <span class="number">0</span>:  <span class="comment"># 只在主进程打印</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    cleanup()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动DDP训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    world_size = torch.cuda.device_count()  <span class="comment"># GPU数量</span></span><br><span class="line">    mp.spawn(train_ddp, args=(world_size,), nprocs=world_size, join=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-torch-distributed-fsdp-FullyShardedDataParallel-FSDP"><strong>2. torch.distributed.fsdp.FullyShardedDataParallel (FSDP)</strong></h4>
<p><strong>作用</strong></p>
<p><strong>全分片数据并行</strong>，更高级的并行策略，可以训练超大模型（如数十亿参数）。</p>
<p>与DDP<strong>的区别</strong>作用</p>
<p><strong>全分片数据并行</strong>，更高级的并行策略，可以训练超大模型（如数十亿参数）。</p>
<p><strong>与DDP的区别</strong></p>
<ul>
<li>
<p><strong>DDP</strong>: 每个GPU存储完整的模型副本</p>
</li>
<li>
<p><strong>FSDP</strong>: 每个GPU只存储模型的一部分，内存效率更高</p>
</li>
<li>
<p><strong>DDP</strong>: 每个GPU存储完整的模型副本</p>
</li>
<li>
<p><strong>FSDP</strong>: 每个GPU只存储模型的一部分，内存效率更高</p>
</li>
</ul>
<p><strong>既然只存了一部分，计算时怎么办？</strong></p>
<p>这是 FSDP 最巧妙的地方。它采用了一种**“按需索取，用完即丢”**的策略。</p>
<p>当模型进行前向传播（Forward Pass）到某一层时：</p>
<ol>
<li><strong>收集 (All-Gather)：</strong> 该层所属的 GPU 会向其他 GPU 广播自己负责的那部分参数，使得所有 GPU 在那一瞬间都拥有了该层的完整参数。</li>
<li><strong>计算：</strong> 每张 GPU 用完整参数处理自己那份 Batch 数据。</li>
<li><strong>释放 (Discard)：</strong> 计算完成后，GPU 立即丢弃掉从别人那里拿来的参数，只保留自己负责的那 1/8 原始分片。</li>
</ol>
<p>**反向传播（Backward Pass）**也是同样的逻辑：计算完梯度并同步后，非自己负责的梯度立即释放。</p>
<ul>
<li><strong>优点（鱼）：</strong> 极大地降低了单卡的显存门槛。它让你可以像写普通数据并行代码一样，训练超出单卡容量的大模型。</li>
<li><strong>代价（熊掌）：</strong> 通信开销增加。因为在每一层计算前都要进行 <code>All-Gather</code> 通信，如果网络带宽（如没有 IB 网络）不够快，训练速度会变慢。</li>
</ul>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line"><span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> MixedPrecision</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> ShardingStrategy</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_fsdp</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化FSDP环境&quot;&quot;&quot;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;12355&#x27;</span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;nccl&quot;</span>, rank=rank, world_size=world_size)</span><br><span class="line">    torch.cuda.set_device(rank)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_fsdp</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;FSDP训练函数 - 适合超大模型&quot;&quot;&quot;</span></span><br><span class="line">    setup_fsdp(rank, world_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 创建超大模型（这里用简化示例）</span></span><br><span class="line">    <span class="comment"># 实际中可能是包含数亿参数的Transformer</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">LargeModel</span>(nn.Module):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="built_in">super</span>().__init__()</span><br><span class="line">            <span class="variable language_">self</span>.layer1 = nn.Linear(<span class="number">1024</span>, <span class="number">4096</span>)  <span class="comment"># 大矩阵</span></span><br><span class="line">            <span class="variable language_">self</span>.layer2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">            <span class="variable language_">self</span>.layer3 = nn.Linear(<span class="number">4096</span>, <span class="number">1024</span>)</span><br><span class="line">            <span class="variable language_">self</span>.layer4 = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">            x = <span class="variable language_">self</span>.layer1(x)</span><br><span class="line">            x = <span class="variable language_">self</span>.layer2(x)</span><br><span class="line">            x = <span class="variable language_">self</span>.layer3(x)</span><br><span class="line">            x = <span class="variable language_">self</span>.layer4(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 初始化模型</span></span><br><span class="line">    model = LargeModel().to(rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 配置FSDP（多种分片策略可选）</span></span><br><span class="line">    sharding_strategy = ShardingStrategy.FULL_SHARD  <span class="comment"># 完全分片</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 混合精度配置</span></span><br><span class="line">    mp_policy = MixedPrecision(</span><br><span class="line">        param_dtype=torch.float16,  <span class="comment"># 参数使用半精度</span></span><br><span class="line">        reduce_dtype=torch.float16,  <span class="comment"># 梯度归约使用半精度</span></span><br><span class="line">        buffer_dtype=torch.float32,  <span class="comment"># 缓冲区使用全精度</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 用FSDP包装模型</span></span><br><span class="line">    fsdp_model = FSDP(</span><br><span class="line">        model,</span><br><span class="line">        sharding_strategy=sharding_strategy,</span><br><span class="line">        mixed_precision=mp_policy,</span><br><span class="line">        device_id=rank,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 训练逻辑</span></span><br><span class="line">    optimizer = torch.optim.Adam(fsdp_model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 生成数据</span></span><br><span class="line">        inputs = torch.randn(<span class="number">16</span>, <span class="number">1024</span>).to(rank)  <span class="comment"># 较小的batch_size</span></span><br><span class="line">        labels = torch.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">16</span>,)).to(rank)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        outputs = fsdp_model(inputs)</span><br><span class="line">        loss = nn.functional.cross_entropy(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 梯度会在FSDP内部自动同步和分片</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> rank == <span class="number">0</span> <span class="keyword">and</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Step <span class="subst">&#123;step&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动FSDP训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    world_size = <span class="number">4</span>  <span class="comment"># 假设有4个GPU</span></span><br><span class="line">    mp.spawn(train_fsdp, args=(world_size,), nprocs=world_size, join=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-torch-utils-data-distributed-DistributedSampler"><strong>3. torch.utils.data.distributed.DistributedSampler</strong></h4>
<p><strong>作用</strong></p>
<p><strong>分布式采样器</strong>，确保每个GPU/进程获得不同的数据子集，避免数据重复。</p>
<p><strong>工作原理</strong></p>
<ol>
<li>将完整数据集划分为多个子集</li>
<li>每个进程获得不同的子集</li>
<li>每个epoch打乱数据，但保持进程间数据不重叠</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建自定义数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = torch.randn(size, <span class="number">10</span>)</span><br><span class="line">        <span class="variable language_">self</span>.labels = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (size,))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx], <span class="variable language_">self</span>.labels[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_sampler</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;初始化分布式环境&quot;&quot;&quot;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;12355&#x27;</span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;nccl&quot;</span>, rank=rank, world_size=world_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_with_sampler</span>(<span class="params">rank, world_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用DistributedSampler的训练函数&quot;&quot;&quot;</span></span><br><span class="line">    setup_sampler(rank, world_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 创建数据集</span></span><br><span class="line">    dataset = CustomDataset(size=<span class="number">1000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 创建分布式采样器</span></span><br><span class="line">    sampler = DistributedSampler(</span><br><span class="line">        dataset,</span><br><span class="line">        num_replicas=world_size,  <span class="comment"># 进程总数</span></span><br><span class="line">        rank=rank,                <span class="comment"># 当前进程排名</span></span><br><span class="line">        shuffle=<span class="literal">True</span>,             <span class="comment"># 是否打乱数据</span></span><br><span class="line">        seed=<span class="number">42</span>                   <span class="comment"># 随机种子，确保可复现</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 创建数据加载器</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        sampler=sampler,         <span class="comment"># 使用分布式采样器</span></span><br><span class="line">        num_workers=<span class="number">2</span>,           <span class="comment"># 数据加载工作进程数</span></span><br><span class="line">        pin_memory=<span class="literal">True</span>,         <span class="comment"># 启用内存锁页，加速GPU传输</span></span><br><span class="line">        drop_last=<span class="literal">True</span>           <span class="comment"># 丢弃最后一个不完整的batch</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 验证采样效果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;rank&#125;</span>: Sampler has <span class="subst">&#123;<span class="built_in">len</span>(sampler)&#125;</span> samples&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="comment"># 重要：每个epoch开始前设置epoch</span></span><br><span class="line">        sampler.set_epoch(epoch)</span><br><span class="line">        </span><br><span class="line">        total_samples = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            total_samples += <span class="built_in">len</span>(data)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 这里应该是实际训练代码</span></span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> batch_idx % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;rank&#125;</span>, Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;batch_idx&#125;</span>: &quot;</span></span><br><span class="line">                      <span class="string">f&quot;Sample indices range: <span class="subst">&#123;batch_idx*batch_size&#125;</span> to &quot;</span></span><br><span class="line">                      <span class="string">f&quot;<span class="subst">&#123;(batch_idx+<span class="number">1</span>)*batch_size-<span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;rank&#125;</span>, Epoch <span class="subst">&#123;epoch&#125;</span> processed <span class="subst">&#123;total_samples&#125;</span> samples&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compare_samplers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对比普通Sampler和DistributedSampler的区别&quot;&quot;&quot;</span></span><br><span class="line">    dataset = CustomDataset(size=<span class="number">100</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 普通数据加载（单GPU）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=== 普通Sampler（单GPU）===&quot;</span>)</span><br><span class="line">    regular_loader = DataLoader(dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    all_indices = []</span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> regular_loader:</span><br><span class="line">        all_indices.extend(<span class="built_in">range</span>(<span class="built_in">len</span>(all_indices)*<span class="number">10</span>, <span class="built_in">len</span>(all_indices)*<span class="number">10</span> + <span class="built_in">len</span>(data)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;总样本数: <span class="subst">&#123;<span class="built_in">len</span>(all_indices)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;样本索引: <span class="subst">&#123;all_indices[:<span class="number">20</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分布式采样器模拟</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== DistributedSampler（4个GPU）===&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        sampler = DistributedSampler(</span><br><span class="line">            dataset, </span><br><span class="line">            num_replicas=<span class="number">4</span>, </span><br><span class="line">            rank=rank, </span><br><span class="line">            shuffle=<span class="literal">True</span>, </span><br><span class="line">            seed=<span class="number">42</span></span><br><span class="line">        )</span><br><span class="line">        indices = <span class="built_in">list</span>(sampler)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;GPU <span class="subst">&#123;rank&#125;</span> 获得 <span class="subst">&#123;<span class="built_in">len</span>(indices)&#125;</span> 个样本&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;  前10个索引: <span class="subst">&#123;indices[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;  这些索引来自: <span class="subst">&#123;[dataset.labels[i].item() <span class="keyword">for</span> i <span class="keyword">in</span> indices[:<span class="number">10</span>]]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动分布式训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    world_size = <span class="number">4</span></span><br><span class="line">    mp.spawn(train_with_sampler, args=(world_size,), nprocs=world_size, join=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对比演示</span></span><br><span class="line">    compare_samplers()</span><br></pre></td></tr></table></figure>
<h3 id="自动梯度同步"><strong>自动梯度同步</strong></h3>
<p>参数和梯度的分布式同步</p>
<ul>
<li>集成在DDP/FSDP中的自动梯度同步机制</li>
</ul>
<h3 id="核心概念">核心概念</h3>
<h4 id="进程组（Process-Group）">进程组（Process Group）</h4>
<p><strong>定义</strong>：一组可以相互通信的进程，是分布式通信的基本单位。</p>
<p><strong>相关API</strong>：</p>
<ul>
<li><code>torch.distributed.group.WORLD</code> - 默认全局进程组</li>
<li><code>torch.distributed.new_group(ranks)</code> - 创建自定义进程组</li>
</ul>
<p><strong>举例</strong>：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 默认全局进程组，包含所有进程</span><br><span class="line">global_group = torch.distributed.group.WORLD</span><br><span class="line"></span><br><span class="line"># 自定义进程组，只包含rank 0,1,2</span><br><span class="line">custom_group = torch.distributed.new_group(ranks=[0, 1, 2])</span><br></pre></td></tr></table></figure>
<h4 id="rank（进程标识符）">rank（进程标识符）</h4>
<p><strong>定义</strong>：进程在进程组中的唯一标识符，从0开始编号。</p>
<p><strong>相关API</strong>：</p>
<ul>
<li><code>torch.distributed.get_rank()</code> - 获取当前进程的global_rank</li>
</ul>
<p><strong>分类</strong>：</p>
<ul>
<li><strong>global_rank</strong>：全局进程组中的rank，范围为0到world_size-1</li>
<li><strong>local_rank</strong>：单节点内的本地rank，通常对应GPU编号</li>
</ul>
<p><strong>举例</strong>：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 单机4卡训练场景</span><br><span class="line"># 节点0: 4个进程，local_rank = 0,1,2,3</span><br><span class="line"># 节点1: 4个进程，local_rank = 0,1,2,3</span><br><span class="line"></span><br><span class="line"># global_rank 计算公式:</span><br><span class="line"># global_rank = node_rank * local_world_size + local_rank</span><br><span class="line"></span><br><span class="line"># 示例：2节点，每节点4卡</span><br><span class="line"># 节点0上的GPU0: local_rank=0, global_rank=0</span><br><span class="line"># 节点0上的GPU1: local_rank=1, global_rank=1</span><br><span class="line"># 节点0上的GPU2: local_rank=2, global_rank=2</span><br><span class="line"># 节点0上的GPU3: local_rank=3, global_rank=3</span><br><span class="line"># 节点1上的GPU0: local_rank=0, global_rank=4</span><br><span class="line"># 节点1上的GPU1: local_rank=1, global_rank=5</span><br><span class="line"># 节点1上的GPU2: local_rank=2, global_rank=6</span><br><span class="line"># 节点1上的GPU3: local_rank=3, global_rank=7</span><br><span class="line"></span><br><span class="line">rank = torch.distributed.get_rank()  # 获取global_rank</span><br><span class="line">local_rank = int(os.environ.get(&#x27;LOCAL_RANK&#x27;, 0))  # 本地rank</span><br></pre></td></tr></table></figure>
<h4 id="world-size（进程总数）">world_size（进程总数）</h4>
<p><strong>定义</strong>：进程组中的总进程数。</p>
<p><strong>相关API</strong>：</p>
<ul>
<li><code>torch.distributed.get_world_size()</code> - 获取进程总数</li>
</ul>
<p><strong>举例</strong>：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 单机4GPU训练</span><br><span class="line">world_size = 4  # 总共4个进程</span><br><span class="line"></span><br><span class="line"># 多机训练：2节点 x 4GPU = 8进程</span><br><span class="line">world_size = 8  # 总共8个进程</span><br><span class="line"></span><br><span class="line">world_size = torch.distributed.get_world_size()</span><br></pre></td></tr></table></figure>
<h3 id="初始化和进程组管理">初始化和进程组管理</h3>
<h4 id="初始化分布式环境">初始化分布式环境</h4>
<p>torch.distributed 的初始化是分布式训练的第一步，主要通过 <code>torch.distributed.init_process_group()</code> 函数完成。</p>
<p><strong>基本初始化 API</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(</span><br><span class="line">    backend=&#x27;nccl&#x27;,           # 通信后端</span><br><span class="line">    init_method=&#x27;env://&#x27;,     # 初始化方法</span><br><span class="line">    world_size=4,             # 总进程数</span><br><span class="line">    rank=0,                   # 当前进程的rank</span><br><span class="line">    timeout=datetime.timedelta(seconds=30)  # 超时时间</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>初始化方法</strong></p>
<ol>
<li>
<p><strong>环境变量 (env://)</strong> - 最常用</p>
<ul>
<li>
<p>通过环境变量传递配置信息</p>
</li>
<li>
<p>自动检测 MASTER_ADDR, MASTER_PORT, RANK, WORLD_SIZE</p>
</li>
</ul>
</li>
<li>
<p><strong>文件系统 (file://)</strong> - 开发环境</p>
<ul>
<li>
<p>使用共享文件系统进行协调</p>
</li>
<li>
<p>所有进程写入同一个文件</p>
</li>
</ul>
</li>
<li>
<p><strong>TCP (tcp://)</strong> - 自定义</p>
<ul>
<li>
<p>直接指定主节点地址和端口</p>
</li>
<li>
<p>格式：tcp://MASTER_ADDR:MASTER_PORT</p>
</li>
</ul>
</li>
</ol>
<p><strong>📍 为什么需要地址？</strong></p>
<p><code>init_method</code> 中的地址是分布式训练的<strong>协调中心</strong>，解决了多进程发现和同步的关键问题：</p>
<p><strong>核心作用</strong>：</p>
<ul>
<li><strong>进程发现</strong>：告诉每个进程如何找到其他进程</li>
<li><strong>信息交换</strong>：让进程们交换 rank、world_size 等配置信息</li>
<li><strong>同步启动</strong>：确保所有进程同时开始训练</li>
</ul>
<p><strong>为什么需要地址？</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">机器A (进程0,1) ──┐</span><br><span class="line">                    ├───&gt; 集合点 (MASTER_ADDR:MASTER_PORT)</span><br><span class="line">机器B (进程2,3) ──┘</span><br></pre></td></tr></table></figure>
<p>分布式进程可能在<strong>不同机器上运行</strong>，需要一个**共同的 rendezvous point（集合点）**来协调。没有这个地址：</p>
<ul>
<li>进程A 不知道进程B 在哪里</li>
<li>无法知道自己是 rank 0 还是 rank 1</li>
<li>无法知道总共有多少个进程</li>
<li>通信完全无法建立</li>
</ul>
<p>这就是为什么需要一个”集合点”来协调所有分布式进程！</p>
<p><strong>那么，这个集合点什么时候会用到呢？</strong></p>
<p><strong>答案：主要在初始化阶段使用，通信阶段通常不依赖</strong></p>
<p><strong>初始化阶段 ✅ 必须使用</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 所有进程都需要通过 init_method 协调</span><br><span class="line">torch.distributed.init_process_group(</span><br><span class="line">    init_method=&#x27;tcp://192.168.1.100:12345&#x27;,  # ← 这里需要地址</span><br><span class="line">    backend=&#x27;nccl&#x27;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>通信阶段 ❌ 通常不依赖</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 一旦建立连接，通信是点对点的</span><br><span class="line">torch.distributed.all_reduce(tensor)  # 不需要再指定地址</span><br><span class="line">torch.distributed.broadcast(tensor, src=0)  # 直接进程间通信</span><br></pre></td></tr></table></figure>
<p><strong>工作流程</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">初始化阶段：</span><br><span class="line">进程A ──连接──&gt; MASTER_ADDR:MASTER_PORT ←──连接── 进程B</span><br><span class="line">进程C ──连接──&gt; MASTER_ADDR:MASTER_PORT ←──连接── 进程D</span><br><span class="line"></span><br><span class="line">通信阶段：</span><br><span class="line">进程A ───直接通信─── 进程B</span><br><span class="line">进程C ───直接通信─── 进程D</span><br><span class="line">      ↘              ↗</span><br><span class="line">        MASTER节点(不再参与通信)</span><br></pre></td></tr></table></figure>
<p><strong>特殊情况仍需依赖</strong></p>
<ul>
<li><strong>动态进程组管理</strong>：添加新进程时可能仍需主节点协调</li>
<li><strong>某些NCCL配置</strong>：复杂网络拓扑下主节点参与通信协调</li>
<li><strong>错误恢复场景</strong>：进程崩溃后重新通过主节点协调恢复</li>
</ul>
<p><strong>实际意义</strong>：<code>init_method</code> 主要是分布式训练的**“媒人”，牵线搭桥后就可以”退场”了！**</p>
<h3 id="环境变量配置">环境变量配置</h3>
<p>在使用 <code>init_method='env://'</code> 时，需要设置以下环境变量：</p>
<ul>
<li><code>MASTER_ADDR</code>: 主节点地址</li>
<li><code>MASTER_PORT</code>: 主节点端口</li>
<li><code>RANK</code>: 当前进程的rank (0 到 world_size-1)</li>
<li><code>WORLD_SIZE</code>: 总进程数</li>
</ul>
<h3 id="进程组管理">进程组管理</h3>
<h4 id="默认进程组">默认进程组</h4>
<p>初始化后会创建一个默认的全局进程组，可以通过以下函数获取信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取当前进程的rank</span></span><br><span class="line">rank = torch.distributed.get_rank()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取world size</span></span><br><span class="line">world_size = torch.distributed.get_world_size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否已初始化</span></span><br><span class="line">is_initialized = torch.distributed.is_initialized()</span><br></pre></td></tr></table></figure>
<h4 id="自定义进程组">自定义进程组</h4>
<p>可以创建自定义的进程组来实现更灵活的通信模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建新的进程组</span></span><br><span class="line">new_group = torch.distributed.new_group(ranks=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特定进程组的大小</span></span><br><span class="line">group_size = torch.distributed.get_world_size(new_group)</span><br></pre></td></tr></table></figure>
<h4 id="进程组生命周期">进程组生命周期</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 销毁进程组</span></span><br><span class="line">torch.distributed.destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 销毁特定进程组</span></span><br><span class="line">torch.distributed.destroy_process_group(new_group)</span><br></pre></td></tr></table></figure>
<h3 id="张量并行">张量并行</h3>
<p><strong>列并行 (Column Parallelism)</strong>：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">设备0: y0 = x @ W0  →  y0</span><br><span class="line">设备1: y1 = x @ W1  →  y1</span><br><span class="line">最终: y = concat([y0, y1])</span><br></pre></td></tr></table></figure>
<p><strong>行并行 (Row Parallelism)</strong>：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">设备0: x0 @ W0  → 部分结果</span><br><span class="line">设备1: x1 @ W1  → 部分结果</span><br><span class="line">最终: y = sum(所有部分结果)</span><br></pre></td></tr></table></figure>
<p><strong>基于torch.distributed的实现</strong></p>
<h4 id="列并行实现-Column-Parallelism">列并行实现 (Column Parallelism)</h4>
<p>列并行通过分割输出维度来实现并行化，特别适合注意力机制中的 Query-Key-Value 投影。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ColumnParallelLinear</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    列并行线性层：分割输出维度，实现并行计算</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    计算过程：y = x @ W + b</span></span><br><span class="line"><span class="string">    分割策略：W 按列分割，每个进程处理部分输出特征</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size: <span class="built_in">int</span>, output_size: <span class="built_in">int</span>, world_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.world_size = world_size</span><br><span class="line">        <span class="variable language_">self</span>.rank = dist.get_rank()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 验证维度可分割性</span></span><br><span class="line">        <span class="keyword">assert</span> output_size % world_size == <span class="number">0</span>, <span class="string">f&quot;output_size (<span class="subst">&#123;output_size&#125;</span>) 必须能被 world_size (<span class="subst">&#123;world_size&#125;</span>) 整除&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个进程的局部输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.local_output_size = output_size // world_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化局部权重矩阵 [local_output_size, input_size]</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(<span class="variable language_">self</span>.local_output_size, input_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化局部偏置向量 [local_output_size]</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.randn(<span class="variable language_">self</span>.local_output_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播：执行局部计算并聚合结果</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_tensor: 输入张量 [batch_size, input_size]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output_tensor: 输出张量 [batch_size, output_size]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Step 1: 局部矩阵乘法</span></span><br><span class="line">        local_output = torch.matmul(input_tensor, <span class="variable language_">self</span>.weight.t()) + <span class="variable language_">self</span>.bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: 收集所有进程的局部输出</span></span><br><span class="line">        gathered_outputs = [torch.zeros_like(local_output) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.world_size)]</span><br><span class="line">        dist.all_gather(gathered_outputs, local_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: 在输出维度上拼接所有局部结果</span></span><br><span class="line">        final_output = torch.cat(gathered_outputs, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<h4 id="行并行实现-Row-Parallelism">行并行实现 (Row Parallelism)</h4>
<p>行并行通过分割输入维度来实现并行化，常用于多头注意力中的输出投影层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">class RowParallelLinear(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    行并行线性层：分割输入维度，实现归约并行</span><br><span class="line"></span><br><span class="line">    计算过程：y = x @ W + b</span><br><span class="line">    分割策略：W 按行分割，每个进程处理部分输入特征</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_size: int, output_size: int, world_size: int):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.world_size = world_size</span><br><span class="line">        self.rank = dist.get_rank()</span><br><span class="line"></span><br><span class="line">        # 验证维度可分割性</span><br><span class="line">        assert input_size % world_size == 0, f&quot;input_size (&#123;input_size&#125;) 必须能被 world_size (&#123;world_size&#125;) 整除&quot;</span><br><span class="line"></span><br><span class="line">        # 计算每个进程的局部输入维度</span><br><span class="line">        self.local_input_size = input_size // world_size</span><br><span class="line"></span><br><span class="line">        # 初始化局部权重矩阵 [output_size, local_input_size]</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(output_size, self.local_input_size))</span><br><span class="line"></span><br><span class="line">        # 初始化全局偏置向量 [output_size] - 所有进程共享相同偏置</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(output_size))</span><br><span class="line"></span><br><span class="line">    def forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向传播：分割输入、局部计算、全局归约</span><br><span class="line"></span><br><span class="line">        Args:</span><br><span class="line">            input_tensor: 输入张量 [batch_size, input_size]</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">            output_tensor: 输出张量 [batch_size, output_size]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # Step 1: 将输入张量按最后一个维度分割</span><br><span class="line">        input_chunks = torch.chunk(input_tensor, self.world_size, dim=-1)</span><br><span class="line">        local_input = input_chunks[self.rank]  # 选择当前进程对应的输入块</span><br><span class="line"></span><br><span class="line">        # Step 2: 局部矩阵乘法</span><br><span class="line">        local_output = torch.matmul(local_input, self.weight.t())</span><br><span class="line"></span><br><span class="line">        # Step 3: 全局归约 - 对所有进程的局部输出求和</span><br><span class="line">        dist.all_reduce(local_output, op=dist.ReduceOp.SUM)</span><br><span class="line"></span><br><span class="line">        # Step 4: 添加偏置（所有进程都需要添加相同的偏置以保持梯度同步）</span><br><span class="line">        local_output += self.bias</span><br><span class="line"></span><br><span class="line">        return local_output</span><br></pre></td></tr></table></figure>
<h4 id="多层感知机张量并行实现">多层感知机张量并行实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TensorParallelMLP</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;张量并行多层感知机&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, world_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.world_size = world_size</span><br><span class="line">        <span class="variable language_">self</span>.rank = dist.get_rank()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层：列并行</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = ColumnParallelLinear(hidden_size, hidden_size * <span class="number">4</span>, world_size)</span><br><span class="line">        <span class="comment"># 第二层：行并行</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = RowParallelLinear(hidden_size * <span class="number">4</span>, hidden_size, world_size)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.gelu = nn.GELU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 第一层：列并行</span></span><br><span class="line">        hidden = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        hidden = <span class="variable language_">self</span>.gelu(hidden)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层：行并行</span></span><br><span class="line">        output = <span class="variable language_">self</span>.fc2(hidden)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h4 id="梯度同步处理">梯度同步处理</h4>
<p>张量并行需要特殊的梯度处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_parallel_backward_hook</span>(<span class="params">module, grad_input, grad_output</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;处理张量并行的梯度同步&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 确保梯度正确传播</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(module, <span class="string">&#x27;weight&#x27;</span>):</span><br><span class="line">        dist.all_reduce(module.weight.grad, op=dist.ReduceOp.SUM)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(module, <span class="string">&#x27;bias&#x27;</span>) <span class="keyword">and</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        dist.all_reduce(module.bias.grad, op=dist.ReduceOp.SUM)</span><br><span class="line">    <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册钩子</span></span><br><span class="line">model.fc1.register_backward_hook(tensor_parallel_backward_hook)</span><br><span class="line">model.fc2.register_backward_hook(tensor_parallel_backward_hook)</span><br></pre></td></tr></table></figure>
<h4 id="完整训练示例">完整训练示例</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_tensor_parallel</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;张量并行训练示例&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化分布式环境</span></span><br><span class="line">    dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)</span><br><span class="line">    torch.cuda.set_device(dist.get_rank())</span><br><span class="line"></span><br><span class="line">    world_size = dist.get_world_size()</span><br><span class="line">    rank = dist.get_rank()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    model = TensorParallelMLP(hidden_size=<span class="number">1024</span>, world_size=world_size)</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 只优化局部参数</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:</span><br><span class="line">        input_data = batch[<span class="string">&#x27;input&#x27;</span>].cuda()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(input_data)</span><br><span class="line">        loss = compute_loss(output, batch[<span class="string">&#x27;target&#x27;</span>].cuda())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度同步已在钩子中处理</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_tensor_parallel()</span><br></pre></td></tr></table></figure>
<h4 id="内存效率分析">内存效率分析</h4>
<p><strong>张量并行的优势</strong>：</p>
<ul>
<li><strong>内存节省</strong>：每个设备只存储模型的一部分</li>
<li><strong>通信效率</strong>：只同步必要的中间结果</li>
<li><strong>扩展性</strong>：支持超大规模模型</li>
</ul>
<p><strong>与数据并行的对比</strong>：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>数据并行</th>
<th>张量并行</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存使用</td>
<td>O(N)</td>
<td>O(N/P)</td>
</tr>
<tr>
<td>通信量</td>
<td>高（梯度同步）</td>
<td>中等（中间结果）</td>
</tr>
<tr>
<td>适用场景</td>
<td>中等模型</td>
<td>超大模型</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>低</td>
<td>高</td>
</tr>
</tbody>
</table>
<h4 id="注意事项">注意事项</h4>
<h5 id="1-维度要求">1. 维度要求</h5>
<ul>
<li>权重矩阵维度必须能被world_size整除</li>
<li>输入输出维度需要仔细规划</li>
</ul>
<h5 id="2-负载均衡">2. 负载均衡</h5>
<ul>
<li>确保各设备计算量均衡</li>
<li>避免某些设备成为瓶颈</li>
</ul>
<h5 id="3-通信优化">3. 通信优化</h5>
<ul>
<li>选择合适的通信原语</li>
<li>考虑通信与计算的重叠</li>
</ul>
<h5 id="4-调试困难">4. 调试困难</h5>
<ul>
<li>张量并行调试相对复杂</li>
<li>需要验证梯度同步的正确性</li>
</ul>
<h3 id="高级应用：Megatron-LM风格实现">高级应用：Megatron-LM风格实现</h3>
<p>对于更复杂的Transformer模型，可以参考Megatron-LM的实现：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class ParallelTransformerLayer(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;并行Transformer层&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, hidden_size, num_heads, world_size):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        # 张量并行注意力</span><br><span class="line">        self.attention = TensorParallelMultiHeadAttention(</span><br><span class="line">            hidden_size, num_heads, world_size</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 张量并行前馈网络</span><br><span class="line">        self.feed_forward = TensorParallelMLP(hidden_size, world_size)</span><br><span class="line"></span><br><span class="line">        self.norm1 = nn.LayerNorm(hidden_size)</span><br><span class="line">        self.norm2 = nn.LayerNorm(hidden_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 残差连接 + 注意力</span><br><span class="line">        attn_output = self.attention(self.norm1(x))</span><br><span class="line">        x = x + attn_output</span><br><span class="line"></span><br><span class="line">        # 残差连接 + 前馈</span><br><span class="line">        ff_output = self.feed_forward(self.norm2(x))</span><br><span class="line">        x = x + ff_output</span><br><span class="line"></span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>
<p>这个实现展示了如何将张量并行应用到完整的Transformer架构中，实现超大规模语言模型的训练。</p>
<hr>
<h2 id="状态检查函数">状态检查函数</h2>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 检查分布式环境状态</span><br><span class="line">print(torch.distributed.is_initialized())      # 是否已初始化</span><br><span class="line">print(torch.distributed.get_rank())            # 当前进程rank</span><br><span class="line">print(torch.distributed.get_world_size())      # 进程总数</span><br><span class="line">print(torch.distributed.get_backend())         # 当前后端</span><br></pre></td></tr></table></figure>
<h2 id="进程组管理工具">进程组管理工具</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取进程组信息</span></span><br><span class="line">default_group = torch.distributed.group.WORLD  <span class="comment"># 默认全局组</span></span><br><span class="line">group_size = torch.distributed.get_world_size(default_group)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查进程是否在组中</span></span><br><span class="line"><span class="built_in">print</span>(torch.distributed.get_rank() <span class="keyword">in</span> torch.distributed.get_process_group_ranks(default_group))</span><br></pre></td></tr></table></figure>
<h2 id="调试和监控工具">调试和监控工具</h2>
<h3 id="性能监控">性能监控</h3>
<p>megatron timer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的通信性能测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark_communication</span>(<span class="params">tensor_size</span>):</span><br><span class="line">    tensor = torch.randn(tensor_size).cuda()</span><br><span class="line"></span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    torch.distributed.all_reduce(tensor)</span><br><span class="line"></span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> end_time - start_time</span><br></pre></td></tr></table></figure>
<h3 id="内存监控">内存监控</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPU内存使用情况</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_gpu_memory</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Rank <span class="subst">&#123;torch.distributed.get_rank()&#125;</span>: &quot;</span></span><br><span class="line">              <span class="string">f&quot;GPU memory: <span class="subst">&#123;torch.cuda.memory_allocated()/<span class="number">1e9</span>:<span class="number">.2</span>f&#125;</span>GB / &quot;</span></span><br><span class="line">              <span class="string">f&quot;<span class="subst">&#123;torch.cuda.memory_reserved()/<span class="number">1e9</span>:<span class="number">.2</span>f&#125;</span>GB&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="实用工具函数">实用工具函数</h2>
<h3 id="分布式随机种子设置">分布式随机种子设置</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_random_seed</span>(<span class="params">seed=<span class="number">42</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置分布式训练的随机种子&quot;&quot;&quot;</span></span><br><span class="line">    rank = torch.distributed.get_rank()</span><br><span class="line">    torch.manual_seed(seed + rank)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed + rank)</span><br><span class="line">        torch.cuda.manual_seed_all(seed + rank)</span><br></pre></td></tr></table></figure>
<h3 id="分布式验证">分布式验证</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distributed_validation</span>(<span class="params">model, val_loader, criterion</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;分布式验证函数&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">    total_samples = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> inputs, targets <span class="keyword">in</span> val_loader:</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 收集所有进程的结果</span></span><br><span class="line">            torch.distributed.all_reduce(loss, op=torch.distributed.ReduceOp.SUM)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算准确率</span></span><br><span class="line">            _, predicted = outputs.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">            correct = predicted.eq(targets).<span class="built_in">sum</span>().item()</span><br><span class="line">            total_correct += correct</span><br><span class="line">            total_samples += targets.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算全局平均</span></span><br><span class="line">    world_size = torch.distributed.get_world_size()</span><br><span class="line">    total_loss /= world_size</span><br><span class="line">    total_correct /= world_size</span><br><span class="line">    total_samples /= world_size</span><br><span class="line"></span><br><span class="line">    accuracy = total_correct / total_samples</span><br><span class="line">    <span class="keyword">return</span> total_loss, accuracy</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Roger-Lv</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/">http://example.com/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Roger-Lv's space</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/llm/">llm</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/pytorch.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/12/22/2025-12-22-LLM%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF%EF%BC%9AMC-TD-Q-Learning-DQN-PG-AC-TRPO-PPO-DPO-GRPO/" title="LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/pytorch.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LLM强化学习算法演进之路：MC->TD->Q-Learning->DQN->PG->AC->TRPO->PPO->DPO->GRPO</div></div><div class="info-2"><div class="info-item-1">LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO https://zhuanlan.zhihu.com/p/20949520788： 这文章很详细 </div></div></div></a><a class="pagination-related" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/tongsearch.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">TongSearch-QR:Reinforced Query Reasoning for Retrieval</div></div><div class="info-2"><div class="info-item-1">TongSearch-QR: Reinforced Query Reasoning for Retrieval 这篇论文《TongSearch-QR: Reinforced Query Reasoning for Retrieval》提出了一种面向推理密集型检索（reasoning-intensive retrieval）任务的新型查询推理与重写模型家族，旨在解决传统信息检索方法在处理复杂、需要多跳推理的查询时性能不足的问题。  一、问题背景 传统信息检索（IR）方法（如 BM25、稠密向量检索）依赖词法匹配或语义相似度，在一般检索任务上表现良好。但在以下场景中表现不佳：  用户问题隐含深层意图（如“找一个可替代函数 Funca 的函数 Funcb”）； 相关文档未显式提及原问题中的关键词； 需要推理链（reasoning chain）才能连接查询与文档。  这类任务被称作 推理密集型检索（reasoning-intensive retrieval），如 BRIGHT 基准测试所定义。  二、现有方法及其局限 1. 大语言模型（LLM）提示工程  使用 GPT-4、LLaMA3-...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/12/09/2025-12-09-Agent%E6%A1%86%E6%9E%B6%E9%9B%86%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E8%83%BD%E5%8A%9B/" title="Agent框架集成多模态能力底层实现"><div class="cover" style="background: /img/cover/langgraph.jepg"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-09</div><div class="info-item-2">Agent框架集成多模态能力底层实现</div></div><div class="info-2"><div class="info-item-1">Agent框架集成多模态能力底层实现 该项目处理多模态RAG返回图片的完整流程： 架构概述 该项目采用分层架构处理多模态RAG：  前端接口层：通过schema.py中的ImageContent和ImageUrl模型支持base64和HTTPS两种图片URL格式 RAG核心层：rag.py中的RagClient提供统一的向量检索接口 多模态嵌入层：multi_model.py中的AliyunEmbeddings使用阿里云DashScope的多模态嵌入API 数据存储层：使用Qdrant向量数据库存储图片和文本的嵌入向量  图片处理流程 1. 图片存储阶段 在feishu-crawler子项目中，图片处理流程如下：  图片下载：DownloadImageTransform从飞书下载图片到本地文件系统 图片摘要生成：GenerateImageSummaryTransform使用VLLM模型为图片生成文字描述 多模态嵌入：EmbedImageTransform调用MultiModelEmbedder生成图片+文字的联合嵌入向量 向量存储：将base64编码的图片数据、文字描述和嵌入向量...</div></div></div></a><a class="pagination-related" href="/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/" title="Agent八股"><div class="cover" style="background: /img/cover/langgraph.jepg"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-17</div><div class="info-item-2">Agent八股</div></div><div class="info-2"><div class="info-item-1">Agent八股 模板1 八股：Encoder与decoder的中Attention区别？ 答案：https://zhuanlan.zhihu.com/p/26252050300 https://www.zhihu.com/question/588325646/answer/1981416261771604279 八股：Attention如何计算？为什么除以根号下Dk？mask attention是如何实现的？ 你的问题涉及 Transformer 模型中 Attention 机制的三个关键点：  Attention 的计算方式 为什么除以 (\sqrt{d_k}) Masked Attention 的实现方式  下面逐一解释：   Attention 如何计算？（以 Scaled Dot-Product Attention 为例）  标准的 Scaled Dot-Product Attention 公式如下： \[ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V ...</div></div></div></a><a class="pagination-related" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/webdancer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="info-item-2">WebDancer:Towards Autonomous Information Seeking Agency</div></div><div class="info-2"><div class="info-item-1">WebDancer: Towards Autonomous Information Seeking Agency 论文标题：WebDancer: Towards Autonomous Information Seeking Agency 论文链接：https://arxiv.org/pdf/2505.22648 论文代码：https://github.com/Alibaba-NLP/DeepResearch 这篇论文介绍了一个基于ReAct范式的网络智能体——WebDancer，通义团队透过训练赋予其自主寻求信息的能力。通义团队的训练流程主要有四个步骤，构造问答对、获得高质量轨迹、监督微调和强化学习。 问答对构造 不同于之前的简单的2到3步就能解决的问答问题，通义团队这里主要想构造的是那些可以激发模型多步推理、目标分解、交互等能力的问答对数据，因此希望对多跳推理的广度和深度都进行扩展。为此，他们提出了两个问答对数据集——CRAWLQA和E2HQA。 CRAWLQA问答对的获取跟之前WebWalkerQA数据集的构造很类似，都是从一个根网页出发递归浏览其中链接指向的网页，基于收集的...</div></div></div></a><a class="pagination-related" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/tongsearch.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="info-item-2">TongSearch-QR:Reinforced Query Reasoning for Retrieval</div></div><div class="info-2"><div class="info-item-1">TongSearch-QR: Reinforced Query Reasoning for Retrieval 这篇论文《TongSearch-QR: Reinforced Query Reasoning for Retrieval》提出了一种面向推理密集型检索（reasoning-intensive retrieval）任务的新型查询推理与重写模型家族，旨在解决传统信息检索方法在处理复杂、需要多跳推理的查询时性能不足的问题。  一、问题背景 传统信息检索（IR）方法（如 BM25、稠密向量检索）依赖词法匹配或语义相似度，在一般检索任务上表现良好。但在以下场景中表现不佳：  用户问题隐含深层意图（如“找一个可替代函数 Funca 的函数 Funcb”）； 相关文档未显式提及原问题中的关键词； 需要推理链（reasoning chain）才能连接查询与文档。  这类任务被称作 推理密集型检索（reasoning-intensive retrieval），如 BRIGHT 基准测试所定义。  二、现有方法及其局限 1. 大语言模型（LLM）提示工程  使用 GPT-4、LLaMA3-...</div></div></div></a><a class="pagination-related" href="/2025/12/23/2025-12-23-Anthropic-skils%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/" title="Anthropic skils解读与实践"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/SKILL.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-23</div><div class="info-item-2">Anthropic skils解读与实践</div></div><div class="info-2"><div class="info-item-1">Anthropic skils解读与实践 https://github.com/anthropics/skills 全流程周期：https://zhuanlan.zhihu.com/p/1984015383276041355  介绍 二者都是扩展LLM能力的一种手段。 Agent Skills 是一种标准化的程序性知识封装格式。如果说 MCP 为智能体提供了&quot;手&quot;来操作工具，那么 Skills 就提供了&quot;操作手册&quot;或&quot;SOP（标准作业程序）&quot;，教导智能体如何正确使用这些工具。 这种设计理念源于一个简单但深刻的洞察：连接性（Connectivity）与能力（Capability）应该分离。MCP 专注于前者，Skills 专注于后者。这种职责分离带来了清晰的架构优势： MCP 的职责：提供标准化的访问接口，让智能体能够&quot;够得着&quot;外部世界的数据和工具 Skills 的职责：提供领域专业知识，告诉智能体在特定场景下&quot;如何组合使用这些工具&quot;  MCP 在使用上的不同之处在于，MCP的流程是...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Roger-Lv</div><div class="author-info-description">Send a flare and light the way.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">150</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">49</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Roger-Lv"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Roger-Lv" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1150568956@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zhongrenjie-lv-5588a928a/" target="_blank" title="LinkedIn"><i class="iconfont icon-linkedin-fill"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">pytorch学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90%EF%BC%88%E5%8C%85%E5%90%AB%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD%E3%80%81%E8%AE%AD%E7%BB%83%E3%80%81%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E6%B5%8B%E8%AF%95%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">简单例子（包含数据集加载、训练、模型保存和测试）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.1.</span> <span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.3.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.4.</span> <span class="toc-text">保存模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">1.1.5.</span> <span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-torch-distributed"><span class="toc-number">1.2.</span> <span class="toc-text">分布式训练:torch.distributed</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1"><span class="toc-number">1.2.1.</span> <span class="toc-text">分布式进程间通信</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%9A%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.2.2.</span> <span class="toc-text">分布式多进程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-number">1.2.3.</span> <span class="toc-text">分布式数据并行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-torch-nn-parallel-DistributedDataParallel-DDP"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">1. torch.nn.parallel.DistributedDataParallel (DDP)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-torch-distributed-fsdp-FullyShardedDataParallel-FSDP"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">2. torch.distributed.fsdp.FullyShardedDataParallel (FSDP)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-torch-utils-data-distributed-DistributedSampler"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">3. torch.utils.data.distributed.DistributedSampler</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6%E5%90%8C%E6%AD%A5"><span class="toc-number">1.2.4.</span> <span class="toc-text">自动梯度同步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.5.</span> <span class="toc-text">核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%BB%84%EF%BC%88Process-Group%EF%BC%89"><span class="toc-number">1.2.5.1.</span> <span class="toc-text">进程组（Process Group）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rank%EF%BC%88%E8%BF%9B%E7%A8%8B%E6%A0%87%E8%AF%86%E7%AC%A6%EF%BC%89"><span class="toc-number">1.2.5.2.</span> <span class="toc-text">rank（进程标识符）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#world-size%EF%BC%88%E8%BF%9B%E7%A8%8B%E6%80%BB%E6%95%B0%EF%BC%89"><span class="toc-number">1.2.5.3.</span> <span class="toc-text">world_size（进程总数）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E8%BF%9B%E7%A8%8B%E7%BB%84%E7%AE%A1%E7%90%86"><span class="toc-number">1.2.6.</span> <span class="toc-text">初始化和进程组管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83"><span class="toc-number">1.2.6.1.</span> <span class="toc-text">初始化分布式环境</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.7.</span> <span class="toc-text">环境变量配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%BB%84%E7%AE%A1%E7%90%86"><span class="toc-number">1.2.8.</span> <span class="toc-text">进程组管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E8%BF%9B%E7%A8%8B%E7%BB%84"><span class="toc-number">1.2.8.1.</span> <span class="toc-text">默认进程组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%9B%E7%A8%8B%E7%BB%84"><span class="toc-number">1.2.8.2.</span> <span class="toc-text">自定义进程组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%BB%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="toc-number">1.2.8.3.</span> <span class="toc-text">进程组生命周期</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C"><span class="toc-number">1.2.9.</span> <span class="toc-text">张量并行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%97%E5%B9%B6%E8%A1%8C%E5%AE%9E%E7%8E%B0-Column-Parallelism"><span class="toc-number">1.2.9.1.</span> <span class="toc-text">列并行实现 (Column Parallelism)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%8C%E5%B9%B6%E8%A1%8C%E5%AE%9E%E7%8E%B0-Row-Parallelism"><span class="toc-number">1.2.9.2.</span> <span class="toc-text">行并行实现 (Row Parallelism)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.9.3.</span> <span class="toc-text">多层感知机张量并行实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%90%8C%E6%AD%A5%E5%A4%84%E7%90%86"><span class="toc-number">1.2.9.4.</span> <span class="toc-text">梯度同步处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.2.9.5.</span> <span class="toc-text">完整训练示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E6%95%88%E7%8E%87%E5%88%86%E6%9E%90"><span class="toc-number">1.2.9.6.</span> <span class="toc-text">内存效率分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">1.2.9.7.</span> <span class="toc-text">注意事项</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E7%BB%B4%E5%BA%A6%E8%A6%81%E6%B1%82"><span class="toc-number">1.2.9.7.1.</span> <span class="toc-text">1. 维度要求</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">1.2.9.7.2.</span> <span class="toc-text">2. 负载均衡</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96"><span class="toc-number">1.2.9.7.3.</span> <span class="toc-text">3. 通信优化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E8%B0%83%E8%AF%95%E5%9B%B0%E9%9A%BE"><span class="toc-number">1.2.9.7.4.</span> <span class="toc-text">4. 调试困难</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8%EF%BC%9AMegatron-LM%E9%A3%8E%E6%A0%BC%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.10.</span> <span class="toc-text">高级应用：Megatron-LM风格实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.</span> <span class="toc-text">状态检查函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%BB%84%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7"><span class="toc-number">1.4.</span> <span class="toc-text">进程组管理工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E5%92%8C%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7"><span class="toc-number">1.5.</span> <span class="toc-text">调试和监控工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7"><span class="toc-number">1.5.1.</span> <span class="toc-text">性能监控</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%9B%91%E6%8E%A7"><span class="toc-number">1.5.2.</span> <span class="toc-text">内存监控</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.6.</span> <span class="toc-text">实用工具函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.6.1.</span> <span class="toc-text">分布式随机种子设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%AA%8C%E8%AF%81"><span class="toc-number">1.6.2.</span> <span class="toc-text">分布式验证</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/23/2025-12-23-Anthropic-skils%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/" title="Anthropic skils解读与实践"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/SKILL.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Anthropic skils解读与实践"/></a><div class="content"><a class="title" href="/2025/12/23/2025-12-23-Anthropic-skils%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E8%B7%B5/" title="Anthropic skils解读与实践">Anthropic skils解读与实践</a><time datetime="2025-12-22T16:00:00.000Z" title="发表于 2025-12-23 00:00:00">2025-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/22/2025-12-22-LLM%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF%EF%BC%9AMC-TD-Q-Learning-DQN-PG-AC-TRPO-PPO-DPO-GRPO/" title="LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/pytorch.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO"/></a><div class="content"><a class="title" href="/2025/12/22/2025-12-22-LLM%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF%EF%BC%9AMC-TD-Q-Learning-DQN-PG-AC-TRPO-PPO-DPO-GRPO/" title="LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO">LLM强化学习算法演进之路：MC-&gt;TD-&gt;Q-Learning-&gt;DQN-&gt;PG-&gt;AC-&gt;TRPO-&gt;PPO-&gt;DPO-&gt;GRPO</a><time datetime="2025-12-21T16:00:00.000Z" title="发表于 2025-12-22 00:00:00">2025-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/" title="pytorch学习"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/pytorch.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch学习"/></a><div class="content"><a class="title" href="/2025/12/19/2025-12-19-pytorch%E5%AD%A6%E4%B9%A0/" title="pytorch学习">pytorch学习</a><time datetime="2025-12-18T16:00:00.000Z" title="发表于 2025-12-19 00:00:00">2025-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/webdancer.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebDancer:Towards Autonomous Information Seeking Agency"/></a><div class="content"><a class="title" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency">WebDancer:Towards Autonomous Information Seeking Agency</a><time datetime="2025-12-17T16:00:00.000Z" title="发表于 2025-12-18 00:00:00">2025-12-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/tongsearch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TongSearch-QR:Reinforced Query Reasoning for Retrieval"/></a><div class="content"><a class="title" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval">TongSearch-QR:Reinforced Query Reasoning for Retrieval</a><time datetime="2025-12-17T16:00:00.000Z" title="发表于 2025-12-18 00:00:00">2025-12-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2024 - 2025 By Roger-Lv</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.4.2"></script><script src="/js/main.js?v=5.4.2"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'smA3tZdRGodG2VgnMubBQjLm-gzGzoHsz',
      appKey: 'biCDxj0lSBtZTMie2kNIKErd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><div class="aplayer no-destroy" data-id="8674547170" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.4.2"></script></div></div></body></html>