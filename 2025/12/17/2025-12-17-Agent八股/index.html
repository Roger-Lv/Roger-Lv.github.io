<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Agent八股 | Roger-Lv's space</title><meta name="author" content="Roger-Lv"><meta name="copyright" content="Roger-Lv"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Agent八股 模板1 八股：Encoder与decoder的中Attention区别？ 答案：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;26252050300 https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;588325646&#x2F;answer&#x2F;1981416261771604279 八股：Attention如何计算？为什么除以根号下Dk？mask attention是">
<meta property="og:type" content="article">
<meta property="og:title" content="Agent八股">
<meta property="og:url" content="http://example.com/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/index.html">
<meta property="og:site_name" content="Roger-Lv&#39;s space">
<meta property="og:description" content="Agent八股 模板1 八股：Encoder与decoder的中Attention区别？ 答案：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;26252050300 https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;588325646&#x2F;answer&#x2F;1981416261771604279 八股：Attention如何计算？为什么除以根号下Dk？mask attention是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.jpg">
<meta property="article:published_time" content="2025-12-16T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-18T03:39:55.686Z">
<meta property="article:author" content="Roger-Lv">
<meta property="article:tag" content="agent">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="llm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Agent八股",
  "url": "http://example.com/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/",
  "image": "http://example.com/img/avatar.jpg",
  "datePublished": "2025-12-16T16:00:00.000Z",
  "dateModified": "2025-12-18T03:39:55.686Z",
  "author": [
    {
      "@type": "Person",
      "name": "Roger-Lv",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="http://example.com/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.4.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Agent八股',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">172</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">148</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">48</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Roger-Lv's space</span></a><a class="nav-page-title" href="/"><span class="site-name">Agent八股</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div><!-- 添加搜索按钮 ↓--><span class="search-button"><i class="fas fa-search" aria-hidden="true"></i></span></div></nav><div id="post-info"><h1 class="post-title">Agent八股</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-16T16:00:00.000Z" title="发表于 2025-12-17 00:00:00">2025-12-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-18T03:39:55.686Z" title="更新于 2025-12-18 11:39:55">2025-12-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/agent/">agent</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/" data-flag-title="Agent八股"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>Agent八股</h1>
<h2 id="模板1">模板1</h2>
<h3 id="八股：Encoder与decoder的中Attention区别？">八股：Encoder与decoder的中Attention区别？</h3>
<p>答案：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26252050300">https://zhuanlan.zhihu.com/p/26252050300</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/1981416261771604279">https://www.zhihu.com/question/588325646/answer/1981416261771604279</a></p>
<h3 id="八股：Attention如何计算？为什么除以根号下Dk？mask-attention是如何实现的？">八股：Attention如何计算？为什么除以根号下Dk？mask attention是如何实现的？</h3>
<p>你的问题涉及 <strong>Transformer 模型中 Attention 机制</strong>的三个关键点：</p>
<ol>
<li><strong>Attention 的计算方式</strong></li>
<li><strong>为什么除以 (\sqrt{d_k})</strong></li>
<li><strong>Masked Attention 的实现方式</strong></li>
</ol>
<p>下面逐一解释：</p>
<hr>
<ol>
<li>Attention 如何计算？（以 Scaled Dot-Product Attention 为例）</li>
</ol>
<p>标准的 <strong>Scaled Dot-Product Attention</strong> 公式如下：</p>
\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
\]

<p>其中：</p>
<ul>
<li>
\(Q \in \mathbb{R}^{n \times d_k}\)

<p>：查询（Query）矩阵</p>
</li>
<li>
\(K \in \mathbb{R}^{m \times d_k}\)

<p>：键（Key）矩阵</p>
</li>
<li>
\(V \in \mathbb{R}^{m \times d_v}\)

<p>：值（Value）矩阵</p>
</li>
<li>
\(d_k\)

<p>：Key 的维度（也等于 Query 的维度）</p>
</li>
</ul>
<p>计算步骤：</p>
<ol>
<li>计算点积 (<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0858em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
)，得到一个 (<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n \times m
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span></p>
) 的相似度矩阵。</li>
<li>除以\(\sqrt{d_k}\)

（缩放，见下文解释）。</li>
<li>对每一行进行 softmax，得到注意力权重。</li>
<li>用权重对 (V) 加权求和，得到输出。</li>
</ol>
<hr>
<ol start="2">
<li>为什么除以 (\sqrt{d_k})？</li>
</ol>
<p>这是为了 <strong>缓解点积过大导致 softmax 梯度消失的问题</strong>。</p>
<ul>
<li>假设 (q) 和 (k) 的每个元素是均值为 0、方差为 1 的独立随机变量。</li>
<li>那么点积\(q \cdot k = \sum_{i=1}^{d_k} q_i k_i\)

的期望为 0，**方差为 **\(d_k\)

。</li>
<li>当 (d_k) 较大时（如 64、512），点积的值会很大，使得 softmax 的输入进入其极值区域（趋于 0 或 1）。</li>
<li>这会导致梯度非常小（softmax 饱和），训练困难。</li>
</ul>
<p><strong>解决方法</strong>：将点积结果除以</p>
\(\sqrt{d_k}\)

<p>，使得其方差回到 1，从而保持 softmax 的输入在合理范围内，梯度更稳定。</p>
<blockquote>
<p>这就是 “Scaled” 的由来：<strong>Scaled Dot-Product Attention</strong>。</p>
</blockquote>
<hr>
<ol start="3">
<li>Masked Attention 是如何实现的？</li>
</ol>
<p><strong>Masked Attention</strong> 主要用于 <strong>自回归模型</strong>（如 GPT、Decoder-only Transformer），防止当前位置“看到”未来信息。</p>
<p>实现方式：</p>
<p>在 softmax 之前，将未来位置的 attention score 设为 <strong>负无穷（(-\infty)）</strong>，这样 softmax 后对应的权重为 0。</p>
<p>具体步骤：</p>
<ol>
<li>
<p>计算</p>
\(QK^T / \sqrt{d_k}\)

<p>，得到 attention scores 矩阵 (A)。</p>
</li>
<li>
<p>构造一个 <strong>上三角 mask</strong>（未来位置为 True），例如对长度为 4 的序列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mask = [[0, 1, 1, 1],</span><br><span class="line">        [0, 0, 1, 1],</span><br><span class="line">        [0, 0, 0, 1],</span><br><span class="line">        [0, 0, 0, 0]]</span><br></pre></td></tr></table></figure>
<p>实际代码中常用 <strong>布尔 mask</strong> 或 <strong>加性 mask</strong>。</p>
</li>
<li>
<p>将 mask 位置的 score 替换为一个极大负数（如 <code>-1e9</code>）：</p>
\[
A_{\text{masked}} = A + \text{mask} \times (-\infty)
\]

</li>
<li>
<p>再对</p>
\(A_{\text{masked}}\)

<p>做 softmax。</p>
</li>
</ol>
<p>代码示例（PyTorch）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">attn_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=<span class="number">1</span>).<span class="built_in">bool</span>()  <span class="comment"># 上三角为True</span></span><br><span class="line">attn_scores = attn_scores.masked_fill(mask, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">attn_weights = torch.softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line">output = torch.matmul(attn_weights, V)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在 Encoder 中通常 <strong>不加 mask</strong>（因为可以看整个输入）；<br>
在 Decoder 的 self-attention 中 <strong>必须加 mask</strong>（防止信息泄露）。</p>
</blockquote>
<hr>
<p>总结</p>
<table>
<thead>
<tr>
<th>问题</th>
<th>答案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attention 计算</td>
<td>(\text{softmax}(QK^T / \sqrt{d_k}) V)</td>
</tr>
<tr>
<td>除以 (\sqrt{d_k}) 原因</td>
<td>控制点积方差，防止 softmax 梯度消失</td>
</tr>
<tr>
<td>Masked Attention</td>
<td>用 mask 将未来位置 score 设为 (-\infty)，再 softmax</td>
</tr>
</tbody>
</table>
<p>如果你对 Multi-Head Attention、Cross-Attention 或具体代码实现感兴趣，也可以继续问！</p>
<h3 id="八股：除了MHA还知道哪些-GQA-MQA-MLA-讲原理">八股：除了MHA还知道哪些(GQA MQA MLA)讲原理</h3>
<p>除了经典的 <strong>MHA（Multi-Head Attention）</strong>，近年来为了在<strong>推理效率、显存占用和模型性能</strong>之间取得更好平衡，研究者提出了多种改进型注意力机制，主要包括：</p>
<ul>
<li><strong>MQA（Multi-Query Attention）</strong></li>
<li><strong>GQA（Grouped-Query Attention）</strong></li>
<li><strong>MLA（Multi-Layer Attention / Memory-efficient Latent Attention）</strong></li>
</ul>
<p>下面分别讲解它们的核心原理和区别：</p>
<hr>
<ol>
<li>MHA（Multi-Head Attention）——基准</li>
</ol>
<ul>
<li><strong>原理</strong>：将 Query、Key、Value 分别用 (h) 个不同的线性投影头（heads）映射。</li>
<li>每个头独立计算注意力，最后拼接或加权融合。</li>
<li><strong>特点</strong>：
<ul>
<li>(h) 个 Q、K、V（例如 32 头 → 32 个 K/V）</li>
<li>表达能力强，但 KV Cache 显存开销大（每个 token 存 (h) 份 K 和 V）</li>
</ul>
</li>
</ul>
<hr>
<ol start="2">
<li>MQA（Multi-Query Attention）——极致压缩 KV</li>
</ol>
<ul>
<li><strong>原理</strong>：<strong>所有 attention head 共享同一组 Key 和 Value</strong>，但每个 head 有自己的 Query。
<ul>
<li>即：(h) 个 Q，但只有 <strong>1 个 K 和 1 个 V</strong></li>
</ul>
</li>
<li><strong>优点</strong>：
<ul>
<li><strong>KV Cache 大幅减少</strong>（从 (h) 份 → 1 份）</li>
<li>推理速度更快，显存占用更低</li>
</ul>
</li>
<li><strong>缺点</strong>：
<ul>
<li>模型表达能力下降，可能导致生成质量略降（尤其在小模型上）</li>
</ul>
</li>
<li><strong>应用场景</strong>：推理效率优先的场景（如早期的 <strong>StarCoder</strong> 使用了 MQA）</li>
</ul>
<hr>
<ol start="3">
<li>GQA（Grouped-Query Attention）——MHA 与 MQA 的折中</li>
</ol>
<ul>
<li><strong>原理</strong>：将 (h) 个 Query heads <strong>分组</strong>（比如 8 组），<strong>每组共享一个 K/V head</strong>。
<ul>
<li>例如：32 个 Q heads → 分成 8 组 → 每组 4 个 Q 共享 1 个 K/V → 总共 8 个 K/V</li>
</ul>
</li>
<li><strong>优点</strong>：
<ul>
<li>相比 MHA：<strong>显著减少 KV Cache</strong>（从 32 份 → 8 份）</li>
<li>相比 MQA：<strong>保留更多表达能力</strong>，性能损失更小</li>
</ul>
</li>
<li><strong>实际应用</strong>：
<ul>
<li><strong>Llama-2 / Llama-3</strong> 的 70B 模型使用了 GQA（32 Q heads, 8 KV heads）</li>
<li>被证明可以在几乎不损失效果的前提下提升推理吞吐</li>
</ul>
</li>
</ul>
<blockquote>
<p>GQA 被认为是 <strong>MHA 和 MQA 之间的最佳平衡点</strong> 。</p>
</blockquote>
<hr>
<ol start="4">
<li>MLA（Multi-Layer Attention / Memory-efficient Latent Attention）</li>
</ol>
<blockquote>
<p>⚠️ 注意：MLA 有两种不同含义，此处指 <strong>Gemini 系列模型中提出的高效注意力机制</strong>（不是“多层注意力”字面意思）。</p>
</blockquote>
<ul>
<li><strong>原理</strong>（以 Google Gemini 的 MLA 为例）：
<ul>
<li>不直接缓存原始 K/V，而是将 K/V <strong>压缩到一个低维共享的“潜在记忆”（latent memory）</strong> 中。</li>
<li>推理时通过这个共享 latent 来重建近似的 K/V。</li>
<li>本质是一种 <strong>KV 压缩+共享</strong> 技术。</li>
</ul>
</li>
<li><strong>优点</strong>：
<ul>
<li><strong>KV Cache 体积极小</strong>（比 GQA 还小）</li>
<li>更适合长上下文、高并发推理</li>
</ul>
</li>
<li><strong>缺点</strong>：
<ul>
<li>实现复杂，需额外训练或微调</li>
<li>可能引入重建误差</li>
</ul>
</li>
<li><strong>应用</strong>：Google <strong>Gemini 1.5 / 2.0</strong> 使用了 MLA 来支持百万 token 上下文 。</li>
</ul>
<hr>
<p>对比总结表</p>
<table>
<thead>
<tr>
<th>机制</th>
<th>Q heads</th>
<th>K/V heads</th>
<th>KV Cache 量</th>
<th>表达能力</th>
<th>典型应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MHA</strong></td>
<td>(h)</td>
<td>(h)</td>
<td>高（(h) 份）</td>
<td>最强</td>
<td>Transformer 原始设计</td>
</tr>
<tr>
<td><strong>MQA</strong></td>
<td>(h)</td>
<td>1</td>
<td>极低（1 份）</td>
<td>较弱</td>
<td>StarCoder</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td>(h)</td>
<td>(g)（(1 &lt; g &lt; h)）</td>
<td>中（(g) 份）</td>
<td>接近 MHA</td>
<td>Llama-2/3 70B</td>
</tr>
<tr>
<td><strong>MLA</strong></td>
<td>(h)</td>
<td>隐式共享（latent）</td>
<td>极低</td>
<td>依赖压缩质量</td>
<td>Gemini 1.5/2.0</td>
</tr>
</tbody>
</table>
<hr>
<p>为什么这些改进重要？</p>
<ul>
<li>在 <strong>大模型推理</strong> 中，<strong>KV Cache 占用显存最多</strong>（尤其长文本）。</li>
<li>MHA 的 KV Cache 与 <strong>head 数 × token 数 × dim</strong> 成正比，成为瓶颈。</li>
<li>MQA/GQA/MLA 通过 <strong>减少或压缩 K/V 的存储</strong>，<strong>提升吞吐、降低延迟、支持更长上下文</strong>，同时尽量保持模型能力 。</li>
</ul>
<hr>
<p>如需进一步了解某一种机制的数学公式或代码实现，也可以告诉我！</p>
<h3 id="八股：为什么要用位置编码？为什么要用sin-cos？">八股：为什么要用位置编码？为什么要用sin_cos？</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/454482273">https://zhuanlan.zhihu.com/p/454482273</a></p>
<p>之所以要sin cos交替，想要位置向量也可以表示相对位置，想象一个旋转的角度，△t，就也可以进行线性变化（矩阵乘）</p>
<h3 id="八股：你提到用DeepSpeed做SFT训练，请讲一下DeepSpeed-ZeRO-Stage-1-3的区别，以及什么时候用FSDP会更好？">八股：你提到用DeepSpeed做SFT训练，请讲一下DeepSpeed ZeRO Stage 1-3的区别，以及什么时候用FSDP会更好？</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/694880795">https://zhuanlan.zhihu.com/p/694880795</a></p>
<p>fp16 fp32</p>
<p>参数/梯度/优化器/激活值对于显存的占用</p>
<p>混合精度训练：权重采用fp16存储，反向传播更新，<strong>优化器会维持fp32模型权重</strong>和fp32优化器参数的副本，因为使用32位更准确</p>
<p>优化器状态：使用adamW优化器，fp32模型权重，fp32的梯度移动平均值和梯度平方移动平均值，fp16的梯度</p>
<p>简单就散，fp16模型需要的显存是其参数量的八倍</p>
<p>zero1: 切分优化器状态。</p>
<ol>
<li>优化器状态是主要的内存占用者。zero1的思路是将这些优化器状态分布到多个GPU上</li>
<li>这样增加了GPU通信，但降低了显存占用</li>
<li>减少4倍显存，保持通信量与dp相同</li>
</ol>
<p>zero2: 切分梯度</p>
<ol>
<li>再次将内存需求减少了一半，总体减少了8倍</li>
<li>仍保持与标准dp相同的通信水平</li>
</ol>
<p>zero3:切分模型参数</p>
<ol>
<li>减少显存占用与gpu数量成正比</li>
<li>增加了50%通讯量？</li>
<li>它的“分割”对象并非模型计算图本身，而是<strong>训练状态</strong>（参数、梯度、优化器状态）。它将这些状态切片，均匀地分发到所有 GPU 上。（动态聚合机制）</li>
</ol>
<p>这类特别说一下deepspeed的zero3的切分模型参数后是怎么计算的：</p>
<p>假设我们有 4 个 GPU (GPU 0, 1, 2, 3)，并且需要计算网络中的 <code>Layer L</code>。</p>
<ul>
<li><strong>第 1 步：计算前的分区状态</strong> 在计算开始前，<code>Layer L</code> 的权重参数 <code>W_L</code> 被切分成了 4 片 (<code>P_0</code>, <code>P_1</code>, <code>P_2</code>, <code>P_3</code>)，分别存储在 4 个 GPU 的显存中。此时，没有任何一个 GPU 能独立开始计算。</li>
<li><strong>第 2 步：计算时的动态聚合 (<code>All-Gather</code>)</strong> 当计算流程到达 <code>Layer L</code> 时，ZeRO-3 触发一次 <code>All-Gather</code> 通信操作。每个 GPU 将自己持有的那一分片参数发送给所有其他 GPU，并同时接收其他所有 GPU 发来的分片。操作完成后：每个 GPU 都在自己的显存里临时拼凑出了一份完全相同的、完整的 <code>Layer L</code> 的参数 <code>W_L</code>。</li>
<li><strong>第 3 步：执行标准的层计算</strong> 现在，每个 GPU 都手握完整的 <code>Layer L</code> 参数，于是它们可以像标准数据并行一样，用<strong>各自的数据分片</strong>和<strong>这份完整的参数</strong>，独立地完成 <code>Layer L</code> 的前向传播计算。</li>
<li><strong>第 4 步：计算后立即释放</strong> 一旦 <code>Layer L</code> 的计算完成，每个 GPU 会<strong>立刻丢弃</strong>刚刚组装起来的完整参数，只保留自己最初负责的那一分片。显存被瞬间释放，为下一层的计算做好了准备。</li>
</ul>
<blockquote>
<p><strong>这里必须强调一个最关键、也最容易被误解的核心要点：ZeRO-3 的 All-Gather 不是一次性聚合整个模型，而是逐层（Layer-by-Layer）进行的。</strong> 它对显存的峰值需求，取决于<strong>模型中最大的那一层的参数大小</strong>，而不是整个模型的总大小。正是这种精细化的、“即用即弃”的逐层管理，才使得 ZeRO-3 能够以数据并行的方式，训练远超单卡显存容量的巨型模型。</p>
</blockquote>
<p>它的“模型分割”实际上是**“训练状态分割”**。它在计算的瞬间，通过通信换取空间，让每个 GPU 能“看到”完整的层参数来执行标准计算。它对模型代码的侵入性很低，但要求单卡至少能容纳下模型最大层带来的瞬时显存开销。</p>
<p>deepspeed没有原生tp，用的还是megatron的。</p>
<p>megatron就是列切，行切</p>
<h3 id="项目：问Agent的工具tool的设计，是否是workflow形式">项目：问Agent的工具tool的设计，是否是workflow形式</h3>
<p>不完全是，一方面有规则，一方面有agentic的成分</p>
<h3 id="项目：了解哪些agent开发框架，例如langchain和LlamaIndex，他们核心应用场景有何不同">项目：了解哪些agent开发框架，例如langchain和LlamaIndex，他们核心应用场景有何不同</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1933162022566106990">https://zhuanlan.zhihu.com/p/1933162022566106990</a></p>
<p>前面就是用来造智能体的框架，提供工具、message、state、graph（1.0）, memory</p>
<p>后者则是增强rag能力的，比如有的库就是导入向量库，有的是能够用来自动构建知识图谱的</p>
<h3 id="项目：问数据的输入输出格式如何保证大模型输出稳定的json做了哪些工作">项目：问数据的输入输出格式如何保证大模型输出稳定的json做了哪些工作</h3>
<h3 id="智力题：有12个外观相同的芯片、其中一个重量不同-不知轻重-，用天平最少称几次能找出这张芯片？">智力题：有12个外观相同的芯片、其中一个重量不同(不知轻重)，用天平最少称几次能找出这张芯片？</h3>
<h3 id="代码题：lc215-数组中的第K个最大元素">代码题：lc215 数组中的第K个最大元素</h3>
<h2 id="模板2">模板2</h2>
<h3 id="1-介绍RAG项目">1. 介绍RAG项目</h3>
<h3 id="2-怎么解决LLM幻觉问题">2.怎么解决LLM幻觉问题</h3>
<p>分阶段来回答：</p>
<p>数据处理阶段：</p>
<ol>
<li><strong>数据质量和多样性</strong>：选择高质量、多样性和平衡的数据集进行训练。这包括从多个来源收集数据，以及确保数据覆盖了各种主题和领域</li>
<li><strong>数据清洗</strong>：在训练模型之前，对数据进行清洗和预处理，以去除错误、偏见和不相关的信息</li>
<li><strong>数据标注</strong>：对数据进行标注，以提供关于信息真实性的额外信息。例如，可以标注数据中的事实是否正确，或者是否包含误导性的信息</li>
</ol>
<p>训练阶段：</p>
<ol>
<li><strong>模型微调</strong>：在特定任务或数据集上进一步训练模型，以改善模型在特定上下文中的表现</li>
<li><strong>模型结构和参数选择</strong>：选择或设计适合任务的模型结构，并调整模型的参数，以优化模型的性能</li>
<li><strong>模型集成</strong>：训练多个模型，并结合它们的输出，以提高输出的真实性</li>
<li><strong>有限状态约束 FST</strong>：使用约束解码，将输入的 FSA x 与一个特殊的 FST T 进行合成，用于编码所有可能的分段决策，并将其投影到输出带中，以获得输出空间的确定性有限状态自动机</li>
</ol>
<p>后处理阶段：</p>
<ol>
<li><strong>后处理和过滤</strong>：在模型生成输出后，使用各种策略来过滤或修改输出，以提高其真实性</li>
<li><strong>模型解释和可视化</strong>：理解模型的决策过程，以帮助识别可能的问题并改进模型</li>
<li><strong>用户反馈</strong>：收集用户对模型输出的反馈，并使用这些反馈来改进模型</li>
<li><strong>Levenshtein 事后对齐算法</strong>：我们使用它将生成的字符串与参考字符串进行对齐，在 LLM 没有精确重新创建输入时，可以消除一些不流畅的文本</li>
<li><strong>Web 检索确认</strong></li>
<li>引入外部知识库</li>
</ol>
<h3 id="3-LLM的参数介绍（temp-topk-top-p等）">3.LLM的参数介绍（temp topk top p等）</h3>
<p>logits/temp，温度越大各个位置的概率就越均衡</p>
<h3 id="4-LLaMA和GLM的区别，模型架构等方面">4.LLaMA和GLM的区别，模型架构等方面</h3>
<p>一篇跟大模型架构对比相关的文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1953492025719644723">https://zhuanlan.zhihu.com/p/1953492025719644723</a></p>
<h4 id="DeepSeek-v3-r1">DeepSeek v3/r1:</h4>
<ol>
<li>
<p>MLA（多头潜在注意力）</p>
<ol>
<li>MHA-&gt;GQA-&gt;MLA</li>
<li>MLA提供了一种不同的内存节省策略，与kv cache配合的比较好</li>
<li>MLA不像GQA那样共享键值头，而是将kv tensor压缩到低维空间，然后将它们存储在kv cache中。</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-f2c8830e57e890a3cbe4fa02675752d7_1440w.jpg" alt="img"></li>
<li><strong>MLA 是一个巧妙的技巧，可以减少 KV 缓存内存的使用，同时在建模性能方面甚至略胜于 MHA。</strong></li>
</ol>
</li>
<li>
<p>混合专家moe</p>
<ol>
<li>
<p><strong>用多个专家层替换 Transformer 模块中的每个前馈模块，其中每个专家层本身也是一个前馈模块</strong>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/v2-f80ceb4abce1f582d75caa230851fd48_1440w.jpg" alt="img"></p>
</li>
<li>
<p>关键在于我们不会为每个 token 使用（“激活”）所有专家。相反，路由器只会为每个 token 选择一小部分专家。</p>
</li>
<li>
<p>由于每次只有少数专家处于活跃状态，因此 MoE 模块通常被称为稀疏模块，这与始终使用完整参数集的密集模块形成对比。然而，通过 MoE 获得的大量参数增加了 LLM 的容量，这意味着它可以在训练期间吸收更多知识。然而，稀疏性保持了推理的高效性，因为我们不会同时使用所有参数。</p>
</li>
<li>
<p>例如，DeepSeek-V3 每个 MoE 模块有 256 位专家，总共 6710 亿个参数。然而，在推理过程中，每次只有 9 位专家处于活动状态（1 位共享专家加上 8 位由路由器选择的专家）。这意味着每个推理步骤仅使用 370 亿个参数，而不是全部 6710 亿个参数。</p>
<p><strong>DeepSeek-V3 的 MoE 设计的一个显著特点是使用了一个共享专家</strong>。这是一个始终对每个 token 保持活跃的专家。这个想法并不新鲜，早在DeepSeek 2024 MoE[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1953492025719644723#ref_3">3]</a>和2022 DeepSpeedMoE[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1953492025719644723#ref_4">4]</a>论文中就已提出。</p>
</li>
<li>
<p>DeepSpeedMoE 论文首次提出了共享专家的优势，他们发现与没有共享专家相比，共享专家可以提升整体建模性能。这可能是因为多个专家无需学习常见或重复的模式，从而为它们提供了更多学习更专业模式的空间。</p>
</li>
</ol>
</li>
</ol>
<h4 id="Qwen3">Qwen3</h4>
<p>moe</p>
<p>Qwen3 也有两种 MoE 版本：30B-A3B 和 235B-A22B。为什么有些架构（例如 Qwen3）会同时提供常规（密集）和 MoE（稀疏）版本？</p>
<p>正如本文开头所述，MoE 变体有助于降低大型基础模型的推理成本。同时提供密集模型和 MoE 版本，让用户能够根据自己的目标和约束条件灵活地进行推理。</p>
<p><strong>密集模型通常更容易在各种硬件上进行微调、部署和优化。</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-5c137aa067787d1cfc74418cf9cce94c_1440w.jpg" alt="img"></p>
<p>如上图所示，DeepSeek-V3 和 Qwen3 235B-A22B 架构非常相似。值得注意的是，Qwen3 模型不再使用共享专家（早期的 Qwen 模型，例如Qwen2.5-MoE，确实使用了共享专家）。</p>
<p>遗憾的是，Qwen3 团队并未透露放弃共享专家的任何原因。如果非要我猜的话，可能是因为在他们将专家数量从2个（Qwen2.5-MoE中的设置）增加到8个（Qwen3中的设置）时，对于他们那个训练架构的稳定性来说，并没有这个必要。然后，他们通过只使用8个专家而不是8+1个专家，节省了额外的计算和内存开销。（不过，这并不能解释为什么DeepSeek-V3仍然保留了它的共享专家。）</p>
<p>Qwen3的开发者之一Junyang Lin对此做出了如下回应：</p>
<blockquote>
<p>当时，我们并没有发现共享专家有足够显著的改进，我们担心共享专家会导致推理优化问题。说实话，这个问题没有直接的答案。</p>
</blockquote>
<h4 id="Kimi-K2">Kimi K2</h4>
<p>它使用了相对较新的Muon优化器的一个变体来替代 AdamW。据我所知，这是 Muon 首次用于这种规模的生产模型，而非 AdamW（此前，它仅被证明可以扩展到 16B）。这带来了非常漂亮的训练损失曲线，这可能有助于该模型跃居上述基准测试的榜首。</p>
<p>该模型本身有 1 万亿个参数，这确实令人印象深刻。</p>
<p>它也完成了一个循环，因为 Kimi 2 使用了我们在本文开头介绍过的 DeepSeek-V3 架构，只不过他们把它做得更大了，如下图所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-2d3ced3d46518ae2033a4410e03c4add_1440w.jpg" alt="img"></p>
<p>384个专家</p>
<h4 id="Grok2-5">Grok2.5</h4>
<p>Grok 2.5 使用少量大型专家（8 个），这反映了一种较旧的趋势。如前所述，较新的设计（例如 DeepSeekMoE 论文中的设计）倾向于使用更多小型专家（Qwen3 中也存在这种情况）。</p>
<p>另一个有趣的选择是使用相当于共享专家的功能。图 32 左侧显示的附加 SwiGLU 模块充当始终在线的共享专家。它与经典的共享专家设计并不完全相同，因为它的中间维度加倍了，但思路是一样的。（我仍然觉得 Qwen3 省略了共享专家这一点很有意思，看看 Qwen4 及后续型号是否会有变化也值得关注。）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-13e18f296bfbbf3165c00ac2f3631a15_1440w.jpg" alt="img"></p>
<h3 id="5-Qwen模型每个版本之间的改进点">5.Qwen模型每个版本之间的改进点</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1902064402053695444">https://zhuanlan.zhihu.com/p/1902064402053695444</a></p>
<p>Qwen 的发展主要体现在以下几点：</p>
<ul>
<li>
<p><strong>模型规模从小到大</strong>：主要体现在：</p>
<ul>
<li>预训练数据集越来越大，从<strong>3万亿</strong>token，逐步发展到2.5版本的 <strong>18万亿</strong>，3.0版本还没公布多少万亿，总之是更多了；</li>
<li>模型参数规模：最早就是主打<strong>7B</strong>，现在就不仅仅是7B了，还包括了 <strong>235B</strong>参数；并在2.5版本开始着手验证scaling law。</li>
<li>对齐阶段SFT的数据量，最早30万条，现在也已经上百万条了。</li>
</ul>
</li>
<li></li>
<li>
<p><strong>不断考虑训练推理效率</strong>：</p>
<ul>
<li>像初代版本还用的是dense FFL，此后就演变成了 <strong>MoE</strong>模型，加快了训练和推理效率；</li>
<li>注意力机制计算也采用了 <strong>GQA</strong>技术，减少计算量；</li>
<li>扩展上下文长度也新增了 <strong>DCA</strong> 和 <strong>Yarn</strong>，总之不是蛮力扩展上下文长度，而是采用了技巧，目的也是为了节省计算资源。</li>
<li>对齐阶段，PPO 也被改成了 <strong>DPO</strong>，同样是省去了复杂的强化模型训练框架。</li>
<li>推理阶段给出了 <strong>AWQ</strong> 量化策略，同样是为了保证效果的同时，兼顾效率。</li>
</ul>
</li>
<li>
<p><strong>向数学、编程发展</strong>：众所周知，数学和编程这两样对于语言模型来说，比较难，是不少公司发力研发的目标。</p>
<ul>
<li>数据组织大幅提高数学、编程的数据占比。</li>
<li>为了控制数学、编程输出结果的稳定性、确定性，在对齐阶段也采用了offline和 online 两种方式。</li>
<li><strong>诟病</strong>：数学和编程能力确实在2.5、3.0版本中有提升，但大量用户反馈，在写报告、回答常识问题、复述一些用户问话、处理简单日常任务能力上，返回幻觉频发，参数量增加了，但效果更差了。这种情况也不是Qwen 一家如此。似乎数学和编程这类问题和日常常识问答存在一些思维方式上的矛盾。</li>
</ul>
</li>
<li>
<p><strong>深度思考</strong>：深度思考现在几乎每家模型都需要接入了，因为这个过程必不可少：</p>
<ul>
<li>深度思考其实是 decoder-only 模型弥补相对于 encoder-decoder 模型不足的一种补充；</li>
<li>深度思考也是解决复杂问题，包括编程、数学、agent 等必不可少的一关；</li>
<li>深度思考是语言表示思维的必经步骤。</li>
</ul>
</li>
</ul>
<h3 id="6-介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别">6.介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别</h3>
<h3 id="7-RAG怎么评估，指标有哪些">7.RAG怎么评估，指标有哪些</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/717985736">https://zhuanlan.zhihu.com/p/717985736</a></p>
<p>precision@k</p>
<p>recall@k</p>
<p>上面这些就是可以完全计算、量化的，还有一些nlp的指标 BLEU ROUGE等</p>
<p>参考ragas中的一些评估指标</p>
<p>correctness 相关性 helpfulness 等，llm as judge</p>
<h3 id="8-RAG如果有噪声怎么办">8.RAG如果有噪声怎么办</h3>
<p>有办法可以减少，多阶段：</p>
<p><strong>文档预处理降噪</strong></p>
<ul>
<li><strong>格式统一化</strong>：将PDF、Word等多格式文档转换为Markdown格式，便于统一处理</li>
<li><strong>结构化过滤</strong>：删除OCR识别的无效图表描述、无意义数字（如&quot;0/1&quot;转换为文字）</li>
<li><strong>冗余清理</strong>：去除大量空格、分割符等格式噪声（参考2025-09-04知乎文章）</li>
</ul>
<p><strong>2. 检索阶段去噪技术</strong></p>
<ul>
<li><strong>多阶段检索</strong>：先通过向量索引初筛，再用BGE-Reranker等模型重排（参考2025-05-09知乎文章）</li>
<li><strong>段落注入机制</strong>：将检索段落融入推理过程，增强模型辨伪能力（中科院2025-11-08研究）</li>
<li><strong>证据质量验证</strong>：使用EviNote-RAG的SEN（支持性证据笔记）标记不确定信息（2025-09-12联合研究）</li>
</ul>
<p><strong>3. 推理阶段抗噪优化</strong></p>
<ul>
<li><strong>证据质量奖励（EQR）</strong>：通过逻辑蕴含监督确保证据支撑答案（EviNote-RAG核心创新）</li>
<li><strong>自我反思机制</strong>：让模型在生成答案时同步判断证据相关性（2025-11-21网页研究）</li>
<li><strong>噪声训练增强</strong>：在SFT阶段按比例加入随机噪声文档，提升模型抗干扰能力（2025-11-23编程知识研究）</li>
</ul>
<p>openai的一个优化策略：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/704291371">https://zhuanlan.zhihu.com/p/704291371</a></p>
<h3 id="9-怎么构建SFT数据集，数据量多少，微调方式是什么">9.怎么构建SFT数据集，数据量多少，微调方式是什么</h3>
<h3 id="10-SFT数据问题不够多样化怎么办">10.SFT数据问题不够多样化怎么办</h3>
<h3 id="11-介绍一下function-calling和MCP">11.介绍一下function calling和MCP</h3>
<p>一个是模型选择、调用工具的能力</p>
<p>另一个是提供了一个标准化的方式，让模型知道有哪些工具，工具的参数、描述等等</p>
<p>最终实际上还是模型输出一个结构化的调用，然后让真正的后端去执行这个工具</p>
<h3 id="12-代码题：lc215-数组中的第-K-个最大元素">12.代码题：lc215 数组中的第 K 个最大元素</h3>
<h2 id="模板3">模板3</h2>
<h3 id="1-lora-原理，初始化，为什么，对比-sft">1.lora 原理，初始化，为什么，对比 sft</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/716893478">https://zhuanlan.zhihu.com/p/716893478</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/702629428">https://zhuanlan.zhihu.com/p/702629428</a></p>
<h3 id="2-训练网络过程的一些优化">2.训练网络过程的一些优化</h3>
<h3 id="3-batchnorm和layernorm-区别，为什么用，在哪里用">3.batchnorm和layernorm 区别，为什么用，在哪里用</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/656647661">https://zhuanlan.zhihu.com/p/656647661</a></p>
<p>batchnorm就是给一批数据的某一个feature做归一化</p>
<p>layernorm就是给某一个sequence的多个feature做归一化</p>
<p>均值为0，方差为一</p>
<h3 id="4-attention-及其变体原理，cross-attention-的-qkv-来自哪里">4.attention 及其变体原理，cross attention 的 qkv 来自哪里</h3>
<p>Attention 机制及其变体（如 Self-Attention、Cross-Attention）的核心思想是通过“查询”（Query）、“键”（Key）和“值”（Value）三部分计算加权表示。下面分别说明其原理及 Cross-Attention 中 Q、K、V 的来源。</p>
<hr>
<ol>
<li>Attention 的基本原理</li>
</ol>
<p>给定 Query ( Q )、Key ( K )、Value ( V )，Attention 的计算公式为：</p>
\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\]

<p>其中：</p>
<ul>
<li>( d_k ) 是 Key 的维度，用于缩放防止点积过大；</li>
<li>softmax 使注意力权重归一化；</li>
<li>输出是 Value 的加权和，权重由 Query 与 Key 的相似度决定。</li>
</ul>
<hr>
<ol start="2">
<li>Self-Attention（自注意力）</li>
</ol>
<ul>
<li><strong>应用场景</strong>：处理单一序列内部的关系（如 Transformer 编码器或解码器内部）；</li>
<li><strong>Q、K、V 来源</strong>：全部来自<strong>同一个输入序列</strong> ( X )；
<ul>
<li>通常通过三个可学习的线性变换（权重矩阵）得到：\[
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
\]

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-Cross-Attention（交叉注意力）">3. Cross-Attention（交叉注意力）</h3>
<ul>
<li>
<p><strong>应用场景</strong>：处理<strong>两个不同序列</strong>之间的关系，常见于 Transformer 的解码器中，用于让目标序列关注源序列（如机器翻译中，目标语言关注源语言）。</p>
</li>
<li>
<p><strong>Q、K、V 来源</strong>：</p>
<ul>
<li><strong>Query (Q)</strong>：来自<strong>目标序列</strong>（例如解码器的输入，如已生成的部分目标句子）；</li>
<li><strong>Key (K) 和 Value (V)</strong>：来自<strong>源序列</strong>（例如编码器的输出，如源语言句子的表示）；</li>
<li>数学表达（以 Transformer 解码器为例）：\[
Q = X_{\text{dec}} W_Q,\quad K = X_{\text{enc}} W_K,\quad V = X_{\text{enc}} W_V
\]

其中\( X_{\text{dec}} \)

是解码器侧的表示，\( X_{\text{enc}} \)

是编码器输出。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-为什么这样设计？">4. 为什么这样设计？</h3>
<ul>
<li><strong>Query 表示“我想关注什么”</strong>，所以由当前正在处理的序列（如目标语言）提供；</li>
<li><strong>Key/Value 表示“我可以提供什么信息”</strong>，所以由外部序列（如源语言）提供；</li>
<li>Cross-Attention 使得模型能动态地从源序列中检索与当前目标 token 最相关的信息。</li>
</ul>
<hr>
<p>总结</p>
<table>
<thead>
<tr>
<th>Attention 类型</th>
<th>Query 来源</th>
<th>Key/Value 来源</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-Attention</td>
<td>同一序列</td>
<td>同一序列</td>
</tr>
<tr>
<td>Cross-Attention</td>
<td>目标序列</td>
<td>源序列</td>
</tr>
</tbody>
</table>
<p>这种机制是 Transformer 能够有效建模跨模态（如文本-图像）或跨语言（如英-中翻译）关系的关键基础。</p>
<h3 id="5-自注意力机制是什么？计算复杂度怎么算？">5.自注意力机制是什么？计算复杂度怎么算？</h3>
<p>自注意力机制（Self-Attention）是 Transformer 模型的核心组件，用于在<strong>单个序列内部</strong>建模元素之间的相互依赖关系。它不依赖于位置或距离，能够直接捕获任意两个位置之间的关联。</p>
<hr>
<p>一、自注意力机制是什么？</p>
<p>给定一个输入序列</p>
\( X = [x_1, x_2, ..., x_n] \in \mathbb{R}^{n \times d} \)

<p>，其中：</p>
<ul>
<li>( n ) 是序列长度（token 数），</li>
<li>( d ) 是每个 token 的特征维度，</li>
</ul>
<p>自注意力通过三个可学习的线性变换生成 Query、Key、Value：</p>
\[
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
\]

<p>其中</p>
\( W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k} \)

<p>（通常 ( d_k = d_v = d ) 或 ( d/ h ) 在多头注意力中）。</p>
<p>然后计算注意力输出：</p>
\[
\text{Self-Attention}(X) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
\]

<ul>
<li>
\( QK^\top \in \mathbb{R}^{n \times n} \)

<p>表示所有 token 对之间的相似度；</p>
</li>
<li>
<p>softmax 后得到注意力权重矩阵</p>
\( A \in \mathbb{R}^{n \times n} \)

<p>；</p>
</li>
<li>
<p>每个输出 token 是所有输入 token 的加权和（权重由相关性决定）。</p>
</li>
</ul>
<hr>
<p>二、计算复杂度分析</p>
<p>假设：</p>
<ul>
<li>序列长度为 ( n )，</li>
<li>每个 token 的维度为 ( d )（通常 ( d_k = d_v = d )）。</li>
</ul>
<p>各步骤的计算复杂度如下：</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>操作</th>
<th>复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>计算 ( Q, K, V )：三个矩阵乘法 ( X W )</td>
<td>( 3 \times (n d \cdot d) = 3 n d^2 )</td>
</tr>
<tr>
<td>2</td>
<td>计算 ( Q K^\top )</td>
<td>( n \cdot d \cdot n = n^2 d )</td>
</tr>
<tr>
<td>3</td>
<td>softmax（元素级操作）</td>
<td>( O(n^2) )（通常忽略，因远小于矩阵乘）</td>
</tr>
<tr>
<td>4</td>
<td>计算 ( A V )（注意力权重 × Value）</td>
<td>( n^2 \cdot d = n^2 d )</td>
</tr>
</tbody>
</table>
<p><strong>总时间复杂度</strong>：</p>
\[
O(n d^2 + n^2 d)
\]

<p>通常在实际模型中（如 BERT、GPT），( d ) 是固定的（例如 768），而 ( n ) 可变。当序列较长时（如 ( n &gt; d )），<strong>主导项是 ( n^2 d )</strong>，即 <strong>自注意力的时间复杂度为 ( O(n^2 d) )</strong>。</p>
<p><strong>空间复杂度</strong>（主要指注意力矩阵 ( A ) 的存储）：</p>
<ul>
<li>需要存储\( QK^\top \)

或 softmax 后的\( A \in \mathbb{R}^{n \times n} \)

，</li>
<li>所以空间复杂度为\( O(n^2) \)

。</li>
</ul>
<hr>
<p>三、实际影响</p>
<ul>
<li>当 ( n = 512 )，<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>=</mo><mn>262</mn><mo separator="true">,</mo><mn>144</mn><mtext> </mtext></mrow><annotation encoding="application/x-tex">n^2 = 262,144 \
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8641em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">262</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">144</span><span class="mspace"> </span></span></span></span></span></p>
—— 可接受；</li>
<li>当 ( n = 8192 )，<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>≈</mo><mn>67</mn><mtext> </mtext><mi>m</mi><mi>i</mi><mi>l</mi><mi>l</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><annotation encoding="application/x-tex"> n^2 \approx 67 \ million
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8641em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">67</span><span class="mspace"> </span><span class="mord mathnormal">mi</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span></span></span></span></span></p>
—— 显存和计算开销剧增；</li>
<li>这也是为什么很多研究致力于 <strong>降低自注意力复杂度</strong>，如：
<ul>
<li>Sparse Attention（稀疏注意力）</li>
<li>Linformer（用低秩近似）</li>
<li>Performer（核函数近似）</li>
<li>FlashAttention（I/O 优化）</li>
</ul>
</li>
</ul>
<hr>
<p>总结</p>
<ul>
<li><strong>自注意力机制</strong>：让序列中每个元素关注所有其他元素，通过 Q、K、V 动态计算相关性；</li>
<li><strong>时间复杂度</strong>：( O(n^2 d) )；</li>
<li><strong>空间复杂度</strong>：( O(n^2) )（主要来自注意力矩阵）；</li>
<li><strong>瓶颈</strong>：长序列处理时的二次复杂度，是当前大模型上下文长度扩展的主要挑战之一。</li>
</ul>
<h3 id="6-KV-Cache的如何加速推理？">6.KV-Cache的如何加速推理？</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic4.zhimg.com/v2-f8706213a1f04fa1e41533bc0eeef601_1440w.jpg" alt="img"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-81197e811503d1ffa5f864f164127ddb_1440w.jpg" alt="img"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-cdf2a0f4e164b8b2fdfb4d65fbda8a20_1440w.jpg" alt="img"></p>
<p>在利用kv cache的时候，输入第一个token，变成embedding，然后proj成为q、k、v。假设只有一层transformer，计算完q<em>k^T</em>v之后，通过一个language head映射到词表，选择概率最大的一个token。此时序列变成2个token，继续生成，只需要第二个token-&gt;embedding，然后proj成为q、k、v，再进行注意力计算的时候，kv要和第一个token已经计算好的kv合并，然后再跟q计算attention score（即q<em>k^T</em>v），然后以此类推。<br>
每次最后生成的token是没有对应的qkv的，如果要继续生成的话，要把最后生成的token先proj成为qkv，然后把前面所有的kv与当前的kv合并，最后利用最后生成的token对应的q与合并后的kv计算attention，然后通过language head生成新的token。</p>
<p>KV-Cache（Key-Value Cache）是<strong>在自回归语言模型（如 Transformer 解码器）推理阶段</strong>用于<strong>避免重复计算</strong>、显著<strong>加速生成过程</strong>的关键优化技术。</p>
<hr>
<p>一、为什么需要 KV-Cache？</p>
<p>在自回归生成中（如 GPT 生成文本）：</p>
<ul>
<li>每次生成一个 token，需对<strong>当前所有已生成的 token</strong>（包括新 token）重新计算 Self-Attention；</li>
<li>如果不缓存，第 ( t ) 步就要重新计算前 ( t ) 个 token 的 Q、K、V —— <strong>大量重复计算</strong>。</li>
</ul>
<p><strong>举例</strong>：<br>
生成第 5 个 token 时，模型会重新计算 token 1~5 的 K、V；<br>
生成第 6 个 token 时，又重新计算 token 1~6 的 K、V —— 其中 1~5 完全重复！</p>
<hr>
<p>二、KV-Cache 的核心思想</p>
<blockquote>
<p><strong>只计算新 token 的 K、V，旧 token 的 K、V 缓存起来复用。</strong></p>
</blockquote>
<p>具体做法：</p>
<ul>
<li>在第 ( t ) 步（生成第 ( t ) 个 token）：
<ul>
<li><strong>Query</strong>：仅对当前新输入（通常是第 ( t ) 个 token）计算 ( q_t )；</li>
<li><strong>Key / Value</strong>：
<ul>
<li>新部分：计算当前 token 的 ( k_t, v_t )；</li>
<li>旧部分：从缓存中读取之前所有 token 的 ( {k_1, …, k_{t-1}}, {v_1, …, v_{t-1}} )；</li>
</ul>
</li>
<li>拼接得到完整 ( K_{1:t}, V_{1:t} )，参与注意力计算。</li>
</ul>
</li>
</ul>
<hr>
<p>三、如何加速推理？</p>
<p>✅ 1. <strong>减少计算量</strong></p>
<ul>
<li>原本每步计算 ( t ) 个 token 的 K、V（复杂度 ( O(t d^2) )）；</li>
<li>使用 KV-Cache 后，<strong>每步只计算 1 个 token 的 K、V</strong>（复杂度 ( O(d^2) )）；</li>
<li><strong>总计算量从</strong> ( O(n^2 d^2) ) <strong>降到</strong> ( O(n d^2) )（( n ) 为生成长度）。</li>
</ul>
<p>✅ 2. <strong>减少访存与 FLOPs</strong></p>
<ul>
<li>避免重复读取历史 token 并做矩阵乘；</li>
<li>虽然要维护缓存（增加内存），但<strong>大大降低每步延迟</strong>，尤其在长文本生成时效果显著。</li>
</ul>
<p>✅ 3. <strong>支持批处理（batching）优化</strong></p>
<ul>
<li>多个请求可共享相同长度的 KV-Cache 结构，便于 GPU 并行。</li>
</ul>
<hr>
<p>四、KV-Cache 的存储开销</p>
<ul>
<li>每层 Transformer 都需要缓存 K 和 V；</li>
<li>假设模型有 ( L ) 层，每层注意力头数 ( h )，每个头维度 ( d_h )，序列长度 ( n )；</li>
<li>总缓存大小 ≈\( 2 \times L \times h \times n \times d_h \times \text{bytes per param} \)

；</li>
<li>例如：Llama-7B（( L=32, h=32, d_h=128 )），生成 2048 个 token，约需 <strong>数 GB 显存</strong>。</li>
</ul>
<blockquote>
<p>📌 这也是长上下文推理显存瓶颈的主要来源。</p>
</blockquote>
<hr>
<p>五、注意事项</p>
<ul>
<li><strong>仅用于推理（inference）</strong>，训练时因并行处理整个序列，不需要 KV-Cache；</li>
<li><strong>不适用于所有模型</strong>：仅自回归解码器（如 GPT）使用；Encoder-only（如 BERT）或非自回归模型无需；</li>
<li><strong>需配合 Position Embedding / RoPE</strong>：缓存的 K、V 仍需正确的位置信息。</li>
</ul>
<hr>
<p>总结</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>无 KV-Cache</th>
<th>有 KV-Cache</th>
</tr>
</thead>
<tbody>
<tr>
<td>每步 K/V 计算量</td>
<td>( O(t d^2) )</td>
<td>( O(d^2) )</td>
</tr>
<tr>
<td>总时间复杂度</td>
<td>( O(n^2 d^2) )</td>
<td>( O(n d^2) )</td>
</tr>
<tr>
<td>是否重复计算</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>推理速度</td>
<td>慢（随长度平方增长）</td>
<td>快（线性增长）</td>
</tr>
<tr>
<td>显存开销</td>
<td>低</td>
<td>高（但可接受）</td>
</tr>
</tbody>
</table>
<p>✅ <strong>KV-Cache 用显存换时间，是现代大语言模型高效推理的基石。</strong></p>
<h3 id="7-LoRA的原理是什么？与P-Tuning、Adapter的异同点？LoRA的参数选择对模型性能有何影响？">7.LoRA的原理是什么？与P-Tuning、Adapter的异同点？LoRA的参数选择对模型性能有何影响？</h3>
<p>LoRA（Low-Rank Adaptation）是一种高效的<strong>参数高效微调</strong>（Parameter-Efficient Fine-Tuning, PEFT）方法，其核心思想是<strong>用低秩矩阵近似原始模型权重的更新</strong>，从而在只训练极少参数的情况下实现接近全参数微调的性能。</p>
<p>下面从原理、与其他方法（P-Tuning、Adapter）的异同、以及参数选择对性能的影响三方面详细说明。</p>
<hr>
<p>一、LoRA 原理</p>
<ol>
<li>基本思想</li>
</ol>
<p>在微调大型预训练模型（如 LLM）时，直接更新全部参数成本高、易过拟合。LoRA 假设：<strong>权重更新 (\Delta W) 具有低秩结构</strong>，即：</p>
\[
\Delta W = A B, \quad A \in \mathbb{R}^{d \times r}, \; B \in \mathbb{R}^{r \times k}, \; r \ll \min(d, k)
\]

<p>其中：</p>
<ul>
<li>
\(W_0 \in \mathbb{R}^{d \times k}\)

<p>是预训练权重（冻结不训练）；</p>
</li>
<li>
<p>(A, B) 是可训练的低秩分解矩阵，秩为 (r)；</p>
</li>
<li>
<p>前向传播时实际使用：</p>
\(h = x (W_0 + \Delta W) = x W_0 + x A B\)

</li>
</ul>
<blockquote>
<p>这样，<strong>只训练 (A) 和 (B)</strong>，原始 (W_0) 保持不变。</p>
</blockquote>
<ol start="2">
<li>应用位置</li>
</ol>
<p>LoRA 通常插入在：</p>
<ul>
<li>Transformer 的 <strong>Attention 模块</strong>（如 (W_q, W_k, W_v, W_o)），</li>
<li>有时也用于 <strong>MLP 层</strong>，但实践中主要用在 Attention。</li>
</ul>
<ol start="3">
<li>推理时合并</li>
</ol>
<p>训练完成后，可将</p>
\(\Delta W = AB\)

<p>加到 (W_0) 上，<strong>零开销部署</strong>（与原始模型无区别）。</p>
<hr>
<p>二、与 P-Tuning、Adapter 的异同</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心机制</th>
<th>可训练参数位置</th>
<th>是否修改模型结构</th>
<th>推理是否需额外开销</th>
<th>典型应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LoRA</strong></td>
<td>低秩分解更新权重矩阵</td>
<td>原有线性层旁路（低秩矩阵）</td>
<td>否（可合并）</td>
<td>否（可合并）</td>
<td>通用，尤其 LLM 微调</td>
</tr>
<tr>
<td><strong>Adapter</strong></td>
<td>在 FFN 或 Attention 后插入小型全连接模块（如 bottleneck）</td>
<td>新增子网络（如 down-projection + up-projection）</td>
<td>是（插入模块）</td>
<td>是（需保留 Adapter 层）</td>
<td>多任务、跨任务迁移</td>
</tr>
<tr>
<td><strong>P-Tuning v2</strong></td>
<td>在输入层或每层前添加可学习的<strong>连续 prompt 向量</strong>（软提示）</td>
<td>输入嵌入空间或每层前缀</td>
<td>是（增加 prompt token）</td>
<td>是（需保留 prompt）</td>
<td>少样本、提示学习</td>
</tr>
</tbody>
</table>
<p>关键区别：</p>
<ul>
<li>
<p><strong>LoRA vs Adapter</strong>：</p>
<ul>
<li>Adapter 修改模型结构，增加额外模块，推理时需保留；</li>
<li>LoRA 不改变前向结构，仅修改权重，可合并，<strong>部署更干净</strong>；</li>
<li>LoRA 通常在相同参数量下表现优于 Adapter。</li>
</ul>
</li>
<li>
<p><strong>LoRA vs P-Tuning</strong>：</p>
<ul>
<li>P-Tuning 仅调整<strong>输入表示</strong>（类似“软提示”），不修改模型内部权重；</li>
<li>LoRA 直接调整<strong>模型参数空间</strong>，表达能力更强；</li>
<li>P-Tuning 更适合<strong>任务提示</strong>（如 NLU），LoRA 更适合<strong>指令微调、领域适配</strong>。</li>
</ul>
</li>
</ul>
<blockquote>
<p>📌 实践中，LoRA 因其<strong>高效果、易部署、灵活性强</strong>，已成为 LLM 微调的事实标准（如 Hugging Face PEFT 库默认支持）。</p>
</blockquote>
<hr>
<p>三、LoRA 参数选择对性能的影响</p>
<p>关键超参数：</p>
<ol>
<li>
<p><strong>秩（rank）(r)</strong>：</p>
<ul>
<li>(r) 越大，表达能力越强，但参数量和过拟合风险增加；</li>
<li>常见值：8、16、32、64；</li>
<li>实验表明：<strong>r=8~32 通常足够</strong>，超过 64 收益递减；</li>
<li>小模型（如 7B）常用 r=8，大模型（70B）可用 r=64。</li>
</ul>
</li>
<li>
<p><strong>应用层位置</strong>：</p>
<ul>
<li>仅在 <strong>Attention 的 (W_q, W_v)</strong> 上加 LoRA，通常就能达到 90%+ 全微调性能；</li>
<li>加在 (W_k, W_o) 或 MLP 上收益有限，甚至有害；</li>
<li><strong>推荐策略</strong>：优先 (q, v)，必要时扩展到 (k, o)。</li>
</ul>
</li>
<li>
<p><strong>缩放因子（alpha）</strong>：</p>
<ul>
<li>LoRA 输出常带缩放：( \frac{\alpha}{r} AB )；</li>
<li>(\alpha) 控制更新幅度，类似学习率；</li>
<li>通常设 (\alpha = 2r)（如 r=8 → α=16），效果较稳定。</li>
</ul>
</li>
<li>
<p><strong>参数量占比</strong>：</p>
<ul>
<li>以 LLaMA-7B 为例：
<ul>
<li>全参数：7B；</li>
<li>LoRA（r=8，仅 q/v）：约 4M 参数（<strong>0.06%</strong>）；</li>
</ul>
</li>
<li>即使如此少的参数，也能在指令微调中接近全微调效果。</li>
</ul>
</li>
</ol>
<blockquote>
<p>⚠️ 注意：<strong>过小的 r（如 r=1~2）会严重限制模型容量</strong>，导致欠拟合；<strong>过大的 r 可能过拟合小数据集</strong>。</p>
</blockquote>
<hr>
<p>总结</p>
<ul>
<li><strong>LoRA 原理</strong>：用低秩矩阵近似权重更新，冻结主干，只训旁路；</li>
<li><strong>vs Adapter / P-Tuning</strong>：
<ul>
<li>LoRA 更高效、可合并、通用性强；</li>
<li>Adapter 需保留结构，P-Tuning 仅改输入；</li>
</ul>
</li>
<li><strong>参数选择</strong>：
<ul>
<li>rank (r) 是关键：8~32 通常最佳；</li>
<li>优先应用于 Attention 的 (W_q, W_v)；</li>
<li>合理设置 (\alpha)（如 (\alpha = 2r)）。</li>
</ul>
</li>
</ul>
<p>✅ LoRA 在<strong>低资源、多任务、快速迭代</strong>场景中极具优势，是当前大模型微调的首选 PEFT 方法。</p>
<h3 id="8-介绍下RLHF的基本流程，与DPO的差异是什么？">8.介绍下RLHF的基本流程，与DPO的差异是什么？</h3>
<p>GRPO PPO DPO: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1910019667268986241">https://zhuanlan.zhihu.com/p/1910019667268986241</a></p>
<p>DPO属于<strong>直接偏好对齐</strong>方法，是在2023年由斯坦福大学研究团队提出的偏好优化算法，主要为了解决<strong>PPO训练难度高导致不容易收敛，资源消耗大</strong>的问题。主要的方法是通过引入人类偏好数据，将在线策略优化，修改为通过二元交叉熵直接拟合人类偏好数据的离线策略。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picx.zhimg.com/v2-d222883db3171863b7790128d4f12233_1440w.jpg" alt="img"></p>
<p>RLHF vs DPO</p>
<p><strong>DPO优点</strong></p>
<ul>
<li><strong>训练流程短</strong>：RLHF的过程，需要提前训练好一个reward model，但DPO由于不需要引入<strong>reward model</strong>，因此也无需这个阶段。DPO根据预先给定的偏好数据直接进行学习，属于<strong>离线策略</strong>，不需要进行在线数据采样。</li>
<li><strong>训练资源要求低</strong>：其中RLHF需要策略模型（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=258164937&amp;content_type=Article&amp;match_order=1&amp;q=Policy+Model&amp;zhida_source=entity">Policy Model</a>）、参考模型（Reference Model）、奖励模型（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=258164937&amp;content_type=Article&amp;match_order=1&amp;q=Reward+Model&amp;zhida_source=entity">Reward Model</a>）、价值模型（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=258164937&amp;content_type=Article&amp;match_order=1&amp;q=Value+Model&amp;zhida_source=entity">Value Model</a>），而DPO仅需要前<strong>两个模型</strong>，并且<strong>参考模型属于可选加载</strong>，可以通过将参考模型的输出结果预先录制好，在训练时就可以不加载。因此对于训练资源显存等要求低。</li>
<li><strong>稳定性高</strong>：DPO属于<strong>有监督学习</strong>（通过概率匹配直接优化策略），摆脱了强化学习由于高方差带来的不稳定（由于奖励稀疏or噪声造成）。DPO可以通过人类偏好数据，用二元交叉熵对策略进行优化，而不需要多次进行在线数据采样进行优化。其中， yw为偏好数据，yl为非偏好数据。</li>
<li><strong>训练难度低</strong>：其中DPO仅需要关注<strong>学习率和偏好权重</strong>β ，而RLHF需要同时关注策略更新幅度、奖励模型置信度等。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-33137b12aebec247601bee7d89183791_1440w.jpg" alt="img"></p>
<p><strong>这个loss是咋来的？</strong></p>
<p><strong>DPO缺点</strong></p>
<ul>
<li><strong>容易过拟合</strong>：DPO由于缺少reward model的泛化，因此容易直接拟合人类偏好数据，造成过拟合。</li>
<li><strong>需求更大标注数据量</strong>：相比PPO等，DPO的效果表现更依赖标注数据量。</li>
<li><strong>多任务适配较难</strong>：由于DPO仅依赖数据，所以如果需要进行多任务的对比，则需要从头标注涉及到多个维度的数据，但是在线策略的方法可以通过单个维度的数据，训练不同的多个reward model，引入多维度的奖励。</li>
</ul>
<table>
<thead>
<tr>
<th>特性</th>
<th>DPO</th>
<th>传统 PPO + RM</th>
</tr>
</thead>
<tbody>
<tr>
<td>是否需要奖励模型</td>
<td>❌ 不需要</td>
<td>✅ 需要</td>
</tr>
<tr>
<td>是否需要强化学习</td>
<td>❌ 不需要</td>
<td>✅ 需要（PPO）</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>高（标准有监督学习）</td>
<td>低（RL 不稳定）</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>低（几十行代码）</td>
<td>高（多模型、值函数、clip 等）</td>
</tr>
<tr>
<td>训练效率</td>
<td>高（可批量训练）</td>
<td>低（需采样、策略梯度）</td>
</tr>
<tr>
<td>性能</td>
<td>通常优于或持平 PPO</td>
<td>基线</td>
</tr>
</tbody>
</table>
<p>**GRPO vs PPO **</p>
<p>为了在PPO和DPO之间取得平衡，deepseek提出了GRPO（<strong>群组相对优化策略</strong>），a在一定程度上能够通过去掉价值模型<strong>Value Model</strong>，缓解PPO对于显存的瓶颈，确保策略更新的稳定性和高效性；同时保留了<strong>Reward Model</strong>，避免了DPO因为直接拟合人类偏好数据，而容易造成的过拟合和效果不佳。</p>
<p>其中GRPO跟PPO的重要区别，主要是去掉了Value Model，同时使用Policy Model的多个output采样的Reward Model输出的多个奖励的平均值作为优势函数。</p>
<p><strong>一、优势函数不同</strong></p>
<p><strong>二、奖励值的归一化方式不同</strong></p>
<p><strong>三、KL散度的作用范围不同</strong></p>
<p>KL散度在<strong>PPO</strong>是放在奖励函数中。在<strong>GRPO</strong>的目标函数直接放在了损失函数</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/v2-666f326c43006ac1679437d4542c0840_1440w.jpg" alt="img"></p>
<p>DPO vs RLHF</p>
<ul>
<li>**训练流程：**其中DPO因为不依赖Reward Model，所以只有一个训练流程，而PPO、GRPO等在线策略需要先训练Reward Model，再进行对齐，需要两个阶段。</li>
<li>**显卡资源需求：**对于显卡的需求PPO（加载4个模型）&gt;GRPO（加载3个模型）&gt;DPO（加载1个必选模型+1个可选模型）</li>
<li>**对样本依赖：**其中PPO、GRPO因为通过Reward Model来进行对齐，有一定的泛化作用，因此对样本标注的精度和数据量依赖相对较小；DPO与之相反。</li>
<li>**灵活扩展性：**当涉及到多个业务场景时，其中PPO、GRPO可以通过多个Reward Model来进行灵活的扩展，而不需要从头标注多业务维度的人工偏向数据；DPO则需要重新构建数据，整体的灵活性和可扩展性较差。</li>
</ul>
<h3 id="9-分布式训练中的TP、PP、DP分别是什么？">9.分布式训练中的TP、PP、DP分别是什么？</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1904506837543420662">https://zhuanlan.zhihu.com/p/1904506837543420662</a></p>
<h3 id="10-flash-attention的原理是什么？">10.flash-attention的原理是什么？</h3>
<p>作用：加速注意力计算！！</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/676655352">https://zhuanlan.zhihu.com/p/676655352</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/668888063">https://zhuanlan.zhihu.com/p/668888063</a></p>
<p>看这个简单些：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/714881594">https://zhuanlan.zhihu.com/p/714881594</a></p>
<p>FlashAttention的核心原理是将输入QKV分块，并保证每个块能够在<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=246992481&amp;content_type=Article&amp;match_order=1&amp;q=SRAM&amp;zhida_source=entity">SRAM</a>（一级缓存）上完成注意力操作，并将结果更新回<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=246992481&amp;content_type=Article&amp;match_order=1&amp;q=HBM&amp;zhida_source=entity">HBM</a>，从而降低对高带宽内存（HBM）的读写操作。总之，FlashAttention从GPU的内存读写入手，减少了内存读写量，从而实现了2~4倍的速度提升。</p>
<p>FlashAttention <strong>has slightly higher FLOP count</strong> than naive attention due to recomputation, but reduces data movement dramatically.</p>
<p>FlashAttention 是一种<strong>高效、内存感知的注意力机制实现</strong>，旨在<strong>减少 GPU 显存访问（I/O）开销</strong>，从而<strong>加速注意力计算并降低显存占用</strong>。它不是改变注意力公式，而是通过<strong>算法重排与分块计算</strong>，在不损失精度的前提下显著提升性能。</p>
<p>SRAM&gt;HBM&gt;DRAM的速度</p>
<hr>
<p>一、背景：标准注意力的瓶颈</p>
<p>标准 Self-Attention 计算如下（忽略 softmax 缩放）：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup><mo stretchy="false">)</mo><mspace width="1em"/><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup><mspace linebreak="newline"></mspace><mtext>Output</mtext><mo>=</mo><mi>A</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">A = \text{softmax}(Q K^\top) \quad \in \mathbb{R}^{n \times n} \\
\text{Output} = A V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8213em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Output</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p>其中：</p>
<ul>
<li>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q, K, V \in \mathbb{R}^{n \times d}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8991em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>(n)：序列长度（如 2048）</p>
</li>
<li>
<p>(d)：特征维度（如 128）</p>
</li>
</ul>
<p>问题：</p>
<ol>
<li><strong>中间矩阵 (QK^\top) 和 (A) 需要 (O(n^2)) 显存</strong>；</li>
<li><strong>GPU 高带宽显存</strong>（HBM）；</li>
<li>即使计算快，<strong>I/O 成为瓶颈</strong>（“memory-bound” 而非 “compute-bound”）。</li>
</ol>
<hr>
<p>二、FlashAttention 的核心思想</p>
<blockquote>
<p><strong>不显式构造完整的 (QK^\top) 或 (A) 矩阵，而是通过分块</strong>（tiling）</p>
</blockquote>
<p>具体来说，FlashAttention 利用两个关键技术：</p>
<p>✅ 1. <strong>分块计算</strong>（Tiling / Blocking）</p>
<ul>
<li>将 (Q, K, V) 沿序列维度（(n)）切分为小块（tiles）；</li>
<li>每次只加载一小块到<strong>高速片上 SRAM</strong>（如 GPU 的 shared memory）；</li>
<li>在 SRAM 内完成局部 (q_i k_j^\top)、softmax、与 (v_j) 的乘积累加；</li>
<li>避免将完整的 (n \times n) 矩阵写入/读出 HBM。</li>
</ul>
<p>✅ 2. <strong>在线 Softmax 技巧</strong>（Online Softmax）</p>
<ul>
<li>Softmax 不能直接分块计算（因需全局最大值和归一化）；</li>
<li>FlashAttention 使用<strong>数值稳定的在线归约方法</strong>：
<ul>
<li>
<p>对每个 query 块，维护局部最大值 (m) 和局部和 (l)；（safe-softmax，减去最大值）</p>
</li>
<li>
<p>合并不同 key 块时，动态更新：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>m</mi><mtext>new</mtext></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>m</mi><mtext>old</mtext></msub><mo separator="true">,</mo><msub><mi>m</mi><mtext>curr</mtext></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><msub><mi>l</mi><mtext>new</mtext></msub><mo>=</mo><msub><mi>l</mi><mtext>old</mtext></msub><msup><mi>e</mi><mrow><msub><mi>m</mi><mtext>old</mtext></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>+</mo><msub><mi>l</mi><mtext>curr</mtext></msub><msup><mi>e</mi><mrow><msub><mi>m</mi><mtext>curr</mtext></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">m_{\text{new}} = \max(m_{\text{old}}, m_{\text{curr}}) \\
l_{\text{new}} = l_{\text{old}} e^{m_{\text{old}} - m_{\text{new}}} + l_{\text{curr}} e^{m_{\text{curr}} - m_{\text{new}}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">new</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">old</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">curr</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">new</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9713em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">old</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">old</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">new</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9713em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">curr</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8213em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">curr</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">new</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>最终结果等价于完整 softmax，但无需存储完整 (A)。</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>这样，<strong>中间注意力矩阵 (A) 从未完整存在于 HBM 中</strong>，只在 SRAM 中临时存在并立即用于计算 (AV)。</p>
</blockquote>
<hr>
<p>三、优势</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>标准 Attention</th>
<th>FlashAttention</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>时间复杂度</strong></td>
<td>(O(n^2 d))</td>
<td>(O(n^2 d))（但常数小）</td>
</tr>
<tr>
<td><strong>显存复杂度</strong></td>
<td>(O(n^2 + nd))</td>
<td>(O(nd))（<strong>不存 (A)</strong>）</td>
</tr>
<tr>
<td><strong>HBM 读写次数</strong></td>
<td>高（多次读写 (A)）</td>
<td><strong>极低</strong>（只读 (Q,K,V)，只写 Output）</td>
</tr>
<tr>
<td><strong>实际速度</strong></td>
<td>慢（I/O 瓶颈）</td>
<td><strong>快 2–5 倍</strong>（尤其长序列）</td>
</tr>
<tr>
<td><strong>支持长上下文</strong></td>
<td>受限（显存爆炸）</td>
<td>更高（如 8K、16K、32K）</td>
</tr>
</tbody>
</table>
<hr>
<p>四、FlashAttention-2 的改进（2023）</p>
<p>在 FlashAttention 基础上进一步优化：</p>
<ul>
<li>更优的线程块调度；</li>
<li>减少 shared memory 同步；</li>
<li>更好的并行性；</li>
<li><strong>速度再提升 1.5–2 倍</strong>，接近理论算力上限。</li>
</ul>
<hr>
<p>五、实际应用</p>
<ul>
<li><strong>主流 LLM 框架默认启用</strong>：
<ul>
<li>Hugging Face Transformers（通过 <code>use_flash_attention_2=True</code>）；</li>
<li>vLLM、TensorRT-LLM、Llama.cpp（部分支持）；</li>
<li>PyTorch 2.0+ 内置 <code>torch.nn.functional.scaled_dot_product_attention</code> 自动 fallback 到 FlashAttention（若硬件支持）。</li>
</ul>
</li>
<li><strong>要求</strong>：
<ul>
<li>GPU 架构 ≥ Ampere（如 A100、RTX 3090/4090）；</li>
<li>安装 <code>flash-attn</code> 库（CUDA 扩展）。</li>
</ul>
</li>
</ul>
<hr>
<p>六、注意事项</p>
<ul>
<li><strong>仅加速计算，不改变模型结构或结果</strong>（数值误差在可接受范围）；</li>
<li><strong>对短序列</strong>（如 (n &lt; 512)），收益有限；</li>
<li><strong>不直接降低算法复杂度</strong>（仍是 (O(n^2))），但通过 I/O 优化使长序列可行；</li>
<li><strong>与 KV Cache 兼容</strong>：推理时仍可使用 KV Cache + FlashAttention 加速每步 attention。</li>
</ul>
<hr>
<p>s总结</p>
<blockquote>
<p><strong>FlashAttention 是一种 I/O 感知的注意力实现，通过分块计算 + 在线 softmax，在不改变数学定义的前提下，大幅减少显存访问和占用，从而加速训练与推理，尤其对长上下文场景至关重要。</strong></p>
</blockquote>
<p>它是现代大模型高效训练/推理的<strong>基础设施级优化</strong>，已被广泛集成到主流框架中。</p>
<h2 id="模板4">模板4</h2>
<h3 id="1-训练数据，有没有做数据处理与增强的工作">1.训练数据，有没有做数据处理与增强的工作</h3>
<p>这里的数据会进行扩写</p>
<h3 id="4-在什么机器上训练，时间，数据量大小">4.在什么机器上训练，时间，数据量大小</h3>
<p>跨机器，4卡A100，tp=2，时间很快，几条，ms-swift</p>
<h3 id="5-rag中怎么做的pdf解析，对pdf里面的图片，表格数据怎么处理的，怎么编码的，检索，召回的时候都做了哪些操作，混合检索的时候的权重怎么处理的，有没有消融实现对比">5.rag中怎么做的pdf解析，对pdf里面的图片，表格数据怎么处理的，怎么编码的，检索，召回的时候都做了哪些操作，混合检索的时候的权重怎么处理的，有没有消融实现对比</h3>
<p>最佳实践：<strong>分层解析 + 结构感知</strong></p>
<table>
<thead>
<tr>
<th>内容类型</th>
<th>工具/方法</th>
<th>输出形式</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>文本</strong>（含格式）</td>
<td><code>pymupdf</code>（PyMuPDF）、<code>pdfplumber</code></td>
<td>保留段落、标题层级、字体大小、位置坐标</td>
</tr>
<tr>
<td><strong>表格</strong></td>
<td><code>camelot-py</code>（PDF 线条表）、<code>table-transformer</code>（无框线表）</td>
<td>转为 <strong>Markdown 表格</strong> 或 <strong>结构化 JSON</strong></td>
</tr>
<tr>
<td><strong>图片/图表</strong></td>
<td><code>pymupdf</code> 提取图像 + <strong>OCR/多模态理解</strong></td>
<td>图像文件 + <strong>Caption</strong>（由多模态模型生成）</td>
</tr>
<tr>
<td><strong>公式/LaTeX</strong></td>
<td><code>latex-ocr</code>（Pix2Text）</td>
<td>转为 LaTeX 字符串</td>
</tr>
<tr>
<td><strong>版面分析</strong></td>
<td><code>unstructured</code> + <code>layoutparser</code></td>
<td>返回 <code>ElementType</code>（text/table/figure/title）</td>
</tr>
</tbody>
</table>
<blockquote>
<p>📌 <strong>关键原则</strong>：<strong>不丢弃任何信息，但转化为可检索的文本形式</strong>。</p>
</blockquote>
<p>caption这里要随着原文档也塞入向量数据库，base64编码了，有个映射关系</p>
<p>文档、表格统一转化成markdown格式。经过最开始的几个实验发现转化成markdown格式对于解析表格有作用。</p>
<p>-&gt;tabulate。</p>
<p>分块：langchain.RecursiveCharacterTextSplitter</p>
<p>rrf</p>
<p>消融实验做了部分</p>
<p>还有agentic rag。</p>
<p>混合召回：语义召回（相似度）+ 图召回+ 关键词召回。</p>
<p>涉及到多跳的仅靠一次查询是不行的。</p>
<p>召回：rrf/融合加权，这里rrf+reranker，一个参数</p>
<p>问题是这里rerank每次都要发起多次API推理。</p>
<p>多阶段重排：向量召回+图召回+关键词召回top30，RRF融合top15, reranker重排</p>
<h3 id="7-sft与rag对比">7.sft与rag对比</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1970490096991081408">https://zhuanlan.zhihu.com/p/1970490096991081408</a></p>
<h3 id="9-agent方面有哪些了解">9.agent方面有哪些了解</h3>
<p>框架 langchain langgraph autogen</p>
<p>记忆</p>
<p>上下文压缩</p>
<p>工具</p>
<p>mcp</p>
<h2 id="模板5">模板5</h2>
<h3 id="1-两三句介绍下agent以及当前的挑战">1. 两三句介绍下agent以及当前的挑战</h3>
<p>Agent（智能体）是指能够感知环境、自主规划并执行任务以达成目标的大模型系统，通常具备记忆、工具调用、推理和自我反思等能力。当前主要挑战包括：<strong>可靠性不足</strong>（如幻觉、工具误用）、<strong>复杂任务规划能力有限</strong>、<strong>多步执行中的错误累积</strong>，以及<strong>缺乏统一评估基准和高效训练范式</strong>。</p>
<h3 id="2-transformer架构-有哪些机制">2. transformer架构 有哪些机制</h3>
<p>Transformer 架构是自然语言处理（NLP）中一种革命性的神经网络结构，最早由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。其核心在于完全基于注意力机制，摒弃了传统的循环（RNN）和卷积（CNN）结构。Transformer 的主要机制包括以下几个关键组成部分：</p>
<hr>
<ol>
<li><strong>自注意力机制（Self-Attention / Scaled Dot-Product Attention）</strong></li>
</ol>
<ul>
<li>
<p>允许模型在处理每个词时关注输入序列中的其他所有词。</p>
</li>
<li>
<p>通过计算 Query (Q)、Key (K)、Value (V) 三个向量来实现：</p>
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

</li>
<li>
<p>“Scaled” 是因为点积可能很大，导致 softmax 梯度消失，因此除以 (\sqrt{d_k}) 进行缩放。</p>
</li>
</ul>
<hr>
<ol start="2">
<li><strong>多头注意力（Multi-Head Attention）</strong></li>
</ol>
<ul>
<li>
<p>将自注意力机制并行地应用多次（多个“头”），每个头学习不同的注意力子空间。</p>
</li>
<li>
<p>将多个头的输出拼接后通过一个线性变换得到最终输出：</p>
[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\]

</li>
<li>
<p>增强模型对不同位置、不同语义关系的建模能力。</p>
</li>
</ul>
<hr>
<ol start="3">
<li><strong>位置编码（Positional Encoding）</strong></li>
</ol>
<ul>
<li>
<p>因为 Transformer 没有像 RNN 那样的顺序处理机制，需显式加入位置信息。</p>
</li>
<li>
<p>通常使用正弦和余弦函数编码位置：</p>
\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]

</li>
<li>
<p>也可使用可学习的位置嵌入（如 BERT 中）。</p>
</li>
</ul>
<hr>
<ol start="4">
<li><strong>残差连接（Residual Connection）与层归一化（Layer Normalization）</strong></li>
</ol>
<ul>
<li>
<p>每个子层（如多头注意力、前馈网络）后都接残差连接和 LayerNorm：</p>
\[
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
\]

</li>
<li>
<p>有助于缓解深层网络训练中的梯度消失问题，并稳定训练。</p>
</li>
</ul>
<hr>
<ol start="5">
<li><strong>前馈神经网络（Position-wise Feed-Forward Network）</strong></li>
</ol>
<ul>
<li>
<p>每个位置独立地通过一个两层全连接网络：</p>
\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\]

</li>
<li>
<p>通常包含 ReLU 激活函数，且同一层对所有位置共享参数（但不同层参数不同）。</p>
</li>
</ul>
<hr>
<ol start="6">
<li><strong>编码器-解码器结构（Encoder-Decoder Architecture）</strong></li>
</ol>
<ul>
<li><strong>编码器（Encoder）</strong>：由 N 个相同层堆叠而成，每层包含多头自注意力 + 前馈网络。</li>
<li><strong>解码器（Decoder）</strong>：也由 N 层组成，但每层包含：
<ol>
<li><strong>掩码多头自注意力</strong>（防止看到未来 token）；</li>
<li><strong>编码器-解码器注意力</strong>（Query 来自解码器，Key/Value 来自编码器）；</li>
<li>前馈网络。</li>
</ol>
</li>
<li>解码器通过自回归方式生成输出（一次一个 token）。</li>
</ul>
<hr>
<ol start="7">
<li><strong>掩码机制（Masking）</strong></li>
</ol>
<ul>
<li><strong>Padding Mask</strong>：忽略输入中的填充 token（如 <code>&lt;pad&gt;</code>）。</li>
<li><strong>Look-ahead Mask（因果掩码）</strong>：在解码器中，防止当前位置关注未来位置的 token，保证自回归性质。</li>
</ul>
<hr>
<p>这些机制共同使得 Transformer 能够高效地并行处理长序列、捕捉长距离依赖，并成为现代大模型（如 BERT、GPT、T5、LLaMA 等）的基础架构。</p>
<h3 id="3-解释下交叉注意力机制">3. 解释下交叉注意力机制</h3>
<p>模板3 问题3</p>
<h3 id="4-介绍下ppo-dpo-grpo算法">4. 介绍下ppo dpo grpo算法</h3>
<p>模板3 问题8</p>
<p>这里grpo单独提出来说一下</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>J</mi><mrow><mi>G</mi><mi>R</mi><mi>P</mi><mi>O</mi></mrow></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>q</mi><mo>∼</mo><mi>P</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>∼</mo><msub><mi>π</mi><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo stretchy="false">(</mo><mi>O</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><mfrac><mn>1</mn><mi>G</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><msub><mi>o</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant="normal">∣</mi><msub><mi>o</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow></munderover><mrow><mo fence="true">(</mo><msub><mi>r</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>−</mo><mi>β</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>π</mi><mi>θ</mi></msub><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>π</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">J_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{old}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left( r_{i,t}(\theta) \hat{A}_{i,t} - \beta D_{KL}(\pi_\theta || \pi_{ref}) \right) \right]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0962em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">GRPO</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.4618em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">Q</span><span class="mclose mtight">)</span><span class="mpunct mtight">,</span><span class="mopen mtight">{</span><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8329em;"><span style="top:-2.1777em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3223em;"><span></span></span></span></span></span></span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4638em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.667em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="3.600em" viewBox="0 0 667 3600"><path d="M403 1759 V84 H666 V0 H319 V1759 v0 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v0 v1759 h84z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">G</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.961em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.386em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">A</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.667em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="3.600em" viewBox="0 0 667 3600"><path d="M347 1759 V0 H0 V84 H263 V1759 v0 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v0 v1759 h84z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>举例假设：</p>
<ul>
<li>q = “翻译：Hello world”</li>
<li>G = 2 个输出：o₁ = “你好世界”，o₂ = “哈喽世界”</li>
<li>|o₁| = 4，|o₂| = 4（按字计）</li>
<li>β = 0.01</li>
<li>π_ref 是旧策略，例如在“你”字处概率为 0.8，在“哈”字处概率为 0.7</li>
<li>当前策略 π_θ 在“你”字处概率为 0.9，在“哈”字处概率为 0.6</li>
</ul>
<p>在 t=1 时刻（第一个字）：</p>
<ul>
<li>假设 reward model 给出 r₁,₁ = 1.0（“你”好），r₂,₁ = 0.8（“哈”稍差）</li>
<li>优势估计：A₁,₁ = 0.6，A₂,₁ = 0.3 → 平均优势 = 0.45 → 相对优势：Â₁,₁ = 0.15，Â₂,₁ = -0.15</li>
<li>KL 项：D_KL(π_θ || π_ref) 在 t=1 位置约为 0.02（粗略估算）</li>
</ul>
<p>则 t=1 的贡献为：</p>
<ul>
<li>o₁: 1.0 × 0.15 - 0.01 × 0.02 = 0.15 - 0.0002 ≈ 0.1498</li>
<li>o₂: 0.8 × (-0.15) - 0.01 × 0.02 = -0.12 - 0.0002 ≈ -0.1202</li>
</ul>
<p>然后对所有 t 和所有 i 求平均，得到最终目标 J_GRPO(θ)，我们通过梯度上升最大化它。</p>
<h3 id="5-grpo的loss怎么计算的-数据用的什么">5. grpo的loss怎么计算的 数据用的什么</h3>
<p>。。不会</p>
<h3 id="6-deepresearch和强化学习怎么结合应用">6.deepresearch和强化学习怎么结合应用</h3>
<p>webdancer</p>
<p>奖励函数设计：开放域问题reward难以量化，其他的到都好说，套用现成的一个算法就行。</p>
<p>有研究就是用的LLM as reward judge（kimi-research 用o3-mini评估答案正确性）</p>
<h3 id="7-解释下topk-topp的实现原理">7. 解释下topk topp的实现原理</h3>
<p>略</p>
<h3 id="8-为什么现在大模型都是decoder架构">8. 为什么现在大模型都是decoder架构</h3>
<h2 id="模板5-2">模板5</h2>
<h3 id="损失函数设计">损失函数设计</h3>
<h3 id="LoRA吟唱">LoRA吟唱</h3>
<h3 id="手撕MHA">手撕MHA</h3>
<h3 id="看你除以了根号k-有什么作用">看你除以了根号k 有什么作用</h3>
<h3 id="梯度消失和梯度爆炸-如何缓解">梯度消失和梯度爆炸 如何缓解</h3>
<h3 id="QKV代表什么-说说理解">QKV代表什么 说说理解</h3>
<h3 id="如果QK变成同一个矩阵你觉得有什么影响">如果QK变成同一个矩阵你觉得有什么影响</h3>
<h3 id="除了LoRA还有什么微调的方法">除了LoRA还有什么微调的方法</h3>
<h2 id="模板6">模板6</h2>
<h3 id="先做个自我介绍吧，可以结合你之前的工作经历聊聊。">先做个自我介绍吧，可以结合你之前的工作经历聊聊。</h3>
<h3 id="你对-SFT（监督微调）中的-scaling-law-有了解吗？在实际训练中，你遇到过哪些比较大的困难？">你对 SFT（监督微调）中的 scaling law 有了解吗？在实际训练中，你遇到过哪些比较大的困难？</h3>
<h3 id="在模型训练时，如果发现-advantage-或者-loss-突然变成-0，一般可能是什么原因导致的？">在模型训练时，如果发现 advantage 或者 loss 突然变成 0，一般可能是什么原因导致的？</h3>
<h3 id="在构建-AI-Agent-时，它的记忆（Memory）机制通常是怎么设计的？">在构建 AI Agent 时，它的记忆（Memory）机制通常是怎么设计的？</h3>
<h3 id="当模型出现-bad-case-时，你一般会怎么分析？后续会采取哪些措施来改进？">当模型出现 bad case 时，你一般会怎么分析？后续会采取哪些措施来改进？</h3>
<h3 id="如果需要为特定领域的文本训练一套-Embedding，你会怎么做？">如果需要为特定领域的文本训练一套 Embedding，你会怎么做？</h3>
<h3 id="聊聊你实习时团队的组成和分工吧，大家是怎么协作的？">聊聊你实习时团队的组成和分工吧，大家是怎么协作的？</h3>
<h3 id="你对大模型分布式训练的底层了解多少？比如数据并行（DP）、张量并行（TP）这些，看过-Megatron-这类框架的源码吗？">你对大模型分布式训练的底层了解多少？比如数据并行（DP）、张量并行（TP）这些，看过 Megatron 这类框架的源码吗？</h3>
<h3 id="核心代码模式算法题：二维数组中的查找">核心代码模式算法题：二维数组中的查找</h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Roger-Lv</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/">http://example.com/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Roger-Lv's space</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/agent/">agent</a><a class="post-meta__tags" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/">多模态</a><a class="post-meta__tags" href="/tags/llm/">llm</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/webdancer.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">WebDancer:Towards Autonomous Information Seeking Agency</div></div><div class="info-2"><div class="info-item-1">WebDancer: Towards Autonomous Information Seeking Agency 论文标题：WebDancer: Towards Autonomous Information Seeking Agency 论文链接：https://arxiv.org/pdf/2505.22648 论文代码：https://github.com/Alibaba-NLP/DeepResearch 这篇论文介绍了一个基于ReAct范式的网络智能体——WebDancer，通义团队透过训练赋予其自主寻求信息的能力。通义团队的训练流程主要有四个步骤，构造问答对、获得高质量轨迹、监督微调和强化学习。 问答对构造 不同于之前的简单的2到3步就能解决的问答问题，通义团队这里主要想构造的是那些可以激发模型多步推理、目标分解、交互等能力的问答对数据，因此希望对多跳推理的广度和深度都进行扩展。为此，他们提出了两个问答对数据集——CRAWLQA和E2HQA。 CRAWLQA问答对的获取跟之前WebWalkerQA数据集的构造很类似，都是从一个根网页出发递归浏览其中链接指向的网页，基于收集的...</div></div></div></a><a class="pagination-related" href="/2025/12/15/2025-12-15-%E5%A4%9A%E6%A8%A1%E6%80%81RAG%E6%A3%80%E7%B4%A2ColPail%E5%92%8CDSE/" title="多模态RAG检索"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/mrag.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">多模态RAG检索</div></div><div class="info-2"><div class="info-item-1">多模态RAG检索 ColPali和DSE解读：https://zhuanlan.zhihu.com/p/826088920 colbert模型(ColPali)：在视觉encoder，也是利用多模态的视觉大模型来生成图片端的向量，但不仅仅只生成单个向量。而是利用VIT的patch embedding，来生成多个向量。直觉上确实是会有收益，因为一整页的pdf，你就压缩在一个固定维度的向量中，那肯定有信息损失，而且以patch为单位生成embedding，真的很make sensen，比文本的colbert都make sense。   实践：https://zhuanlan.zhihu.com/p/1975243477060167021  加载所有文档，使用 http://unstructured.io 等文档加载器（document loader）提取文本块、图像和表格。 如需转换，*将 HTML 表格转为 Markdown 格式* —— 这种格式对大型语言模型（LLM）通常非常有效。 将每个文本块、图像和表格传入 GPT-4o 等多模态大型语言模型（multimodal LLM）...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/12/09/2025-12-09-Agent%E6%A1%86%E6%9E%B6%E9%9B%86%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E8%83%BD%E5%8A%9B/" title="Agent框架集成多模态能力底层实现"><div class="cover" style="background: /img/cover/langgraph.jepg"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-09</div><div class="info-item-2">Agent框架集成多模态能力底层实现</div></div><div class="info-2"><div class="info-item-1">Agent框架集成多模态能力底层实现 该项目处理多模态RAG返回图片的完整流程： 架构概述 该项目采用分层架构处理多模态RAG：  前端接口层：通过schema.py中的ImageContent和ImageUrl模型支持base64和HTTPS两种图片URL格式 RAG核心层：rag.py中的RagClient提供统一的向量检索接口 多模态嵌入层：multi_model.py中的AliyunEmbeddings使用阿里云DashScope的多模态嵌入API 数据存储层：使用Qdrant向量数据库存储图片和文本的嵌入向量  图片处理流程 1. 图片存储阶段 在feishu-crawler子项目中，图片处理流程如下：  图片下载：DownloadImageTransform从飞书下载图片到本地文件系统 图片摘要生成：GenerateImageSummaryTransform使用VLLM模型为图片生成文字描述 多模态嵌入：EmbedImageTransform调用MultiModelEmbedder生成图片+文字的联合嵌入向量 向量存储：将base64编码的图片数据、文字描述和嵌入向量...</div></div></div></a><a class="pagination-related" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/tongsearch.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="info-item-2">TongSearch-QR:Reinforced Query Reasoning for Retrieval</div></div><div class="info-2"><div class="info-item-1">TongSearch-QR: Reinforced Query Reasoning for Retrieval 这篇论文《TongSearch-QR: Reinforced Query Reasoning for Retrieval》提出了一种面向推理密集型检索（reasoning-intensive retrieval）任务的新型查询推理与重写模型家族，旨在解决传统信息检索方法在处理复杂、需要多跳推理的查询时性能不足的问题。  一、问题背景 传统信息检索（IR）方法（如 BM25、稠密向量检索）依赖词法匹配或语义相似度，在一般检索任务上表现良好。但在以下场景中表现不佳：  用户问题隐含深层意图（如“找一个可替代函数 Funca 的函数 Funcb”）； 相关文档未显式提及原问题中的关键词； 需要推理链（reasoning chain）才能连接查询与文档。  这类任务被称作 推理密集型检索（reasoning-intensive retrieval），如 BRIGHT 基准测试所定义。  二、现有方法及其局限 1. 大语言模型（LLM）提示工程  使用 GPT-4、LLaMA3-...</div></div></div></a><a class="pagination-related" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/webdancer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-18</div><div class="info-item-2">WebDancer:Towards Autonomous Information Seeking Agency</div></div><div class="info-2"><div class="info-item-1">WebDancer: Towards Autonomous Information Seeking Agency 论文标题：WebDancer: Towards Autonomous Information Seeking Agency 论文链接：https://arxiv.org/pdf/2505.22648 论文代码：https://github.com/Alibaba-NLP/DeepResearch 这篇论文介绍了一个基于ReAct范式的网络智能体——WebDancer，通义团队透过训练赋予其自主寻求信息的能力。通义团队的训练流程主要有四个步骤，构造问答对、获得高质量轨迹、监督微调和强化学习。 问答对构造 不同于之前的简单的2到3步就能解决的问答问题，通义团队这里主要想构造的是那些可以激发模型多步推理、目标分解、交互等能力的问答对数据，因此希望对多跳推理的广度和深度都进行扩展。为此，他们提出了两个问答对数据集——CRAWLQA和E2HQA。 CRAWLQA问答对的获取跟之前WebWalkerQA数据集的构造很类似，都是从一个根网页出发递归浏览其中链接指向的网页，基于收集的...</div></div></div></a><a class="pagination-related" href="/2025/12/04/2025-12-04-LangGraph-%E4%B8%AD-checkpoint_id-%E7%9A%84%E6%9B%B4%E6%96%B0%E6%97%B6%E6%9C%BA%EF%BC%9A%E6%AF%8F%E4%B8%AA%E5%AF%B9%E8%AF%9D%E8%BD%AE%E6%AC%A1%E8%BF%98%E6%98%AF%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E6%B5%81%E8%BD%AC%EF%BC%9F/" title="LangGraph 中 checkpoint_id 的更新时机：每个对话轮次还是每个节点流转？"><div class="cover" style="background: /img/cover/langgraph.jepg"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-04</div><div class="info-item-2">LangGraph 中 checkpoint_id 的更新时机：每个对话轮次还是每个节点流转？</div></div><div class="info-2"><div class="info-item-1">LangGraph 中 checkpoint_id 的更新时机：每个对话轮次还是每个节点流转？ 在使用 LangGraph 构建多轮对话或工作流时，我们经常会遇到 checkpoint（检查点）的概念。每个检查点都有一个唯一的 checkpoint_id，用于标识该次状态快照。一个常见的问题是：checkpoint_id 是在每个对话轮次更新一次，还是在节点（node）之间流转时就会更新一次？ 本文将通过分析 LangGraph 源码（基于 langgraph==0.2.0 左右版本）来回答这个问题，并解释其背后的设计逻辑。 1. checkpoint_id 是如何生成的？ 首先，我们来看 checkpoint_id 的生成方式。在 langgraph/checkpoint/base/__init__.py 中，有一个 create_checkpoint 函数： 12345678910111213141516171819def create_checkpoint(    checkpoint: Checkpoint,    channels: Mapping[str, BaseC...</div></div></div></a><a class="pagination-related" href="/2025/12/15/2025-12-15-LangGraph-%E5%85%AB%E8%82%A1/" title="LangGraph 八股"><div class="cover" style="background: /img/cover/langgraph.jepg"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-15</div><div class="info-item-2">LangGraph 八股</div></div><div class="info-2"><div class="info-item-1">LangGraph 八股 官方文档：https://docs.langchain.com/oss/python/releases/langchain-v1 https://zhuanlan.zhihu.com/p/1914230995034564014 langchain&amp;laanggraph 1.0：  https://zhuanlan.zhihu.com/p/1966891862062265076 https://zhuanlan.zhihu.com/p/1968427472388335014  langchain新特性：   全新create_agent接口：默认运行在 LangGraph 引擎之上。   中间件定义了一组钩子，允许您自定义代理循环中的行为，从而实现代理采取的每个步骤的细粒度控制；支持自定义中间件 ，这些中间件可以连接到代理循环中的多个点。      钩子函数 触发时机 应用场景     before_agent 在调用代理之前 加载记忆数据、验证输入   before_model 在每次大模型调用之前 更新提示词、精简消息历史   wrap_mod...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Roger-Lv</div><div class="author-info-description">Send a flare and light the way.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">172</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">148</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">48</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Roger-Lv"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Roger-Lv" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1150568956@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zhongrenjie-lv-5588a928a/" target="_blank" title="LinkedIn"><i class="iconfont icon-linkedin-fill"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Agent八股</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF1"><span class="toc-number">1.1.</span> <span class="toc-text">模板1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E8%82%A1%EF%BC%9AEncoder%E4%B8%8Edecoder%E7%9A%84%E4%B8%ADAttention%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">1.1.1.</span> <span class="toc-text">八股：Encoder与decoder的中Attention区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E8%82%A1%EF%BC%9AAttention%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%A4%E4%BB%A5%E6%A0%B9%E5%8F%B7%E4%B8%8BDk%EF%BC%9Fmask-attention%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">1.1.2.</span> <span class="toc-text">八股：Attention如何计算？为什么除以根号下Dk？mask attention是如何实现的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E8%82%A1%EF%BC%9A%E9%99%A4%E4%BA%86MHA%E8%BF%98%E7%9F%A5%E9%81%93%E5%93%AA%E4%BA%9B-GQA-MQA-MLA-%E8%AE%B2%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">八股：除了MHA还知道哪些(GQA MQA MLA)讲原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E8%82%A1%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8sin-cos%EF%BC%9F"><span class="toc-number">1.1.4.</span> <span class="toc-text">八股：为什么要用位置编码？为什么要用sin_cos？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB%E8%82%A1%EF%BC%9A%E4%BD%A0%E6%8F%90%E5%88%B0%E7%94%A8DeepSpeed%E5%81%9ASFT%E8%AE%AD%E7%BB%83%EF%BC%8C%E8%AF%B7%E8%AE%B2%E4%B8%80%E4%B8%8BDeepSpeed-ZeRO-Stage-1-3%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%94%A8FSDP%E4%BC%9A%E6%9B%B4%E5%A5%BD%EF%BC%9F"><span class="toc-number">1.1.5.</span> <span class="toc-text">八股：你提到用DeepSpeed做SFT训练，请讲一下DeepSpeed ZeRO Stage 1-3的区别，以及什么时候用FSDP会更好？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%EF%BC%9A%E9%97%AEAgent%E7%9A%84%E5%B7%A5%E5%85%B7tool%E7%9A%84%E8%AE%BE%E8%AE%A1%EF%BC%8C%E6%98%AF%E5%90%A6%E6%98%AFworkflow%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.1.6.</span> <span class="toc-text">项目：问Agent的工具tool的设计，是否是workflow形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%BA%86%E8%A7%A3%E5%93%AA%E4%BA%9Bagent%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%BE%8B%E5%A6%82langchain%E5%92%8CLlamaIndex%EF%BC%8C%E4%BB%96%E4%BB%AC%E6%A0%B8%E5%BF%83%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E6%9C%89%E4%BD%95%E4%B8%8D%E5%90%8C"><span class="toc-number">1.1.7.</span> <span class="toc-text">项目：了解哪些agent开发框架，例如langchain和LlamaIndex，他们核心应用场景有何不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%EF%BC%9A%E9%97%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%A8%B3%E5%AE%9A%E7%9A%84json%E5%81%9A%E4%BA%86%E5%93%AA%E4%BA%9B%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.1.8.</span> <span class="toc-text">项目：问数据的输入输出格式如何保证大模型输出稳定的json做了哪些工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E5%8A%9B%E9%A2%98%EF%BC%9A%E6%9C%8912%E4%B8%AA%E5%A4%96%E8%A7%82%E7%9B%B8%E5%90%8C%E7%9A%84%E8%8A%AF%E7%89%87%E3%80%81%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E9%87%8D%E9%87%8F%E4%B8%8D%E5%90%8C-%E4%B8%8D%E7%9F%A5%E8%BD%BB%E9%87%8D-%EF%BC%8C%E7%94%A8%E5%A4%A9%E5%B9%B3%E6%9C%80%E5%B0%91%E7%A7%B0%E5%87%A0%E6%AC%A1%E8%83%BD%E6%89%BE%E5%87%BA%E8%BF%99%E5%BC%A0%E8%8A%AF%E7%89%87%EF%BC%9F"><span class="toc-number">1.1.9.</span> <span class="toc-text">智力题：有12个外观相同的芯片、其中一个重量不同(不知轻重)，用天平最少称几次能找出这张芯片？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%A2%98%EF%BC%9Alc215-%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E7%AC%ACK%E4%B8%AA%E6%9C%80%E5%A4%A7%E5%85%83%E7%B4%A0"><span class="toc-number">1.1.10.</span> <span class="toc-text">代码题：lc215 数组中的第K个最大元素</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF2"><span class="toc-number">1.2.</span> <span class="toc-text">模板2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8DRAG%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. 介绍RAG项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3LLM%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.怎么解决LLM幻觉问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-LLM%E7%9A%84%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D%EF%BC%88temp-topk-top-p%E7%AD%89%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">3.LLM的参数介绍（temp topk top p等）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-LLaMA%E5%92%8CGLM%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E7%AD%89%E6%96%B9%E9%9D%A2"><span class="toc-number">1.2.4.</span> <span class="toc-text">4.LLaMA和GLM的区别，模型架构等方面</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DeepSeek-v3-r1"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">DeepSeek v3&#x2F;r1:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Qwen3"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">Qwen3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kimi-K2"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">Kimi K2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Grok2-5"><span class="toc-number">1.2.4.4.</span> <span class="toc-text">Grok2.5</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Qwen%E6%A8%A1%E5%9E%8B%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E4%B9%8B%E9%97%B4%E7%9A%84%E6%94%B9%E8%BF%9B%E7%82%B9"><span class="toc-number">1.2.5.</span> <span class="toc-text">5.Qwen模型每个版本之间的改进点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%BB%8B%E7%BB%8D%E6%A3%80%E7%B4%A2%E5%81%9A%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%8C%E5%85%B7%E4%BD%93%E8%BF%BD%E9%97%AE%E5%AD%90%E9%97%AE%E9%A2%98%E5%88%86%E8%A7%A3%E6%80%8E%E4%B9%88%E5%81%9A%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E5%81%9A%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB"><span class="toc-number">1.2.6.</span> <span class="toc-text">6.介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-RAG%E6%80%8E%E4%B9%88%E8%AF%84%E4%BC%B0%EF%BC%8C%E6%8C%87%E6%A0%87%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">1.2.7.</span> <span class="toc-text">7.RAG怎么评估，指标有哪些</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-RAG%E5%A6%82%E6%9E%9C%E6%9C%89%E5%99%AA%E5%A3%B0%E6%80%8E%E4%B9%88%E5%8A%9E"><span class="toc-number">1.2.8.</span> <span class="toc-text">8.RAG如果有噪声怎么办</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E6%80%8E%E4%B9%88%E6%9E%84%E5%BB%BASFT%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%87%8F%E5%A4%9A%E5%B0%91%EF%BC%8C%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.2.9.</span> <span class="toc-text">9.怎么构建SFT数据集，数据量多少，微调方式是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-SFT%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%E4%B8%8D%E5%A4%9F%E5%A4%9A%E6%A0%B7%E5%8C%96%E6%80%8E%E4%B9%88%E5%8A%9E"><span class="toc-number">1.2.10.</span> <span class="toc-text">10.SFT数据问题不够多样化怎么办</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bfunction-calling%E5%92%8CMCP"><span class="toc-number">1.2.11.</span> <span class="toc-text">11.介绍一下function calling和MCP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E4%BB%A3%E7%A0%81%E9%A2%98%EF%BC%9Alc215-%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E7%AC%AC-K-%E4%B8%AA%E6%9C%80%E5%A4%A7%E5%85%83%E7%B4%A0"><span class="toc-number">1.2.12.</span> <span class="toc-text">12.代码题：lc215 数组中的第 K 个最大元素</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF3"><span class="toc-number">1.3.</span> <span class="toc-text">模板3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-lora-%E5%8E%9F%E7%90%86%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%8C%E5%AF%B9%E6%AF%94-sft"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.lora 原理，初始化，为什么，对比 sft</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C%E8%BF%87%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.训练网络过程的一些优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-batchnorm%E5%92%8Clayernorm-%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%EF%BC%8C%E5%9C%A8%E5%93%AA%E9%87%8C%E7%94%A8"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.batchnorm和layernorm 区别，为什么用，在哪里用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-attention-%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E5%8E%9F%E7%90%86%EF%BC%8Ccross-attention-%E7%9A%84-qkv-%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C"><span class="toc-number">1.3.4.</span> <span class="toc-text">4.attention 及其变体原理，cross attention 的 qkv 来自哪里</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Cross-Attention%EF%BC%88%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89"><span class="toc-number">1.3.5.</span> <span class="toc-text">3. Cross-Attention（交叉注意力）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E6%A0%B7%E8%AE%BE%E8%AE%A1%EF%BC%9F"><span class="toc-number">1.3.6.</span> <span class="toc-text">4. 为什么这样设计？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%80%8E%E4%B9%88%E7%AE%97%EF%BC%9F"><span class="toc-number">1.3.7.</span> <span class="toc-text">5.自注意力机制是什么？计算复杂度怎么算？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-KV-Cache%E7%9A%84%E5%A6%82%E4%BD%95%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86%EF%BC%9F"><span class="toc-number">1.3.8.</span> <span class="toc-text">6.KV-Cache的如何加速推理？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-LoRA%E7%9A%84%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%B8%8EP-Tuning%E3%80%81Adapter%E7%9A%84%E5%BC%82%E5%90%8C%E7%82%B9%EF%BC%9FLoRA%E7%9A%84%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%AF%B9%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%9C%89%E4%BD%95%E5%BD%B1%E5%93%8D%EF%BC%9F"><span class="toc-number">1.3.9.</span> <span class="toc-text">7.LoRA的原理是什么？与P-Tuning、Adapter的异同点？LoRA的参数选择对模型性能有何影响？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E4%BB%8B%E7%BB%8D%E4%B8%8BRLHF%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%EF%BC%8C%E4%B8%8EDPO%E7%9A%84%E5%B7%AE%E5%BC%82%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.10.</span> <span class="toc-text">8.介绍下RLHF的基本流程，与DPO的差异是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84TP%E3%80%81PP%E3%80%81DP%E5%88%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.11.</span> <span class="toc-text">9.分布式训练中的TP、PP、DP分别是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-flash-attention%E7%9A%84%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.12.</span> <span class="toc-text">10.flash-attention的原理是什么？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF4"><span class="toc-number">1.4.</span> <span class="toc-text">模板4</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E5%81%9A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%A2%9E%E5%BC%BA%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.训练数据，有没有做数据处理与增强的工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%9C%A8%E4%BB%80%E4%B9%88%E6%9C%BA%E5%99%A8%E4%B8%8A%E8%AE%AD%E7%BB%83%EF%BC%8C%E6%97%B6%E9%97%B4%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.在什么机器上训练，时间，数据量大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-rag%E4%B8%AD%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84pdf%E8%A7%A3%E6%9E%90%EF%BC%8C%E5%AF%B9pdf%E9%87%8C%E9%9D%A2%E7%9A%84%E5%9B%BE%E7%89%87%EF%BC%8C%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%8C%E6%80%8E%E4%B9%88%E7%BC%96%E7%A0%81%E7%9A%84%EF%BC%8C%E6%A3%80%E7%B4%A2%EF%BC%8C%E5%8F%AC%E5%9B%9E%E7%9A%84%E6%97%B6%E5%80%99%E9%83%BD%E5%81%9A%E4%BA%86%E5%93%AA%E4%BA%9B%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%E7%9A%84%E6%97%B6%E5%80%99%E7%9A%84%E6%9D%83%E9%87%8D%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E6%B6%88%E8%9E%8D%E5%AE%9E%E7%8E%B0%E5%AF%B9%E6%AF%94"><span class="toc-number">1.4.3.</span> <span class="toc-text">5.rag中怎么做的pdf解析，对pdf里面的图片，表格数据怎么处理的，怎么编码的，检索，召回的时候都做了哪些操作，混合检索的时候的权重怎么处理的，有没有消融实现对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-sft%E4%B8%8Erag%E5%AF%B9%E6%AF%94"><span class="toc-number">1.4.4.</span> <span class="toc-text">7.sft与rag对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-agent%E6%96%B9%E9%9D%A2%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BA%86%E8%A7%A3"><span class="toc-number">1.4.5.</span> <span class="toc-text">9.agent方面有哪些了解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF5"><span class="toc-number">1.5.</span> <span class="toc-text">模板5</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%A4%E4%B8%89%E5%8F%A5%E4%BB%8B%E7%BB%8D%E4%B8%8Bagent%E4%BB%A5%E5%8F%8A%E5%BD%93%E5%89%8D%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">1.5.1.</span> <span class="toc-text">1. 两三句介绍下agent以及当前的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-transformer%E6%9E%B6%E6%9E%84-%E6%9C%89%E5%93%AA%E4%BA%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.2.</span> <span class="toc-text">2. transformer架构 有哪些机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A7%A3%E9%87%8A%E4%B8%8B%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.3.</span> <span class="toc-text">3. 解释下交叉注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BB%8B%E7%BB%8D%E4%B8%8Bppo-dpo-grpo%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.4.</span> <span class="toc-text">4. 介绍下ppo dpo grpo算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-grpo%E7%9A%84loss%E6%80%8E%E4%B9%88%E8%AE%A1%E7%AE%97%E7%9A%84-%E6%95%B0%E6%8D%AE%E7%94%A8%E7%9A%84%E4%BB%80%E4%B9%88"><span class="toc-number">1.5.5.</span> <span class="toc-text">5. grpo的loss怎么计算的 数据用的什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-deepresearch%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%8E%E4%B9%88%E7%BB%93%E5%90%88%E5%BA%94%E7%94%A8"><span class="toc-number">1.5.6.</span> <span class="toc-text">6.deepresearch和强化学习怎么结合应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E8%A7%A3%E9%87%8A%E4%B8%8Btopk-topp%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="toc-number">1.5.7.</span> <span class="toc-text">7. 解释下topk topp的实现原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AFdecoder%E6%9E%B6%E6%9E%84"><span class="toc-number">1.5.8.</span> <span class="toc-text">8. 为什么现在大模型都是decoder架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF5-2"><span class="toc-number">1.6.</span> <span class="toc-text">模板5</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.6.1.</span> <span class="toc-text">损失函数设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LoRA%E5%90%9F%E5%94%B1"><span class="toc-number">1.6.2.</span> <span class="toc-text">LoRA吟唱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E6%92%95MHA"><span class="toc-number">1.6.3.</span> <span class="toc-text">手撕MHA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9C%8B%E4%BD%A0%E9%99%A4%E4%BB%A5%E4%BA%86%E6%A0%B9%E5%8F%B7k-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8"><span class="toc-number">1.6.4.</span> <span class="toc-text">看你除以了根号k 有什么作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3"><span class="toc-number">1.6.5.</span> <span class="toc-text">梯度消失和梯度爆炸 如何缓解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#QKV%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88-%E8%AF%B4%E8%AF%B4%E7%90%86%E8%A7%A3"><span class="toc-number">1.6.6.</span> <span class="toc-text">QKV代表什么 说说理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E6%9E%9CQK%E5%8F%98%E6%88%90%E5%90%8C%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5%E4%BD%A0%E8%A7%89%E5%BE%97%E6%9C%89%E4%BB%80%E4%B9%88%E5%BD%B1%E5%93%8D"><span class="toc-number">1.6.7.</span> <span class="toc-text">如果QK变成同一个矩阵你觉得有什么影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%A4%E4%BA%86LoRA%E8%BF%98%E6%9C%89%E4%BB%80%E4%B9%88%E5%BE%AE%E8%B0%83%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.6.8.</span> <span class="toc-text">除了LoRA还有什么微调的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%9D%BF6"><span class="toc-number">1.7.</span> <span class="toc-text">模板6</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%88%E5%81%9A%E4%B8%AA%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D%E5%90%A7%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%BB%93%E5%90%88%E4%BD%A0%E4%B9%8B%E5%89%8D%E7%9A%84%E5%B7%A5%E4%BD%9C%E7%BB%8F%E5%8E%86%E8%81%8A%E8%81%8A%E3%80%82"><span class="toc-number">1.7.1.</span> <span class="toc-text">先做个自我介绍吧，可以结合你之前的工作经历聊聊。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%A0%E5%AF%B9-SFT%EF%BC%88%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%89%E4%B8%AD%E7%9A%84-scaling-law-%E6%9C%89%E4%BA%86%E8%A7%A3%E5%90%97%EF%BC%9F%E5%9C%A8%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E4%B8%AD%EF%BC%8C%E4%BD%A0%E9%81%87%E5%88%B0%E8%BF%87%E5%93%AA%E4%BA%9B%E6%AF%94%E8%BE%83%E5%A4%A7%E7%9A%84%E5%9B%B0%E9%9A%BE%EF%BC%9F"><span class="toc-number">1.7.2.</span> <span class="toc-text">你对 SFT（监督微调）中的 scaling law 有了解吗？在实际训练中，你遇到过哪些比较大的困难？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%8F%91%E7%8E%B0-advantage-%E6%88%96%E8%80%85-loss-%E7%AA%81%E7%84%B6%E5%8F%98%E6%88%90-0%EF%BC%8C%E4%B8%80%E8%88%AC%E5%8F%AF%E8%83%BD%E6%98%AF%E4%BB%80%E4%B9%88%E5%8E%9F%E5%9B%A0%E5%AF%BC%E8%87%B4%E7%9A%84%EF%BC%9F"><span class="toc-number">1.7.3.</span> <span class="toc-text">在模型训练时，如果发现 advantage 或者 loss 突然变成 0，一般可能是什么原因导致的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E6%9E%84%E5%BB%BA-AI-Agent-%E6%97%B6%EF%BC%8C%E5%AE%83%E7%9A%84%E8%AE%B0%E5%BF%86%EF%BC%88Memory%EF%BC%89%E6%9C%BA%E5%88%B6%E9%80%9A%E5%B8%B8%E6%98%AF%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F"><span class="toc-number">1.7.4.</span> <span class="toc-text">在构建 AI Agent 时，它的记忆（Memory）机制通常是怎么设计的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%93%E6%A8%A1%E5%9E%8B%E5%87%BA%E7%8E%B0-bad-case-%E6%97%B6%EF%BC%8C%E4%BD%A0%E4%B8%80%E8%88%AC%E4%BC%9A%E6%80%8E%E4%B9%88%E5%88%86%E6%9E%90%EF%BC%9F%E5%90%8E%E7%BB%AD%E4%BC%9A%E9%87%87%E5%8F%96%E5%93%AA%E4%BA%9B%E6%8E%AA%E6%96%BD%E6%9D%A5%E6%94%B9%E8%BF%9B%EF%BC%9F"><span class="toc-number">1.7.5.</span> <span class="toc-text">当模型出现 bad case 时，你一般会怎么分析？后续会采取哪些措施来改进？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%E4%B8%BA%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E6%96%87%E6%9C%AC%E8%AE%AD%E7%BB%83%E4%B8%80%E5%A5%97-Embedding%EF%BC%8C%E4%BD%A0%E4%BC%9A%E6%80%8E%E4%B9%88%E5%81%9A%EF%BC%9F"><span class="toc-number">1.7.6.</span> <span class="toc-text">如果需要为特定领域的文本训练一套 Embedding，你会怎么做？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%8A%E8%81%8A%E4%BD%A0%E5%AE%9E%E4%B9%A0%E6%97%B6%E5%9B%A2%E9%98%9F%E7%9A%84%E7%BB%84%E6%88%90%E5%92%8C%E5%88%86%E5%B7%A5%E5%90%A7%EF%BC%8C%E5%A4%A7%E5%AE%B6%E6%98%AF%E6%80%8E%E4%B9%88%E5%8D%8F%E4%BD%9C%E7%9A%84%EF%BC%9F"><span class="toc-number">1.7.7.</span> <span class="toc-text">聊聊你实习时团队的组成和分工吧，大家是怎么协作的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%A0%E5%AF%B9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E5%BA%95%E5%B1%82%E4%BA%86%E8%A7%A3%E5%A4%9A%E5%B0%91%EF%BC%9F%E6%AF%94%E5%A6%82%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%88DP%EF%BC%89%E3%80%81%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%EF%BC%88TP%EF%BC%89%E8%BF%99%E4%BA%9B%EF%BC%8C%E7%9C%8B%E8%BF%87-Megatron-%E8%BF%99%E7%B1%BB%E6%A1%86%E6%9E%B6%E7%9A%84%E6%BA%90%E7%A0%81%E5%90%97%EF%BC%9F"><span class="toc-number">1.7.8.</span> <span class="toc-text">你对大模型分布式训练的底层了解多少？比如数据并行（DP）、张量并行（TP）这些，看过 Megatron 这类框架的源码吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%BC%8F%E7%AE%97%E6%B3%95%E9%A2%98%EF%BC%9A%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE"><span class="toc-number">1.7.9.</span> <span class="toc-text">核心代码模式算法题：二维数组中的查找</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/webdancer.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebDancer:Towards Autonomous Information Seeking Agency"/></a><div class="content"><a class="title" href="/2025/12/18/2025-12-18-WebDancer-Towards-Autonomous-Information-Seeking-Agency/" title="WebDancer:Towards Autonomous Information Seeking Agency">WebDancer:Towards Autonomous Information Seeking Agency</a><time datetime="2025-12-17T16:00:00.000Z" title="发表于 2025-12-18 00:00:00">2025-12-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/tongsearch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TongSearch-QR:Reinforced Query Reasoning for Retrieval"/></a><div class="content"><a class="title" href="/2025/12/18/2025-12-17-TongSearch-QR-Reinforced-Query-Reasoning-for-Retrieval/" title="TongSearch-QR:Reinforced Query Reasoning for Retrieval">TongSearch-QR:Reinforced Query Reasoning for Retrieval</a><time datetime="2025-12-17T16:00:00.000Z" title="发表于 2025-12-18 00:00:00">2025-12-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/" title="Agent八股"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/17/2025-12-17-Agent%E5%85%AB%E8%82%A1/" title="Agent八股">Agent八股</a><time datetime="2025-12-16T16:00:00.000Z" title="发表于 2025-12-17 00:00:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-DeepResearch%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E6%96%B9%E6%A1%88/" title="DeepResearch智能体方案"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-DeepResearch%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E6%96%B9%E6%A1%88/" title="DeepResearch智能体方案">DeepResearch智能体方案</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-LangGraph-%E5%85%AB%E8%82%A1/" title="LangGraph 八股"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-LangGraph-%E5%85%AB%E8%82%A1/" title="LangGraph 八股">LangGraph 八股</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2024 - 2025 By Roger-Lv</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.4.2"></script><script src="/js/main.js?v=5.4.2"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'smA3tZdRGodG2VgnMubBQjLm-gzGzoHsz',
      appKey: 'biCDxj0lSBtZTMie2kNIKErd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><div class="aplayer no-destroy" data-id="8674547170" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.4.2"></script></div></div></body></html>