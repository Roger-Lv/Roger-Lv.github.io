<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计 | Roger-Lv's space</title><meta name="author" content="Roger-Lv"><meta name="copyright" content="Roger-Lv"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计 转载：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;26833089345 在 RL、蒸馏等任务中需要多个模型协同完成计算、数据通信、流程控制等工作。例如 PPO 及各类衍生算法中，就需要管理 Actor、Rollout、Ref、Critic、Reward 等最多 5 类模块，每类模块还承担着 train、eval、ge">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计">
<meta property="og:url" content="http://example.com/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/index.html">
<meta property="og:site_name" content="Roger-Lv&#39;s space">
<meta property="og:description" content="基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计 转载：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;26833089345 在 RL、蒸馏等任务中需要多个模型协同完成计算、数据通信、流程控制等工作。例如 PPO 及各类衍生算法中，就需要管理 Actor、Rollout、Ref、Critic、Reward 等最多 5 类模块，每类模块还承担着 train、eval、ge">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cover/Ray.jpg">
<meta property="article:published_time" content="2025-08-12T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-13T11:02:44.082Z">
<meta property="article:author" content="Roger-Lv">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cover/Ray.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计",
  "url": "http://example.com/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/",
  "image": "http://example.com/img/cover/Ray.jpg",
  "datePublished": "2025-08-12T16:00:00.000Z",
  "dateModified": "2025-08-13T11:02:44.082Z",
  "author": [
    {
      "@type": "Person",
      "name": "Roger-Lv",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="http://example.com/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.4.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">147</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">48</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Roger-Lv's space</span></a><a class="nav-page-title" href="/"><span class="site-name">基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div><!-- 添加搜索按钮 ↓--><span class="search-button"><i class="fas fa-search" aria-hidden="true"></i></span></div></nav><div id="post-info"><h1 class="post-title">基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-12T16:00:00.000Z" title="发表于 2025-08-13 00:00:00">2025-08-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-13T11:02:44.082Z" title="更新于 2025-08-13 19:02:44">2025-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/" data-flag-title="基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计</h1>
<p>转载：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26833089345">https://zhuanlan.zhihu.com/p/26833089345</a></p>
<p>在 RL、蒸馏等任务中需要多个模型协同完成计算、数据通信、流程控制等工作。例如 PPO 及各类衍生算法中，就需要管理 Actor、Rollout、Ref、Critic、Reward 等最多 5 类模块，每类模块还承担着 train、eval、generate 其中的一种或多种职责，而蒸馏任务中也存在着多组 Teacher 和多组 Student 共同蒸馏的场景。</p>
<p>如果我们仍然采用 Pretrain、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=254389392&amp;content_type=Article&amp;match_order=1&amp;q=SFT&amp;zhida_source=entity">SFT</a> 训练这种基于单脚本多进程的运行模式（通过 deepspeed、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=254389392&amp;content_type=Article&amp;match_order=1&amp;q=torchrun&amp;zhida_source=entity">torchrun</a> 等命令启动任务），是难以实现灵活的任务调度和资源分配策略的。而 <strong>Ray</strong> 提供的 remote 异步调用和 Actor 抽象，可以让每个模块有独立的运行单元和任务处理逻辑，<strong>这种分离式架构使之天然适配多模型之间的频繁交互和协同工作的场景</strong>。</p>
<p>这篇文章以当今最为流行的两个 RL 框架 <strong><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl">veRL</a></strong> 和 <strong><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF">OpenRLHF</a></strong> 为例，从工程角度分析这两个框架的特点和优势，以及它们是如何实现多 Actor 场景下的分离式训推混合任务的（也为我最近开发的一个蒸馏框架提供一些灵感hhh）。</p>
<p>我会从以下 3 个角度去分析两个框架：</p>
<ol>
<li>每个模型/模块的<strong>职责</strong>，以及它们训练/推理所采用的 <strong>backend</strong> 是什么；</li>
<li>如何通过 Ray <strong>分配各个模块的资源</strong>，以及如何实现模块的<strong>资源共享</strong>（colocate、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=254389392&amp;content_type=Article&amp;match_order=1&amp;q=Hybrid+Engine&amp;zhida_source=entity">Hybrid Engine</a>）；</li>
<li>运行过程的 <strong>data flow</strong> 和 <strong>control flow</strong> 是怎样的，从任务分发到任务实际执行的<strong>调度链路</strong>是什么。</li>
</ol>
<p>文章中有非常多的代码的跳转链接，我非常推荐各位大佬们<strong>结合实际代码</strong>来理解文章内容，毕竟 talk is cheap，多看代码才能快速提升基本功～</p>
<p>好的，大家坐稳了，我们要出发了～</p>
<h2 id="先简单介绍下-Ray">先简单介绍下 Ray</h2>
<p>这里给不太熟悉 Ray 工作模式的同学普及一下基本概念。</p>
<h3 id="1-启动方式">1. 启动方式</h3>
<p>Ray 提供了多种语言的调用接口，但我们用的最多的还是 Python 接口，一般我们会运行一个 Python 脚本，并在这个脚本中运行<code>ray.init()</code>就自动创建了一个 Ray 集群，通常这个脚本的运行进程叫做 <strong>driver</strong> process。除此之外，我们也可以通过在命令行运行<code>ray start</code>手动启动 Ray 集群，并在脚本中去 attach 到这个集群上。</p>
<h3 id="2-运行逻辑">2. 运行逻辑</h3>
<p>Ray 集群在操作系统层面上主要体现为节点上一组驻留的进程池。当我们创建一个函数或者一个类，并用<code>@ray.remote</code>装饰后，这个函数/类就成为了一个可调度的 <strong><a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/tasks.html">Task</a>/<a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/actors.html">Actor</a></strong>。我们可以调用这个 Task/Actor 的 remote 方法，按照调度策略将这个 Task/Actor 分配到某个节点的进程池上运行或初始化。对于 driver 来说，分发出去的任务是异步运行的，因此还需要通过 <code>ray.get</code>去获取异步运行结果。</p>
<p>Task/Actor 所传入的参数和返回的结果都会先被<a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/objects/serialization.html">序列化</a>为一个 <strong><a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/objects.html">Object</a></strong>，存放在 Ray 集群的 Object Store 里面。从 Ray 的层面看，一个 Ray 集群中所有节点的 CPU memory 共同组成了一个 (Shared) Object Store，节点之间在逻辑上是共享这个 Object Store 的所有资源的，因此我们（在逻辑上）不需要关心哪个对象存放在哪个节点，只需要<code>ray.get</code>这个 Object 的 reference，然后 Ray 就会自动拿取实际的 Object 并<a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/objects/serialization.html">反序列化</a>到运行进程中。</p>
<p>下面提供了一个案例，我们在 remote 初始化了一个 <code>ParentActor</code>并调用了它的 <code>create_child</code> 方法，这个方法在 remote 创建了一个 <code>ChildActor</code>。后续 <code>parent_actor.get_work</code>会让 parent 链式调用 child 的方法，获得 child 的运行结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"></span><br><span class="line">ray.init()</span><br><span class="line"></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChildActor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">do_work</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Work done by child&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ParentActor</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_child</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.child_actor = ChildActor.remote()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_work</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> ray.get(<span class="variable language_">self</span>.child_actor.do_work.remote())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ChildActor) <span class="comment"># &lt;__main__.ActorClass(ChildActor) object at 0x7f98b3224d60&gt;</span></span><br><span class="line"><span class="built_in">print</span>(ParentActor) <span class="comment"># &lt;__main__.ActorClass(ParentActor) object at 0x7f98b32b8460&gt;</span></span><br><span class="line"></span><br><span class="line">parent_actor = ParentActor.remote()</span><br><span class="line">ray.get(parent_actor.create_child.remote())</span><br><span class="line"><span class="built_in">print</span>(ray.get(parent_actor.get_work.remote())) <span class="comment"># Work done by child</span></span><br></pre></td></tr></table></figure>
<p>上面的例子说明，Actor 可以通过<strong>组合</strong>的方式创建和运行，即在一个 Actor 中可以 remote 创建和调用另一个 Actor。值得注意的是，在现版本的 Ray 中是不能用<strong>继承</strong>的方式去继承一个 Actor 的方法的，所以我们只会在最终的子类上用 <code>@ray.remote</code> 装饰。</p>
<h3 id="3-资源调度">3. 资源调度</h3>
<p>在创建 Actor 时，我们可以指定这个 Actor 所需要的<a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/scheduling/resources.html">运行资源</a>（num_cpus, num_gpus 等），并从资源池中获取这些资源，若所需的资源不足则无法立即调度，这种方式只能实现资源的独占。同时我们还可以事先分配一个<a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/scheduling/placement-group.html">资源组</a>（placement group），并将一个或多个 Actor 分配到这个资源组的一个 <a href="https://link.zhihu.com/?target=https%3A//docs.ray.io/en/latest/ray-core/scheduling/placement-group.html%23bundles">bundle</a> 上，实现资源的独占或共享。显然后者的资源调度方式更为灵活，像 veRL、OpenRLHF 均采用了这个策略。</p>
<p>在我的蒸馏框架里面也采用了资源组的方式分配资源：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">remote_ray_worker = ray.remote(</span><br><span class="line">    num_gpus=<span class="variable language_">self</span>.num_gpus_per_worker,</span><br><span class="line">    scheduling_strategy=PlacementGroupSchedulingStrategy(</span><br><span class="line">        placement_group=<span class="variable language_">self</span>.resource_group.ray_pg,</span><br><span class="line">        placement_group_bundle_index=worker_rank,</span><br><span class="line">    ),</span><br><span class="line">    runtime_env=&#123;<span class="string">&quot;env_vars&quot;</span>: env_vars&#125;,</span><br><span class="line">    name=worker_name,</span><br><span class="line">    max_concurrency=<span class="number">2</span>,</span><br><span class="line">)(<span class="variable language_">self</span>.worker_cls).remote(</span><br><span class="line">    <span class="variable language_">self</span>.num_workers,</span><br><span class="line">    worker_rank,</span><br><span class="line">    model_cls=<span class="variable language_">self</span>.model_cls,</span><br><span class="line">    model_args=<span class="variable language_">self</span>.model_args,</span><br><span class="line">    **kwargs,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>例如 RLHF 中的一个 Actor 模块，开启 dp=4，那么就可以创建一个 GPU=4 的资源组（暂时忽略 CPU 资源），并建立 4 个 GPU=1 的 bundle，然后对每一个 Actor worker 的分片，依次分配一个资源组的 bundle 即可。</p>
<h2 id="2-解析-OpenRLHF">2. 解析 OpenRLHF</h2>
<p>从工程逻辑的角度看，OpenRLHF 的代码较为简洁易懂，而 veRL 有一些工程实现上的小 trick。所以我们先从 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF">OpenRLHF</a> 入手解读。</p>
<p>我现在使用的版本是最新的发版版本 v0.5.9.post1，当前支持 Ray 训练部分的仅有 PPO 及其衍生算法（REINFORCE++, GRPO, RLOO），我们主要看这一部分的代码。</p>
<p>与 Ray 相关的核心代码文件有：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/cli/train_ppo_ray.py">cli/train_ppo_ray.py</a>：启动脚本</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/launcher.py">trainer/ray/launcher.py</a>：核心调度组件 PPORayActorGroup，以及 Ref Actor 和 Reward Actor 的实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/ppo_actor.py">trainer/ray/ppo_actor.py</a>：Actor 和 Actor Trainer（继承 PPOTrainer）的实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/ppo_critic.py">trainer/ray/ppo_critic.py</a>：Critic 和 Critic Trainer（继承 PPOTrainer）的实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/vllm_engine.py">trainer/ray/vllm_engine.py：</a><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=254389392&amp;content_type=Article&amp;match_order=1&amp;q=vLLM&amp;zhida_source=entity">vLLM</a> Rollout Actor 的实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/vllm_worker_wrap.py">trainer/ray/vllm_worker_wrap.py</a>：vLLM Worker 子类，同步 Actor 和 Rollout 模块权重的逻辑</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ppo_trainer.py">trainer/ppo_trainer.py</a>：PPOTrainer 实现，即 PPO 算法主体</li>
</ul>
<p>先放一张整体架构图：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/v2-5baa7c95f3f0d668c7c9b674ad6b377f_1440w.jpg" alt="img"></p>
<p>上图区分了 driver process 和 remote 上存在的实例。在 Driver 上有着各种模块对应的 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/launcher.py%23L143">PPORayActorGroup</a> 实例，每一个 Group 实例代表着逻辑上的一个完整模型，而 Group 中的每个 remote worker 是这个完整模型的 DP 分片。对于 Rollout 模块而言，driver 上存在一个或多个 LLMRayActor 的 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/cli/train_ppo_ray.py%23L82-L94">handle</a>，每个 Actor 代表一个 vLLM engine，也就是一个完整的 DP 模型，每个 engine 内部还会通过 Ray 启动 TP worker Actor（这个 Ray 会 attach 到已有的 cluster，不会新建一个）。</p>
<p>Group 中<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/launcher.py%23L178">创建 worker</a> 是依次进行的：首先创建 rank0 worker（master actor），并由它获取整个 Group 建立通信的 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/launcher.py%23L210">addr 和 port</a>，接着依次创建其他 worker 并传入通信的 addr 和 port。在初始化模型时，统一作<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ray/launcher.py%23L108-L109">通信组的初始化</a>。注意 Group 之间的通信是相互隔离的，因此每一个 Group 的训练就可以等价于平时做的多进程模型训练。</p>
<p>因此在 Ray 的抽象下，各个模块都可以看成是独立的 multi-process training / generate，而模块之间的交互是通过 Object Store 和 Object Ref 做数据的收发来实现的。我们可以看到 Ray 在底层帮我们隐藏了许多技术细节，从而简化了多模型协同训练的搭建逻辑。</p>
<h3 id="2-1-训推模块与-backend">2.1. 训推模块与 backend</h3>
<p>我们首先整理一下 PPO 算法中各个模块的功能和职责：</p>
<ul>
<li>Actor：训练模块，前向反向都计算，需要更新权重</li>
<li>Critic：训练 + Eval 模块，前向反向都计算，需要更新权重</li>
<li>Rollout：批量推理模块，用于生成 trace samples，需要和 Actor 同步权重</li>
<li>RM、Ref：Eval 模块，仅前向计算，权重不更新</li>
</ul>
<p>理论上，训练模块可以采用市面上所有的训练引擎充当 backend（torch DDP、FSDP、torchtitan、Megatron、Deepspeed 等），批量推理模块可以采用所有的推理引擎充当 backend（SGLang、vLLM、TGI 等）。但 Eval 模块训练推理引擎都可以做，需要仔细斟酌要用哪个，考虑到<a href="https://link.zhihu.com/?target=https%3A//github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">训练和推理引擎的精度差异</a>（logit 数值上约有 10% 的相对误差），在涉及关键的 loss 计算还是要优先确保精度而非速度，所以我们可能会更倾向于在训练引擎跑一个 plain forward。</p>
<p>在 OpenRLHF 中，训练模块采用 Deepspeed，它的好处在于和现有 HF 生态融合地非常好，基本没有兼容性问题，也不太依赖特定版本，当然用 FSDP 也差不多。推理模块用 vLLM（为什么不用更快的 SGLang？可能是因为兼容性、框架熟悉程度或者使用习惯？），同时支持了 DP（多个 engine）、TP（每个 engine 内部）并行。</p>
<h3 id="2-2-Ray-资源调度与-colocate">2.2. Ray 资源调度与 colocate</h3>
<p>所谓 colocate，在这里的含义就是多个 <strong>Ray Actor 共享同一个 GPU 资源</strong>。由于 OpenRLHF 中每个 Ray Actor 都是某个模块的 DP（对 vLLM 而言是 DP+TP）分片，因此可以理解为<strong>不同模块对应的分片同时存放于一张卡上</strong>。这里的“同时存放”是概念上的，不一定要同时占用显存，实际上每个模块的分片可以通过 offload/reload 轮流占用显存。</p>
<p>OpenRLHF 提供了三种 colocate 方式：<code>colocate_actor_ref</code>，<code>colocate_critic_reward</code> 和 <code>colocate_all_models</code>。其中 <code>colocate_all_models</code>既包括了前两者，又增加了 Actor 和 Rollout 的 colocate。这三种 colocate 的实现方式都是类似的，也就是上面提到的事先分配资源组并给每个 worker 分片指定 bundle 的方式。</p>
<p>具体而言，PPO 的每个模块在逻辑上属于一个 PPORayActorGroup，如果模块之间存在 colocate，则往这个 Group 中传入同一个 placement_group（pg），然后在 Group 内部分配每个 worker 的 bundle。由于我们希望至多 5 种模块的 worker 共享一张卡，因此设置 num_gpus_per_actor=0.2 可以刚好满足资源需求。【不过这里存在一个 caveat：当开启 <code>colocate_all_models</code>并存在多个 reward model 时，那就有 6 个及以上的模块了，那么资源分配应该会失败，看官方是否认为这是个问题吧。】</p>
<p>不做 colocate 的模块则在 Group 内部新建资源组并分配 bundle，每个 DP 分片独占 1 个 GPU，因此也不会抢占其他 colocate 模块的 GPU 资源。</p>
<p>**这里需要尤其关注 Actor 和 Rollout colocate 的情况。**在 OpenRLHF 中，Actor 和 Rollout 是独立的两个模型，一个放在 deepspeed 训练引擎，一个放在 vLLM 中，它们需要保持权重的同步。因此当 Actor 更新时，需要<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py%23L167">将新权重 broadcast 到 Rollout 上</a>。由于两个模块时 colocate 到一张卡上的，而 NCCL 无法做同一张卡上两个进程的通信，所以需要<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py%23L223-L232">用 CUDA IPC 做进程间通信</a>。通信组是在 Actor 的 worker0 和所有 vLLM engine 的所有 worker 之间<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py%23L78-L118">建立</a>的，权重同步<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py%23L209-L235">分两步</a>：一是在 Actor workers 内部 all_gather 权重，二是由 worker0 代表 Actor 向所有 Rollout 实例 broadcast 权重。</p>
<h3 id="2-3-Data-Control-Flow-梳理">2.3. Data/Control Flow 梳理</h3>
<p>OpenRLHF 各个模块写的非常整洁，然而它缺少了贯穿这些模块的统一的 Control 模块，使得实际的执行流程分散在各个模块之间，这同时也是这个代码库最难理解和 track 的部分。</p>
<p>从<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/cli/train_ppo_ray.py%23L46">启动脚本</a>出发，完成参数初始化、各个模块建立和模型初始化后，控制逻辑交给了隶属于 Actor 的 Group，调用 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ray/launcher.py%23L242">async_fit_actor_model</a>，这个方法内会调用所有 Actor worker 的 <code>fit</code>方法，其本质是调用了<code>PPOTrainer.fit</code>，至此所有 worker 同时开训。</p>
<p>此时控制逻辑在每个 Actor worker 的 trainer 中，同时<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ray/ppo_actor.py%23L426-L431">每个 Actor worker 都绑定到一组 (Ref, Critic, RMs) worker 上</a>，Actor worker 生成或需要的数据只通过这些绑定的 worker 传输。理论上由于所有 Actor、Ref、Critic、RM 都是 DP 分片，Actor worker 向任何一个分片发送/接受数据都是等价的，实际上 OpenRLHF 是通过 <a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ray/launcher.py%23L274-L284">round-robin 轮询</a>的策略挑选组合的。</p>
<p>后续的控制逻辑比较分散，我整理后展示其伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In `PPOTrainer.fit`: https://github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ppo_trainer.py#L189</span></span><br><span class="line"><span class="keyword">for</span> eposide <span class="keyword">in</span> <span class="built_in">range</span>(num_eposides):</span><br><span class="line">    <span class="keyword">for</span> prompt <span class="keyword">in</span> prompt_dataloader:</span><br><span class="line">        sample_and_generate_rollout() <span class="comment"># `micro_rollout_bs` per step, total `rollout_bs`</span></span><br><span class="line">        make_exps() <span class="comment"># all_batch (i.e. `rollout_bs`)</span></span><br><span class="line">        put_in_replay_buffer()</span><br><span class="line">        <span class="comment"># In `ActorPPOTrainer.ppo_train`: https://github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py#L122</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs): <span class="comment"># train_bs = micro_train_bs * num_grad_acc</span></span><br><span class="line">            <span class="keyword">for</span> exps <span class="keyword">in</span> replay_buffer:  <span class="comment"># micro_train_bs per step</span></span><br><span class="line">                train(exps)</span><br><span class="line"></span><br><span class="line">        clear_replay_buffer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># In `training_step`: https://github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ppo_trainer.py#L327</span></span><br><span class="line"><span class="comment"># 实际上 actor 和 critic 会各自运行自己的 training step</span></span><br><span class="line">train(): </span><br><span class="line">    train_actor()</span><br><span class="line">    train_critic()</span><br><span class="line"></span><br><span class="line"><span class="comment"># In `training_step_actor`: https://github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ppo_trainer.py#L336</span></span><br><span class="line">train_actor():</span><br><span class="line">    cal_actor_loss()</span><br><span class="line">    cal_kl_loss() <span class="comment"># optional</span></span><br><span class="line">    cal_aux_loss() <span class="comment"># optional</span></span><br><span class="line">    backward()</span><br><span class="line">    pretrain_forward_and_cal_ptx_loss()</span><br><span class="line">    backward()</span><br><span class="line">    step()</span><br><span class="line">    ema_update()</span><br><span class="line"></span><br><span class="line"><span class="comment"># In `training_step_critic`: https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ppo_trainer.py#L452</span></span><br><span class="line">train_critic():</span><br><span class="line">    cal_critic_loss()</span><br><span class="line">    cal_aux_loss()</span><br><span class="line">    backward()</span><br><span class="line">    step()</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这些所有的控制逻辑全部由 Actor workers 执行，同时每一个 Actor worker 负责控制对应它绑定的 (Actor, Ref, Critic, RM, Rollout) 这个 worker 组的 data flow，因此 Actor workers 的计算和通信负担是非常重的。当然这里只是直观上的结论，具体的性能瓶颈仍然需要跑实验看下。</p>
<h2 id="3-解析-veRL">3. 解析 veRL</h2>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/tree/main">veRL 代码的开源部分</a>稍微乱了些，有许多看起来比较多余的抽象和封装，不过它是从闭源代码中抽出来的部分，内部应该还会对大量业务方和研究课题做了扩展和适配，因此这种“凌乱”可以理解。除此之外，我觉得 veRL 设计上最好的一点就是模块间充分的解耦，这使得修改和扩展自定义模块非常容易，同时框架使用了很多 Python 语法糖来巧妙的让一个 Ray Actor 在多种角色之间自由切换。</p>
<p>我目前看的也是最新版本 v0.2.0.post2。当前整个代码库基本都建立在 Ray 之上，我们这里主要关注 veRL 与 Ray + FSDP 相关的工程部分，相对而言会忽略 Megatron 部分以及绝大部分的模型、算法细节，不过 veRL 对于这部分做了充分的解耦，阅读和修改代码不会有太大的困难。</p>
<p>相关的核心代码文件有：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/trainer/main_ppo.py">trainer/main_ppo.py</a>：启动文件</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/workers/fsdp_workers.py">workers/fsdp_workers.py</a>：所有与 FSDP backend 相关的 Worker 实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/trainer/ppo/ray_trainer.py">ppo/ray_trainer.py</a>：Trainer 和资源管理</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/single_controller/ray/base.py">single_controller/ray/base.py</a>：基于 Ray 的 colocate 和 WorkerGroup 实现</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/workers/rollout/vllm_rollout/vllm_rollout.py">workers/rollout/vllm_rollout/vllm_rollout.py</a>：vLLM Rollout 实现</li>
</ul>
<p>周边代码还有：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/single_controller/base/decorator.py">single_controller/base/decorator.py</a>：数据分发策略</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/single_controller/base/worker.py">single_controller/base/worker.py</a>：Worker 基类</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/single_controller/base/worker_group.py">single_controller/base/worker_group.py</a>：WorkerGroup 基类</li>
</ul>
<p>首先还是从整体来看框架的组成部分：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-d945f7a92b123ee67b6287015c1a3600_1440w.jpg" alt="img"></p>
<p>不同于 OpenRLHF 由多个 Actor 控制一组 workers 的 control flow，veRL 的主体控制逻辑<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/trainer/main_ppo.py%23L37">集中于一个 Ray Actor 中</a>（veRL 官方称之为 <strong>single controller</strong>），这个 single controller 仅运行在 CPU 上，负责管理 data flow、control flow、各类数据结构的初始化，WorkerDict 的 remote 创建和调用，以及数据收发的统一管理。由于 single controller 的负载较大，官方推荐 single controller 尽可能调度在非 head 节点上。</p>
<p>这里最精妙的结构是 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L440">WorkerDict</a>，它本身只是一个 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/base/worker.py%23L81">Worker</a> 的基类，也就是 RLHF 某一个模块的模型分片，但实际上它绑定了 Actor、Critic、Rollout、Ref、Reward 等所有模块的公开方法，因此可以灵活地动态指定或切换一个 WorkerDict 实际代表的模块，可以看作一个万能的 Worker。</p>
<p>在 WorkerDict 之上是一个名为 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L176">RayWorkerGroup</a> 的数据结构。它主要是用于从资源组获取资源，动态指定 WorkerDict 的模块（通过 method 的重命名和 rebind 来实现）并创建 WorkerDict，同时作为任务调度器向指定的 WorkerDict <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L335">分发执行任务</a>。</p>
<h3 id="3-1-训推模块和-backend">3.1. 训推模块和 backend</h3>
<p>因为 PPO 算法对模块的需求是相同的，因此这部分的分析同 OpenRLHF。那么在 veRL 中，训练模块可以是 FSDP/HSDP 或者 Megatron，推理模块仍然是 vLLM（SGLang 应该在接入中）。</p>
<h3 id="3-2-Ray-调度与-Hybrid-Engine">3.2. Ray 调度与 Hybrid Engine</h3>
<p>尽管这里的副标题没有提到，但 veRL 实际上也可以做模块之间的 colocate，相比于 OpenRLHF 有限的 3 种 colocate 方式，veRL 理论上可以实现任意的 colocate 组合。从代码上看，我们可以将需要 colocate 的模块绑定到同一个 resource pool 中，然后逐个创建 resource pool 对应的模块 class。</p>
<p>但在实际的源代码中，veRL 目前的策略只有一种，也就是 colocate 所有模块。我个人认为，如果要在现有代码的基础上支持多种（或者任意的）colocate 策略，WorkerDict 和 RayWorkerGroup 可能要大改，至少需要考虑如何建立每个 resource group 的通信组，如何做环境变量的设置，以及如何做不同资源组之间的 method bind/rebind 等等（这块可以跟 OpenRLHF 学一学 hhhh）。</p>
<p>所以 veRL 主要强调的还是它的 Hybrid Engine 能力，也就是不同模块共享同一个数据结构（WorkerDict）和资源组，并且 WorkerDict 可以灵活地在多种模块、多个 engine 之间切换。这个 Hybrid Engine 的定义与 <a href="https://link.zhihu.com/?target=https%3A//github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md">Deepspeed-Chat</a> 非常接近。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/v2-a7ff959f616d22456cd54c93a47ef2a8_1440w.jpg" alt="img"></p>
<p>有个非常值得注意的点是，在 veRL 中 Actor 和 Rollout 是<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/workers/rollout/vllm_rollout/vllm_rollout.py%23L93">共享同一个模型权重</a>的，因此它不需要像 OpenRLHF 一样做权重同步和 CUDA IPC 通信【更正：感谢评论区同学的指正，veRL 通过 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/workers/sharding_manager/fsdp_vllm.py%23L76-L86">ShardingManager</a> 也做了模型的更新和同步，但由于 Actor 和 Rollout 是在同一个进程上的，所以也不需要做通信，此外我还抓到了 veRL 在 vLLM repo 提的 <a href="https://link.zhihu.com/?target=https%3A//github.com/sgl-project/sglang/issues/2736">feature request</a>】，但是原生的 vLLM 不支持直接传入模型结构/权重，所以 veRL 还对 vLLM 做了许多 patch 来适配。两种方案哪种更好呢？我觉得 OpenRLHF 实现上更加简单，可维护性和兼容性更好，而 veRL 更节省显存资源，性能上（可能）更好，只能说各有优劣吧。</p>
<p>接下来我们重点来看整个代码库中最 tricky 的部分，也就是如何实现 WorkerDict 的动态特性的。这部分的<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/trainer/ppo/ray_trainer.py%23L679-L691">入口代码</a>仅有几行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize WorkerGroup</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> if you want to use a different resource pool for each role, which can support different parallel size,</span></span><br><span class="line"><span class="comment"># you should not use `create_colocated_worker_cls`. Instead, directly pass different resource pool to different worker groups.</span></span><br><span class="line"><span class="comment"># See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.</span></span><br><span class="line">all_wg = &#123;&#125;</span><br><span class="line"><span class="variable language_">self</span>.wg_dicts = []</span><br><span class="line"><span class="keyword">for</span> resource_pool, class_dict <span class="keyword">in</span> <span class="variable language_">self</span>.resource_pool_to_cls.items():</span><br><span class="line">    worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)</span><br><span class="line">    wg_dict = <span class="variable language_">self</span>.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls)</span><br><span class="line">    spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())</span><br><span class="line">    all_wg.update(spawn_wg)</span><br><span class="line">    <span class="comment"># keep the referece of WorkerDict to support ray &gt;= 2.31. Ref: https://github.com/ray-project/ray/pull/45699</span></span><br><span class="line">    <span class="variable language_">self</span>.wg_dicts.append(wg_dict)</span><br></pre></td></tr></table></figure>
<p>这里 class_dict 的类型是 <code>dict[str, RayClassWithInitArgs]</code>，前者是一个 key string，后者是一个预先保存初始化 RLHF 模块参数的包装类，取 <code>RayClassWithInitArgs.cls</code>就可以得到原本的 user_defined_cls。key 和 user_defined_cls 的对应关系如下：</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>User_defined_cls</th>
</tr>
</thead>
<tbody>
<tr>
<td>actor_rollout</td>
<td>ActorRolloutRefWorker</td>
</tr>
<tr>
<td>critic</td>
<td>CriticWorker</td>
</tr>
<tr>
<td>ref</td>
<td>ActorRolloutRefWorker</td>
</tr>
<tr>
<td>rm</td>
<td>RewardModelWorker</td>
</tr>
</tbody>
</table>
<p>在 <code>create_colocated_worker_cls</code>中包含着 WorkerDict 的<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L440-L451">初始化</a>逻辑：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WorkerDict</span>(<span class="title class_ inherited__">worker_cls</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.worker_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key, user_defined_cls <span class="keyword">in</span> cls_dict.items():</span><br><span class="line">            user_defined_cls = _unwrap_ray_remote(user_defined_cls)</span><br><span class="line">            <span class="comment"># directly instantiate the class without remote</span></span><br><span class="line">            <span class="keyword">with</span> patch.<span class="built_in">dict</span>(os.environ, &#123;<span class="string">&#x27;DISABLE_WORKER_INIT&#x27;</span>: <span class="string">&#x27;1&#x27;</span>&#125;):</span><br><span class="line">                <span class="variable language_">self</span>.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(<span class="string">&#x27;args&#x27;</span>, ()),</span><br><span class="line">                                                         **init_args_dict[key].get(<span class="string">&#x27;kwargs&#x27;</span>, &#123;&#125;))</span><br></pre></td></tr></table></figure>
<p>注意哦，在这里 <code>worker_cls</code>就是 Worker，而所有 user_defined_cls 也都继承 Worker。所以 WorkerDict 初始化过程不仅会运行一个 Worker 的完整 <code>__init__</code> 函数，而且还会创建所有 user_defined_cls 并运行一个不做分布式初始化的 <code>__init__</code> 函数。因此一个 WorkerDict 其实同时包含了 <code>ActorRolloutRefWorker、CriticWorker、RewardModelWorker</code>的所有实例。</p>
<p>接下来，<code>_bind_workers_method_to_parent</code>函数将这些 user_defined_cls 的所有被<code>@register</code>装饰的公开方法绑定到 WorkerDict 本身的方法中。</p>
<p>通过调试，我们可以看到一个 WorkerDict 绑定了哪些方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(Pdb) p self.workers</span><br><span class="line">[Actor(create_colocated_worker_cls.&lt;locals&gt;.WorkerDict, 0beb69e9e6b716d1650b3c4601000000)]</span><br><span class="line"></span><br><span class="line">(Pdb) p dir(self.workers[0]) </span><br><span class="line">[&#x27;__init__&#x27;, &#x27;__new__&#x27;, &#x27;__ray_call__&#x27;, &#x27;__ray_ready__&#x27;, &#x27;__ray_terminate__&#x27;, </span><br><span class="line">&#x27;_configure_before_init&#x27;, &#x27;_configure_with_meta&#x27;, &#x27;_get_free_port&#x27;, </span><br><span class="line">&#x27;_get_node_ip&#x27;, &#x27;_get_pid&#x27;, &#x27;actor_rollout_compute_log_prob&#x27;, &#x27;actor_rollout_compute_ref_log_prob&#x27;, </span><br><span class="line">&#x27;actor_rollout_execute_func_rank_zero&#x27;, &#x27;actor_rollout_execute_with_func_generator&#x27;,</span><br><span class="line">&#x27;actor_rollout_generate_sequences&#x27;, &#x27;actor_rollout_init_model&#x27;, &#x27;actor_rollout_load_checkpoint&#x27;, </span><br><span class="line">&#x27;actor_rollout_save_checkpoint&#x27;, &#x27;actor_rollout_update_actor&#x27;, &#x27;critic_compute_values&#x27;,</span><br><span class="line">&#x27;critic_execute_func_rank_zero&#x27;, &#x27;critic_execute_with_func_generator&#x27;, &#x27;critic_init_model&#x27;,</span><br><span class="line">&#x27;critic_load_checkpoint&#x27;, &#x27;critic_save_checkpoint&#x27;, &#x27;critic_update_critic&#x27;, &#x27;execute_func_rank_zero&#x27;,</span><br><span class="line">&#x27;execute_with_func_generator&#x27;, &#x27;get_availale_master_addr_port&#x27;, &#x27;get_cuda_visible_devices&#x27;,</span><br><span class="line">&#x27;get_master_addr_port&#x27;, &#x27;ref_compute_log_prob&#x27;, &#x27;ref_compute_ref_log_prob&#x27;, </span><br><span class="line">&#x27;ref_execute_func_rank_zero&#x27;, &#x27;ref_execute_with_func_generator&#x27;, &#x27;ref_generate_sequences&#x27;,</span><br><span class="line">&#x27;ref_init_model&#x27;, &#x27;ref_load_checkpoint&#x27;, &#x27;ref_save_checkpoint&#x27;, &#x27;ref_update_actor&#x27;]</span><br></pre></td></tr></table></figure>
<p>可以看到有一些带 <code>actor_rollout_</code>、<code>critic_</code>、<code>ref_</code> 前缀的方法，这些就是新增的绑定方法。</p>
<p>在调用 WorkerDict 这些新增的绑定方法时，实际上是调用了<code>ActorRolloutRefWorker / CriticWorker / RewardModelWorker</code><a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L399">对应实例的方法</a>，WorkerDict 只是起到一个代理作用。</p>
<p>从 <code>create_colocated_worker_cls</code> 返回后，我们会将这个 WorkerDict 交给 <code>RayWorkerGroup</code>，在这里，除了完成 WorkerDict 的资源分配和创建之外，我们还需要额外做一个工作，那就是将 WorkerDict 新增的绑定方法再绑定到这个<code>RayWorkerGroup</code>上去，<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L203">这个工作</a>在 <code>_bind_worker_method</code>里面完成。</p>
<p>从代码中可以看出，<code>RayWorkerGroup</code>上的绑定方法具有了自由执行 dispatch、execute 和 collect 方法的<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L36-L46">功能</a>，可以按照 <code>@register</code> 预先指定的数据分发、集合方案和运行方案来指配每个 WorkerDict 实际接受到/应该返回的数据，这些方案可以从 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/v0.2.0.post2/verl/single_controller/base/decorator.py">decorator.py</a> 中找到。</p>
<p>我们从上至下理一下调用的先后顺序，当我们在 <code>RayPPOtrainer</code> 中调用<code>RayWorkerGroup</code>的某个绑定方法时，首先会运行数据分发逻辑（例如 broadcast 和 split），然后执行 execute 逻辑（所有 WorkerDict 都跑任务，或者只有 rank0 跑任务，等等），将任务和数据下发到每个 WorkerDict，每个 WorkerDict 在 remote 拿到数据后开始执行任务，任务执行完成后，结果被 <code>RayWorkerGroup</code>捕获，它随后执行数据的 collect 逻辑（reduce、concat 等），最后返回处理后的数据给 <code>RayPPOtrainer</code>。</p>
<hr>
<p>那么还剩下最后一个问题，当我们<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/trainer/ppo/ray_trainer.py%23L695">调用</a> <code>init_model()</code> 时，我们怎么知道它应该调用的是 Critic 的<code>critic_init_model</code> 方法，还是 Ref Model 的 <code>ref_init_model</code> 方法呢？veRL 的处理方法是在原有<code>RayWorkerGroup</code>的基础上，<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/trainer/ppo/ray_trainer.py%23L688">spawn</a> 出 4 个几乎一模一样的<code>RayWorkerGroup</code>，分别命名为 actor_rollout_wg、critic_wg、ref_policy_wg、rm_wg，每个 wg 对应一个 PPO 的模型。然后 veRL 对这些 spawn 出来的 wg 做了一个 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L298">rebind</a>，其实就是重命名。例如对于 actor_rollout_wg 而言，它的 <code>actor_rollout_init_model</code>方法就会复制一份，重命名为 <code>init_model</code>，这样调用 <code>actor_rollout_wg.init_model()</code>就等价于调用原来那个 <code>RayWorkerGroup</code>的 <code>actor_rollout_init_model</code>方法，类似地可以对其他 wg 和绑定方法做 rebind 处理。</p>
<p>经过上面的一系列处理后，我们调用 <code>actor_rollout_wg.init_model()</code>，就可以让 remote 的所有 WorkerDict 运行 <a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/workers/fsdp_workers.py%23L333">actor_rollout 的模型初始化函数</a>了。尽管这个工程实现较为复杂，但最后的效果是能让指定的 WorkerDict 运行任何模块的公开方法，并自动处理数据分发和接受逻辑，总体而言是非常精妙的设计！</p>
<p>理解了这个部分，我们就可以跳出技术细节，从宏观上就可以看出，veRL<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/ray/base.py%23L214">在 Ray 层面的调度</a>非常简单，就是每卡调度一个 WorkerDict 作为 Ray Actor，并让所有模块的对应分片共享这一个 WorkerDict 所分配到的资源。创建每一个 WorkerDict（本质上是 Worker）的方式和 OpenRLHF 基本一致，先创建 rank0 worker，拿到 master addr / port 后，再创建其他 worker。不过 veRL 还<a href="https://link.zhihu.com/?target=https%3A//github.com/volcengine/verl/blob/fb532783ad3176b4f2a1acbe4f75a5d695b4e0b4/verl/single_controller/base/worker.py%23L112">创建了一个 register center</a> 用来管理这个 master addr / port，这个 center 就是一个独立的 cpu Ray Actor。</p>
<h3 id="3-3-Data-Control-Flow">3.3. Data/Control Flow</h3>
<p>veRL 采用的 single control 设计将控制逻辑集中在 <code>RayPPOTrainer</code> 里面，首先运行了 <code>init_workers</code>初始化 WorkerDict 及每个模块的模型分片，而 <code>fit</code> 部分就是 PPO 算法的主体逻辑了。<code>RayPPOTrainer</code>里面所有与 wg 相关的 method 调用，都可以参考上面整理的思路来 trace 它的实际运行步骤。PPO 算法部分就留给大家自行阅读了～</p>
<h2 id="4-总结与评价">4. 总结与评价</h2>
<p>到这里我们总结一下 OpenRLHF 和 veRL 两大框架，然后穿插着我的一些个人观点来讲吧～</p>
<ul>
<li><strong>模块设计和 Backend</strong></li>
</ul>
<p>这个部分两个 RL 框架的设计大同小异，PPO 训练的各个主体都有单独的模块和调用方法，同时每个主体的训推 pipeline 一定程度上都是互相独立的。主要区别在于 <strong>veRL 可以兼容并扩展多种训推后端，并且可以任意组合</strong>，例如 FSDP/Megatron + vLLM/SGLang。</p>
<p>实际上我在开发训练框架的时候，也提到过 veRL 的这种兼容多 backend 的思想，具体来说是让用户在配置文件中仅修改一个 backend 参数，就能在 Deepspeed、Megatron、torchtitan（或者叫 torch native dist training）以及其他训练引擎之间任意切换。要统一各种引擎的训练接口以及训练精度，还是一个庞大的工程项目呢～（但愿我们能够做成hhh</p>
<ul>
<li><strong>Ray 调度和资源管理</strong></li>
</ul>
<p><strong>两个框架起 Ray Worker 和分配资源的方式有些微的区别</strong>：OpenRLHF 是每个模型都起一组 worker，只不过不同模型间对应 rank 的 worker 共享同一个 placement group 的 bundle；而 veRL 是直接让几个模型共生于一个 WorkerDict 里面了，自然也是共享了资源。我个人比较喜欢第一种方案，因为简单明了，veRL 的方法多多少少有点 tricky 了，为了实现这个“共生体”在工程上可费了不少劲。</p>
<p>Colocate/Hybrid Engine 以及类似的功能还是非常有必要的，一方面，在 RL 这种模型之间存在 pipeline 有强依赖的任务上，如果资源隔离就容易导致 A 在跑但 B 在等待，B 在跑 A 又不动了，让 A、B 资源共享就不至于让 GPU 闲置。另一方面，如果任务编排和显存 offload 策略得当，让模型间资源共享，是可以将投入产出压榨到极致的（要知道 RL 训练对 GPU 资源的需求普遍较高），这有利于推动 RL 训练的民主化呀！</p>
<ul>
<li><strong>Data flow 与 Control flow</strong></li>
</ul>
<p>相比于如何理清各个模块之间的关系，写出正确的 data flow 和 control flow，我其实更关心<strong>这两个 flow 应该运行在何处</strong>，因为它直接影响整个系统的性能表现。OpenRLHF 将这两个 flow 都集中在 Actor 所处的 workers 上，Actor worker 同时承担计算、数据分发和任务调度等工作，因此 Actor 很可能是整个系统的性能瓶颈。相对而言，veRL 将主体的控制逻辑和数据传输集中在 single controller 这个中枢上，而将每个个体的主观能动的逻辑交由它们自行处理，也许是性能上更好的策略。从代码可读性和扩展性上说，这种统一管理的方式也比较有优势吧。</p>
<p>另外一个需要关注的点是，两大框架的许多数据传输都依赖于 Ray 的 Object Store，<strong>但一旦通信的数据量非常大，序列化和反序列化的时间会显著变长</strong>，同时 GPU 和 CPU 之间的来回拷贝也较为耗时，这在一些传递大 Tensor 的场景下（比如蒸馏场景下的 logits 等）会有明显的性能瓶颈。如果能将大 Tensor 的通信替换为 NCCL 或 IPC 通信，并充分利用 Ray 的异步性（运用 Double Buffer 等技巧），我觉得还有大量潜在的性能提升空间。当然要做到这些，那么如何管理显存资源，如何处理数据传输的存放位置，甚至可能要考虑如何动态做 offload/onload，在工程上都要花时间来处理。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Roger-Lv</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/">http://example.com/2025/08/13/2025-08-13-%E5%9F%BA%E4%BA%8E-Ray-%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E6%9E%B6%E6%9E%84veRL%E3%80%81OpenRLHF-%E5%B7%A5%E7%A8%8B%E8%AE%BE%E8%AE%A1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Roger-Lv's space</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/Ray.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/13/2025-08-13-MCP-Zero%EF%BC%9ALLM%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%BB%E5%8A%A8%E5%B7%A5%E5%85%B7%E5%8F%91%E7%8E%B0%E7%9A%84%E6%96%B0%E8%8C%83%E5%BC%8F/" title="MCP-Zero：LLM智能体主动工具发现的新范式"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">MCP-Zero：LLM智能体主动工具发现的新范式</div></div><div class="info-2"><div class="info-item-1">MCP-Zero：LLM智能体主动工具发现的新范式 转自：https://zhuanlan.zhihu.com/p/1928760473630798292 引言 大语言模型（LLMs）在处理复杂任务时，通常需要借助外部工具来扩展其能力范围。然而，当前 LLM 智能体与工具集成的主流范式存在显著局限性：它们往往将预定义的工具模式注入到系统提示中，导致模型扮演被动选择者的角色，而非主动发现所需能力。这种方法不仅造成了巨大的上下文开销，也限制了模型的决策自主性。 为了解决这些问题，本文引入了 MCP-Zero，一个旨在恢复 LLM 智能体工具发现自主性的主动框架。MCP-Zero 的核心思想是，智能体能够主动识别自身能力差距，并按需请求特定工具，从而将自身从大规模检索器转变为真正的自主智能体。该框架通过三大核心机制运行：主动工具请求、分层语义路由和迭代能力扩展。这些机制共同作用，使得 MCP-Zero 能够在最小化上下文开销和保持高准确性的前提下，动态构建多步工具链。   图：LLM 智能体的工具选择范例比较。(a) 基于系统提示的方法将所有 MCP 工具模式注入上下文，导致提示过长，...</div></div></div></a><a class="pagination-related" href="/2025/08/13/2025-08-13-%E6%BC%AB%E8%B0%88%20LLM%20%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5%EF%BC%9A%E9%87%87%E6%A0%B7%E7%AD%96%E7%95%A5%EF%BC%88%E8%B4%AA%E5%BF%83%E8%A7%A3%E7%A0%81%E3%80%81%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E3%80%81Top-K%20%E9%87%87%E6%A0%B7%E3%80%81Top-P%20%E9%87%87%E6%A0%B7%E3%80%81%E6%A0%B8%E9%87%87%E6%A0%B7%EF%BC%89%E5%92%8C%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5%EF%BC%88%20Beam%20Search%EF%BC%89/" title="漫谈 LLM 解码策略-采样策略 贪心解码、随机采样、Top-K 采样、Top-P 采样、核采样 和搜索策略Beam Search"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">漫谈 LLM 解码策略-采样策略 贪心解码、随机采样、Top-K 采样、Top-P 采样、核采样 和搜索策略Beam Search</div></div><div class="info-2"><div class="info-item-1">漫谈 LLM 解码策略：采样策略（贪心解码、随机采样、Top-K 采样、Top-P 采样、核采样）和搜索策略（ Beam Search） 转载：https://zhuanlan.zhihu.com/p/29031912458 一. 前言 解码策略是大语言模型（Large Language Model, LLM）生成最终文本的关键环节，它直接影响文本的流畅性、连贯性和多样性。为了生成尽可能高质量的文本，研究者们发挥自己的聪明才智设计了各种各样的解码策略，以在准确性和创造行之间取得平衡。本文将系统梳理并总结常见的解码策略，涵盖**贪心解码（Greedy Decoding）、随机采样（Random Sampling）、Top-K 采样、Top-P 采样（核采样）以及束搜索（Beam Search）**等策略，帮助读者理解各策略的工作原理，并探讨它们各自的优缺点。 注：笔者水平有限，若有描述不当之处，敬请大家批评指正，与大家共同进步！ 二. 什么是解码（Decoding）？ 在大语言模型（如 Deepseek、GPT-4o）中，解码指的是模型在生成文本时，按照一定规则逐步选取下一个tok...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/11/2024-09-11-RLHF/" title="RLHF"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/RL.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-11</div><div class="info-item-2">RLHF</div></div><div class="info-2"><div class="info-item-1">RLHF 从零实现ChatGPT——RLHF技术笔记 - 知乎 (zhihu.com) 一文读懂「RLHF」基于人类反馈的进行强化学习-CSDN博客 大模型 | 通俗理解RLHF基础知识以及完整流程-CSDN博客 </div></div></div></a><a class="pagination-related" href="/2024/09/11/2024-09-11-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="强化学习笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/RL.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-11</div><div class="info-item-2">强化学习笔记</div></div><div class="info-2"><div class="info-item-1">强化学习笔记 强化学习极简入门：通俗理解MDP、DP MC TC和Q学习、策略梯度、PPO-CSDN博客 </div></div></div></a><a class="pagination-related" href="/2025/08/14/2025-08-13-Qwen3%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/" title="Qwen3技术报告解读"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/Qwen.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="info-item-2">Qwen3技术报告解读</div></div><div class="info-2"><div class="info-item-1">Qwen3技术报告解读 转自：https://zhuanlan.zhihu.com/p/1905926139756680880 模型架构 Qwen3系列，包括6个Dense模型，分别是Qwen3-0.6B、Qwen3-1.7B、Qwen3-4B、Qwen3-8B、Qwen3-14B和Qwen3-32B；2个MoE模型，分别是Qwen3-30B-A3B和Qwen3-235B-A22B。 Qwen3 Dense模型的架构与Qwen2.5相似，包括GQA、SwiGLU、RoPE以及RMSNorm with pre-normalization。此外，移除了Qwen2中使用的QKV偏置，并在注意力机制中引入了QK-Norm，以确保Qwen3的稳定训练。  Qwen3 MoE模型采用了细粒度专家分割，共有128个专家，激活8个专家。但与Qwen2.5-MoE不同，Qwen3-MoE去除了共享专家。同时，采用了全局批次负载平衡损失。  预训练 预训练数据共36T Tokens，包含119种语言和方言，涉及代码、STEM、推理任务、书籍、合成数据等。 其中，有部分数据是Qwen2.5-VL模型对...</div></div></div></a><a class="pagination-related" href="/2025/08/14/2025-08-14-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%E6%8A%80%E6%9C%AF/" title="大模型蒸馏技术"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/Distillation.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="info-item-2">大模型蒸馏技术</div></div><div class="info-2"><div class="info-item-1">导读 在人工智能快速发展的今天，模型的规模越来越大，计算成本也越来越高，这对中小型开发者来说无疑是一个巨大的挑战：如何通过将大模型的知识和能力浓缩到更小、更轻量化的模型中，降低硬件要求，以更低的成本享受到先进的人工智能技术？  DeepSeek-R1及其API的开源标志着这一领域的重要突破。 对于中小型开发者而言，这意味着他们不再需要依赖庞大的计算资源就能实现高效、强大的人工智能应用。DeepSeek提供的开源蒸馏检查点（如基于Qwen2.5和Llama3系列的1.5B、7B、8B等参数规模）为开发者提供了丰富的选择空间，无论是初创公司还是个人项目，都可以根据自身需求灵活调用这些模型。   github 地址：https://github.com/deepseek-ai/DeepSeek-R1  这一技术不仅降低了人工智能的准入门槛，也为中小型开发者在资源有限的情况下实现创新提供了更多可能性。通过蒸馏模型，他们可以更专注于业务逻辑和应用场景的优化，而无需过多关注底层计算资源的限制。这无疑将推动人工智能技术在更广泛的领域中落地生根。  接下来，详细跟大家聊聊模型蒸馏。  一、为什么...</div></div></div></a><a class="pagination-related" href="/2025/08/20/2025-08-20-UloRLAn-Ultra-Long-Output-Reinforcement-Learning-Approach-for/" title="UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-20</div><div class="info-item-2">UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities</div></div><div class="info-2"><div class="info-item-1">UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities 论文链接：https://arxiv.org/pdf/2507.19766 转自：https://zhuanlan.zhihu.com/p/1932380821412638989 得益于Test-time Scaling的成功，大模型的推理能力取得了突破性的进展。为了探索Test-time Scaling的上限，我们尝试通过强化学习来扩展模型输出长度，以提升模型的推理能力。然而，强化学习在处理超长输出时面临两个问题：1) 由于输出长度的长尾分布问题，整体的训练效率低下；2) 超长序列的训练过程中会面临熵崩塌问题。为应对这些挑战，我们对GRPO做了一系列优化，提出了一个名为UloRL的强化学习算法。在Qwen3-30B-A3B的实验表明，通过我们的方法进行强化训练，模型在AIME-2025上由70.9提升到85.1，在BeyondAIME上由50.7提升...</div></div></div></a><a class="pagination-related" href="/2025/08/13/%E5%9B%BE%E8%A7%A3%E5%A4%A7%E6%A8%A1%E5%9E%8BRLHF%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%9A%E4%BA%BA%E4%BA%BA%E9%83%BD%E8%83%BD%E7%9C%8B%E6%87%82%E7%9A%84PPO%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/" title="TD lamda和GAE"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/AsyncRL.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-13</div><div class="info-item-2">TD lamda和GAE</div></div><div class="info-2"><div class="info-item-1">图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读 参考：https://zhuanlan.zhihu.com/p/677607581 </div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Roger-Lv</div><div class="author-info-description">Send a flare and light the way.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">169</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">147</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">48</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Roger-Lv"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Roger-Lv" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1150568956@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zhongrenjie-lv-5588a928a/" target="_blank" title="LinkedIn"><i class="iconfont icon-linkedin-fill"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">基于 Ray 的分离式架构:veRL、OpenRLHF 工程设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%88%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%8B-Ray"><span class="toc-number">1.1.</span> <span class="toc-text">先简单介绍下 Ray</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. 启动方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%BF%90%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. 运行逻辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6"><span class="toc-number">1.1.3.</span> <span class="toc-text">3. 资源调度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%A7%A3%E6%9E%90-OpenRLHF"><span class="toc-number">1.2.</span> <span class="toc-text">2. 解析 OpenRLHF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AE%AD%E6%8E%A8%E6%A8%A1%E5%9D%97%E4%B8%8E-backend"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1. 训推模块与 backend</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Ray-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E4%B8%8E-colocate"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2. Ray 资源调度与 colocate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Data-Control-Flow-%E6%A2%B3%E7%90%86"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3. Data&#x2F;Control Flow 梳理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%A7%A3%E6%9E%90-veRL"><span class="toc-number">1.3.</span> <span class="toc-text">3. 解析 veRL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AE%AD%E6%8E%A8%E6%A8%A1%E5%9D%97%E5%92%8C-backend"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1. 训推模块和 backend</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Ray-%E8%B0%83%E5%BA%A6%E4%B8%8E-Hybrid-Engine"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2. Ray 调度与 Hybrid Engine</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Data-Control-Flow"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3. Data&#x2F;Control Flow</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%AF%84%E4%BB%B7"><span class="toc-number">1.4.</span> <span class="toc-text">4. 总结与评价</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-DeepResearch%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E6%96%B9%E6%A1%88/" title="DeepResearch多智能体方案"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-DeepResearch%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E6%96%B9%E6%A1%88/" title="DeepResearch多智能体方案">DeepResearch多智能体方案</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-LangGraph-%E5%85%AB%E8%82%A1/" title="LangGraph 八股"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-LangGraph-%E5%85%AB%E8%82%A1/" title="LangGraph 八股">LangGraph 八股</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E5%89%8D%E6%B2%BF%E6%B1%87%E6%80%BB/" title="多智能体系统工作前沿汇总"><div style="background: /img/cover/langgraph.jepg"></div></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E5%89%8D%E6%B2%BF%E6%B1%87%E6%80%BB/" title="多智能体系统工作前沿汇总">多智能体系统工作前沿汇总</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/15/2025-12-15-%E5%A4%9A%E6%A8%A1%E6%80%81RAG%E6%A3%80%E7%B4%A2ColPail%E5%92%8CDSE/" title="多模态RAG检索"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/mrag.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多模态RAG检索"/></a><div class="content"><a class="title" href="/2025/12/15/2025-12-15-%E5%A4%9A%E6%A8%A1%E6%80%81RAG%E6%A3%80%E7%B4%A2ColPail%E5%92%8CDSE/" title="多模态RAG检索">多模态RAG检索</a><time datetime="2025-12-14T16:00:00.000Z" title="发表于 2025-12-15 00:00:00">2025-12-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/10/2025-12-10-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Megatron-LM/" title="深入理解Megatron-LM"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/Megatron.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入理解Megatron-LM"/></a><div class="content"><a class="title" href="/2025/12/10/2025-12-10-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Megatron-LM/" title="深入理解Megatron-LM">深入理解Megatron-LM</a><time datetime="2025-12-09T16:00:00.000Z" title="发表于 2025-12-10 00:00:00">2025-12-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2024 - 2025 By Roger-Lv</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.4.2"></script><script src="/js/main.js?v=5.4.2"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'smA3tZdRGodG2VgnMubBQjLm-gzGzoHsz',
      appKey: 'biCDxj0lSBtZTMie2kNIKErd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><div class="aplayer no-destroy" data-id="8674547170" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.4.2"></script></div></div></body></html>