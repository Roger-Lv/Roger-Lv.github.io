<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SFT专攻Pass@k，RL强化Pass@1? | Roger-Lv's space</title><meta name="author" content="Roger-Lv"><meta name="copyright" content="Roger-Lv"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深挖RLVR探索机制：SFT专攻Pass@k，RL强化Pass@1转自：https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;QSi580SJ2RFewyFirAe65A 先前的工作已经证明了 RLVR 在实践中的成功，但其背后的根本机制，特别是模型在训练过程中的探索行为，仍有待深入研究。来自中国人民大学高瓴人工智能学院的研究者们发表了一篇题为《From Trial-and-Error to Im">
<meta property="og:type" content="article">
<meta property="og:title" content="SFT专攻Pass@k，RL强化Pass@1?">
<meta property="og:url" content="http://example.com/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/index.html">
<meta property="og:site_name" content="Roger-Lv&#39;s space">
<meta property="og:description" content="深挖RLVR探索机制：SFT专攻Pass@k，RL强化Pass@1转自：https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;QSi580SJ2RFewyFirAe65A 先前的工作已经证明了 RLVR 在实践中的成功，但其背后的根本机制，特别是模型在训练过程中的探索行为，仍有待深入研究。来自中国人民大学高瓴人工智能学院的研究者们发表了一篇题为《From Trial-and-Error to Im">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cover/RLVR.png">
<meta property="article:published_time" content="2025-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-21T03:24:17.831Z">
<meta property="article:author" content="Roger-Lv">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="SFT">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cover/RLVR.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SFT专攻Pass@k，RL强化Pass@1?",
  "url": "http://example.com/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/",
  "image": "http://example.com/img/cover/RLVR.png",
  "datePublished": "2025-08-20T16:00:00.000Z",
  "dateModified": "2025-08-21T03:24:17.831Z",
  "author": [
    {
      "@type": "Person",
      "name": "Roger-Lv",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="http://example.com/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.4.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":-1,"unescape":true,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SFT专攻Pass@k，RL强化Pass@1?',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">101</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">112</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Roger-Lv's space</span></a><a class="nav-page-title" href="/"><span class="site-name">SFT专攻Pass@k，RL强化Pass@1?</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div><!-- 添加搜索按钮 ↓--><span class="search-button"><i class="fas fa-search" aria-hidden="true"></i></span></div></nav><div id="post-info"><h1 class="post-title">SFT专攻Pass@k，RL强化Pass@1?</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-20T16:00:00.000Z" title="发表于 2025-08-21 00:00:00">2025-08-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-21T03:24:17.831Z" title="更新于 2025-08-21 11:24:17">2025-08-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/" data-flag-title="SFT专攻Pass@k，RL强化Pass@1?"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span class="leancloud-visitors-count"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="深挖RLVR探索机制：SFT专攻Pass-k，RL强化Pass-1"><a href="#深挖RLVR探索机制：SFT专攻Pass-k，RL强化Pass-1" class="headerlink" title="深挖RLVR探索机制：SFT专攻Pass@k，RL强化Pass@1"></a>深挖RLVR探索机制：SFT专攻Pass@k，RL强化Pass@1</h1><p>转自：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/QSi580SJ2RFewyFirAe65A">https://mp.weixin.qq.com/s/QSi580SJ2RFewyFirAe65A</a></p>
<p>先前的工作已经证明了 RLVR 在实践中的成功，但其背后的根本机制，特别是模型在训练过程中的探索行为，仍有待深入研究。来自中国人民大学高瓴人工智能学院的研究者们发表了一篇题为《From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR》的技术报告，系统性地研究了RLVR 中的探索机制。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL880oOkkr4fp0qaL20Y5mqGQyGQXndXY9kXTqHFKoLmmxXI84jk1dichQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<ul>
<li>论文题目：From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR</li>
<li>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.07534">https://arxiv.org/pdf/2508.07534</a></li>
</ul>
<p>这篇报告结合了详尽的文献回顾和创新的实证分析，围绕<strong>探索空间塑造</strong>、<strong>熵与性能的相互作用</strong>以及<strong>强化学习性能优化</strong>这三个维度展开。其目标是为理解和推进 RLVR 系统提供一个基础性框架。本文将详细解读这篇报告的核心内容、创新发现以及实践意义。</p>
<h2 id="一、-量化探索能力"><a href="#一、-量化探索能力" class="headerlink" title="一、 量化探索能力"></a><strong>一、 量化探索能力</strong></h2><p>为了系统性地研究探索机制，首先需要能够量化模型的探索能力。报告认为，模型的探索能力本质上受其“能力边界”（Ability Boundary）的限制，即模型解决问题的能力上限。只有在模型能力边界内的“可探索空间”中，RLVR 训练才能生效。为此，报告提出并扩展了两种核心指标来刻画这一边界。</p>
<h3 id="1-1-Pass-k-度量与“k-rollout-不可解问题”"><a href="#1-1-Pass-k-度量与“k-rollout-不可解问题”" class="headerlink" title="1.1 Pass@k 度量与“k-rollout 不可解问题”"></a><strong>1.1 Pass@k 度量与“k-rollout 不可解问题”</strong></h3><p><strong>Pass@k</strong> 是一个广泛用于评估代码生成和数学推理任务的指标。它的含义是：对于一个给定的问题，模型尝试 <code>k</code> 次，只要有一次能够生成正确答案，就认为该问题被解决了。Pass@k 的分数代表了在整个测试集中，模型能够在 <code>k</code> 次尝试内解决的问题比例。</p>
<p>为了获得更稳定和无偏的估计，实践中通常会先让模型对每个问题生成 <code>n</code> 个候选答案（<code>n &gt;= k</code>），统计其中正确答案的数量 <code>c</code>，然后通过以下公式计算 Pass@k 的无偏估计值：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/08/21/oOlSu8LcfZEt2hU.png" alt="image.png"></p>
<p>这个公式计算的是，从 <code>n</code> 个答案中随机抽取 <code>k</code> 个，至少包含一个正确答案的概率。</p>
<p>基于 Pass@k，报告提出了一个创新的概念——<strong>k-rollout 不可解问题 (k-rollout Unsolvable Problems)</strong> 。这个指标定义为，在经过 <code>k</code> 次尝试后，模型仍然无法解决的问题集合。当 <code>k</code> 值足够大时，这个集合趋于稳定，可以被视为模型当前能力边界的一个实用度量。通过比较不同模型（例如，基础模型、SFT 模型、RL 模型）的不可解问题集合的差异和重叠，研究者可以更直观地分析不同训练方法如何改变模型的能力边界。</p>
<h3 id="1-2-策略熵与“Rollout-分支因子”"><a href="#1-2-策略熵与“Rollout-分支因子”" class="headerlink" title="1.2 策略熵与“Rollout 分支因子”"></a><strong>1.2 策略熵与“Rollout 分支因子”</strong></h3><p>除了评估最终结果的 Pass@k，报告还关注生成过程中的多样性，并使用<strong>策略分布的熵 (Entropy of Policy Distribution)</strong> 来度量。在信息论中，熵是系统不确定性的度量。在 LLM 中，词元级别的熵（Token-level Entropy）量化了模型在生成每个词元时的不确定性。第 个词元的熵 计算公式如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2025/08/21/KCNFodlz3V8hpiB.png" alt="image.png"></p>
<p>其中， 是词汇表， 是模型在给定前文 的条件下，生成词元 的概率。更高的熵值意味着模型在当前位置的选择更多样，不确定性更高，反映了更强的探索行为。</p>
<p>为了更直观地衡量这种生成多样性，报告进一步提出了<strong>Rollout 分支因子 (Rollout Branching Factor)</strong> 。这个指标借鉴了常见的 <code>top-p</code> 采样策略（例如 <code>top-p = 0.95</code>），将累积概率达到 95% 的所有候选词元视为“有效候选集”。Rollout 分支因子就是这个集合中词元的数量。这个值越大，表明模型在当前步骤可供选择的、可能性较高的路径就越多，其探索能力和能力边界也相应更强。</p>
<h3 id="1-3-实验对比：SFT-vs-RL"><a href="#1-3-实验对比：SFT-vs-RL" class="headerlink" title="1.3 实验对比：SFT vs. RL"></a><strong>1.3 实验对比：SFT vs. RL</strong></h3><p>报告通过一系列精巧的实验，对比了监督微调（Supervised Fine-Tuning, SFT）和强化学习（RL）对模型探索能力的影响。实验设置了一个序贯的训练流程：</p>
<ol>
<li><strong>基础模型 (Base):</strong> Qwen2.5-Math-7B</li>
<li><strong>SFT 模型:</strong> 在基础模型上进行 SFT 得到的 DeepSeek-R1-Distill-Qwen-7B</li>
<li><strong>RL 模型:</strong> 在 SFT 模型上进一步进行 RL 训练得到的 Skywork-OR1-Math-7B</li>
</ol>
<p>实验得出了三个关键结论：</p>
<ul>
<li><strong>SFT 扩展 Pass@k 边界，RL 锐化 Pass@1 性能：</strong>如下图 (a) 所示，SFT 显著提升了模型的 Pass@k 分数，意味着 SFT 能够通过学习外部高质量数据，有效扩大模型的能力边界，使其能够解决更多之前无法解决的问题。 相比之下，RL 模型的 Pass@k 曲线几乎没有提升，甚至略有下降，但 RL 训练通常会提升 Pass@1 性能（即首次尝试的成功率）。这表明 RL 的作用更侧重于“利用”（Exploitation），即在已有的解空间内优化和强化最高概率的正确路径，而这种优化是以牺牲探索广度为代价的。图 (b) 中的答案多样性对比也证实了这一点，RL 模型的答案多样性下降最为显著。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL863dk8icwe29mwDAgsKeHoGjtMOJnL00koDpYCxGo6GrDQiaib6R4CQHMA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8F1IptwtGibCsM1C779Cu7niaIa3DfzhKaq24YjPGm4ZvofbiasKqL62cA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<ul>
<li><strong>SFT 和 RL 都会“移动”能力边界，而非简单扩展：</strong>通过分析三个模型的“不可解问题”集合（图 c），报告发现这些集合之间并非简单的包含关系。这意味着训练过程（无论是 SFT 还是 RL）不仅仅是单向地扩大能力边界，还会导致一些原本可解的问题变得不可解。这揭示了训练过程的复杂性：它是在重新配置模型的能力，而非简单的线性增强。值得注意的是，RL 模型的不可解问题集比 SFT 模型更大，再次说明 RL 倾向于收窄探索空间。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8FOhGGjGiaCIaCdsF0sNC18rllmcSfckPia1grOphyOlzMCwLiahiblZdDA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<ul>
<li><strong>SFT 促进词元级多样性，RL 导致策略更固化：</strong>通过对比 Rollout 分支因子（图 d），报告发现，无论是针对特定领域（SFT-Math）还是多领域（SFT-All）的 SFT，都能显著增加分支因子，即提升模型在每个生成步骤的候选词元多样性。而 RL 模型的分支因子则没有增加。这说明，从高质量的外部数据中学习（SFT 的方式）是增强模型内在探索能力的关键机制。而 RL 局限于模型自身生成的数据，更倾向于强化已有的高概率路径，而不会主动促进词元层面的多样性。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL81swZHtmIAkcqPNSjEicJkn4Ox9NtMufI1oImcgJShsdB7sePvpKbfKw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<h3 id="1-4-外部工具的影响"><a href="#1-4-外部工具的影响" class="headerlink" title="1.4 外部工具的影响"></a><strong>1.4 外部工具的影响</strong></h3><p>报告还探讨了集成外部工具（如代码解释器）对模型能力边界的影响。实验对比了四种设置：纯文本推理的基础模型、使用代码推理的基础模型、经过 RL 训练的纯文本模型，以及经过工具增强 RL 训练的代码模型。结果（如下图所示）清晰地表明，集成工具能够极大地提升 Pass@k 性能，其效果远超单纯的 RL 训练。这证明了外部工具通过提供结构化的计算和利用能力，为模型开辟了全新的探索路径，是扩展能力边界的另一条有效途径。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8REic7lBgicYKzZkQNoqdTstLc7BsKpcsUIfc04NicxWOAuUvTUf09sqew/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><strong>小结：</strong> 这一部分通过引入新的度量指标和精细的对比实验，清晰地剖析了 SFT 和 RL 在塑造 LLM 探索能力上的不同角色。<strong>SFT 通过学习外部知识来拓宽视野（提升 Pass@k 和词元多样性）；而 RL 则通过反复练习来精通某项技艺（提升 Pass@1），但代价是视野可能变得狭窄。</strong></p>
<h2 id="二、-熵与性能的相互作用"><a href="#二、-熵与性能的相互作用" class="headerlink" title="二、 熵与性能的相互作用"></a><strong>二、 熵与性能的相互作用</strong></h2><p>在量化了探索能力之后，报告进一步深入分析了探索（以策略熵为代表）与性能（以准确率为代表）之间的相互作用关系。研究者将 RLVR 的训练过程进行细粒度拆解，从<strong>阶段</strong>、<strong>实例</strong>和<strong>词元</strong>三个层面进行了系统研究，揭示了学习过程在不同粒度下的动态机制。</p>
<h3 id="2-1-阶段动态（Stage-level-Dynamics）：上升期与平台期"><a href="#2-1-阶段动态（Stage-level-Dynamics）：上升期与平台期" class="headerlink" title="2.1 阶段动态（Stage-level Dynamics）：上升期与平台期"></a><strong>2.1 阶段动态（Stage-level Dynamics）：上升期与平台期</strong></h3><p>先前的工作发现，RLVR 训练过程通常呈现出两个不同的阶段：</p>
<ol>
<li><strong>上升期 (Rising Stage):</strong> 模型性能快速提升，同时策略熵迅速下降。</li>
<li><strong>平台期 (Plateau Stage):</strong> 模型性能增长变得缓慢、平稳，熵的变化也趋于平缓。</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8ndW5QADKNJAcmatdibXK1zWFBSkRlibD5dIjxOorlbBPY8P4j7Gu0uEg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p>报告的核心问题是：这两个阶段的性能提升分别是由什么机制驱动的？</p>
<ul>
<li><p><strong>上升期：负样本驱动熵下降，确立推理模式</strong>通过将每个训练步的样本分为正样本（回答正确）和负样本（回答错误），并分别追踪它们的熵变化，报告发现了一个关键现象：<strong>在上升期，熵的下降主要来源于负样本</strong>。如下图 (c) 所示，负样本的平均熵始终高于正样本，并且其下降速度远快于正样本。这表明，在学习的初始阶段，模型的主要任务是通过惩罚大量的错误推理路径来快速收缩探索空间，从而摆脱混乱状态。这种对错误路径的“修剪”是模型建立起有效推理模式的基础。</p>
<p>进一步分析词元的分布变化（下图 b）也支持了这一结论。熵下降最显著的词元大多是与任务无关的、无意义的符号（如 <code>erot</code>, <code>whim</code>），而与数学推理强相关的词元（如 <code>\</code>, <code>=</code>, <code>frac</code>）的出现频率则显著增加。同时，报告还将低质量的回答分为三类：格式错误、内容无关和语言混杂。如下图 (d) 所示，在上升期，这三类错误的比例都显著下降。这都说明了上升期的学习本质上是一个“去伪存真”的过程，通过降低无关路径的概率，来巩固和强化有效的推理模式。</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8libQgACbxGvxb91ebicZWOj6DpR0DdVYzfUXHnjgiaDibrUB5AGdCezseA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8VPcRvYVhq1cGic1wGa9bkn9Wjqeticwnl2rB7sm1n9Of3EGGd6XLic5nQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<ul>
<li><p><strong>平台期：学习集中于高熵、高梯度的关键“分叉点”</strong>进入平台期后，性能提升变得困难，学习信号也变得稀疏。报告通过分析词元概率的更新发现，此时绝大多数（超过 99%）词元的概率变化都非常小。学习信号集中在了一小部分关键的词元上。如下图 (a) 所示，这些被显著更新的词元（无论是概率被增强的正样本，还是被抑制的负样本）都集中在高熵区域（图 b）。这些高熵词元往往对应着推理路径中的关键“分叉点”，模型在这些点上存在较大的不确定性。平台期的学习，正是在解决这些关键节点上的不确定性。</p>
<p>报告还将词元按照语义功能分为四类：形式推理（数字、运算符）、逻辑结构（因此、但是）、元认知（让我们检查一下）和语义支持（语法成分）。分析显示，在平台期，受到最强更新信号的，是与<strong>形式推理</strong>相关的词元。这表明，在掌握了基本的推理模式后，平台期的学习重点转向了更精细、更复杂的逻辑和计算，以攻克更难的问题。</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL86HrRNGD80nLEqlzwHhPCWxwbOGYR4QT5FUd5j3Iaowaib8efe7H5ekA/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8vIwJ0YFAtFPubuF4B7lialUmRRqcOCLJcGYYcQsG6j9HwibfkbQeN7MQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<h3 id="2-2-实例效率（Instance-level-Efficacy）：低-PPL-样本更关键"><a href="#2-2-实例效率（Instance-level-Efficacy）：低-PPL-样本更关键" class="headerlink" title="2.2 实例效率（Instance-level Efficacy）：低 PPL 样本更关键"></a><strong>2.2 实例效率（Instance-level Efficacy）：低 PPL 样本更关键</strong></h3><p>并非所有训练样本都对学习做出同等贡献。报告引入了<strong>实例级别的困惑度（Perplexity, PPL）</strong>作为衡量模型对整个生成序列不确定性的指标。PPL 越低，说明模型对生成该序列的“把握”越大，序列本身也通常更流畅、更符合模型的内在逻辑。</p>
<p>分析表明：</p>
<ul>
<li><strong>学习信号集中在低 PPL 样本中：</strong> 如下图 (a) 所示，那些经历了最大概率更新的词元，绝大多数都来自于低 PPL 的样本。这说明模型更倾向于在那些它认为“合理”的生成中进行学习和调整。</li>
<li><strong>低 PPL 样本代表更鲁棒的推理路径：</strong> 通过对低 PPL 和高 PPL 样本进行“词元替换”的干预实验，报告发现，替换低 PPL 样本中的高熵词元，对最终答案准确率的影响更小（图 b）。这说明低 PPL 样本中的推理路径更加稳定和鲁棒，即使在关键节点上做出微调，也不容易偏离正确的方向。</li>
<li><strong>优先学习低 PPL 样本能提升 RL 效果：</strong> 报告设计了一个动态重加权的实验，分别增强低 PPL 样本和高 PPL 样本在训练中的权重。结果（图 c, d）显示，优先学习低 PPL 样本能够带来更好的性能提升和更低的策略熵，而侧重于高 PPL 样本则会导致性能下降和熵的剧增。这有力地证明了，将学习资源集中于模型已经有一定把握的、更“自信”的样本上，是一种更高效的优化策略。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8a4OVA8efIY9Nds36ck1oibYMyqWBv0klxZaZYETmvHALjpS9Qibz9c8A/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<h3 id="2-3-词元重要性（Token-level-Significance）：序列末端更关键"><a href="#2-3-词元重要性（Token-level-Significance）：序列末端更关键" class="headerlink" title="2.3 词元重要性（Token-level Significance）：序列末端更关键"></a><strong>2.3 词元重要性（Token-level Significance）：序列末端更关键</strong></h3><p>最后，报告分析了词元在序列中的不同位置对其学习重要性的影响。</p>
<ul>
<li><strong>词元熵呈 U 型分布：</strong> 如下图 (a) 所示，序列的开头和结尾部分的词元熵普遍较高。开头的熵高，反映了模型在刚开始解决问题时，需要从广阔的探索空间中选择一个切入点；结尾的熵高，则反映了模型在得出最终结论时的不确定性，这与任务目标直接相关。</li>
<li><strong>开头的高熵决定方向，结尾的高熵反映不确定性：</strong> 通过词元替换实验（图 b），报告发现，替换序列<strong>开头</strong>的高熵词元，对最终结果的准确率影响巨大。这说明早期的决策对整个推理路径起着决定性作用。而替换序列<strong>结尾</strong>的高熵词元，对准确率影响则小得多。这表明结尾的高熵更多是模型对最终答案信心的体现，而非路径方向的根本性改变。</li>
<li><strong>优化序列末端的词元更高效：</strong> 基于上述发现，报告设计了给予序列不同位置词元不同奖励加成的实验。结果（图 c, d）显示，<strong>给予序列末端词元更高奖励的策略，取得了最好的性能</strong>。虽然这两种策略都会增加策略熵，但奖励末端词元会引导模型生成更长的推理链条，这表明模型花更多时间在最终决策的“精雕细琢”上，从而提高了准确率。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8hVhrsWe92Y6DFexHEiaS8NDUJ0MhG7ujZacjpiaUOe1XrgrLAbznlnVw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><strong>小结：</strong> 这一部分的细粒度分析揭示了 RLVR 训练过程的内部动态。学习并非一个均质的过程，而是<strong>分阶段、分实例、分位置</strong>的。<strong>初期靠“排除法”学习，后期靠“攻坚战”提升；学习的重点在于模型有把握的（低 PPL）样本；而优化的关键则在于序列末端的决策词元。</strong> 这些发现为后续设计更高效的 RL 算法提供了坚实的理论基础。</p>
<h2 id="三、-探索增强的强化学习方法"><a href="#三、-探索增强的强化学习方法" class="headerlink" title="三、 探索增强的强化学习方法"></a><strong>三、 探索增强的强化学习方法</strong></h2><p>在系统性地分析了探索机制之后，报告的最后一部分将这些洞见转化为具体的、可操作的性能提升方法。这部分内容分为两块：一是回顾现有的探索增强方法，二是通过实验验证基于前文发现提出的新策略。</p>
<h3 id="3-1-现有探索增强方法回顾"><a href="#3-1-现有探索增强方法回顾" class="headerlink" title="3.1 现有探索增强方法回顾"></a><strong>3.1 现有探索增强方法回顾</strong></h3><p>报告首先对领域内已有的探索增强技术进行了梳理，并将其归纳为四大类：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8JCv5o4JXA11OkMwWKS5jqweUrTXNoQfRPVQG7Os05QJA5Z3AdQPEyQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<ol>
<li><strong>优势函数优化 (Advantage Refinement):</strong> 通过修改优势函数（Advantage）来直接影响奖励信号。例如，为高熵词元提供额外的奖励，或者对负样本进行更强的惩罚。</li>
<li><strong>词元&#x2F;梯度选择 (Token&#x2F;Gradient Selection):</strong> 在反向传播时，有选择性地更新部分词元的梯度。例如，只更新高熵的“分叉词元”的梯度，或者保护那些低概率但有潜力的词元不被过度抑制。</li>
<li><strong>KL 正则化 (KL Regularization):</strong> 通过 KL 散度惩罚当前策略与某个参考策略（通常是训练前的模型）之间的差异，防止模型在训练中“跑偏”太远，从而维持一定的探索性。</li>
<li><strong>工具增强 (Tool Augmentation):</strong> 前文已述，通过集成外部工具来扩展模型的探索空间和能力边界。</li>
</ol>
<p>这些方法的核心目标都是缓解 RLVR 训练中固有的“探索空间崩塌”问题，确保模型在整个训练过程中都保持足够的多样性，从而将探索潜力有效地转化为最终的性能提升。</p>
<h3 id="4-2-维持探索能力：保留-Pass-k-的-RFT-策略"><a href="#4-2-维持探索能力：保留-Pass-k-的-RFT-策略" class="headerlink" title="4.2 维持探索能力：保留 Pass@k 的 RFT 策略"></a><strong>4.2 维持探索能力：保留 Pass@k 的 RFT 策略</strong></h3><p>考虑到标准 RL 训练的计算成本很高，报告选择使用一种更轻量级的方法——<strong>拒绝采样微调（Rejection-sampling Fine-Tuning, RFT）</strong>来进行实验。RFT 的流程是：模型生成多个候选答案，通过一个验证器（或规则）筛选出其中的正确答案，然后用这些正确的答案对模型自身进行微调。</p>
<p>为了在 RFT 过程中维持并增强模型的探索能力（即 Pass@k），报告提出了三种数据选择策略，并与基线（只用规则过滤后的正确样本）进行对比：</p>
<ol>
<li><strong>引入噪声数据 (Incorporating Noisy Data):</strong> 在训练数据中混入一小部分（5%）的负样本。目的是防止模型变得过于“确定”，通过接触少量错误来激发更广阔的探索。</li>
<li><strong>选择高熵数据 (Selecting High-Entropy Data):</strong> 优先选择那些平均词元熵最高的样本进行微调。这直接优化了生成过程的多样性。</li>
<li><strong>选择高 Rollout 分支因子数据 (Selecting High-RBF Data):</strong> 优先选择平均 Rollout 分支因子最高的样本。这是对高熵策略的进一步优化，直接针对模型在每一步的探索潜力。</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8FpW1D36NF1ZEZ511atIVK7EPsjkDicXicLveo1cYEssyVWSgsIrXNJicw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p>实验结果（如上表）非常清晰：</p>
<ul>
<li><strong>可控的噪声注入是有效的：</strong> 加入 5% 的噪声数据后，模型的 Pass@k 性能得到了提升，而 Pass@1 性能几乎不受影响。这说明适度的“干扰”有助于模型保持探索活力。</li>
<li><strong>直接优化探索指标效果显著：</strong> 基于高熵和高 RBF 的选择策略都大幅提升了 Pass@k 性能，其中 RBF 策略效果最好。这证明了直接针对探索性指标进行数据选择是一种强大的优化手段。</li>
<li><strong>探索增强的 RFT 超越了强大的 RL 基线：</strong> 报告将“噪声注入”和“高 RBF”两种策略结合，训练出的最终模型，在所有测试基准上的 Pass@k 性能都超过了一个强大的 RL 基线模型（Qwen-2.5-32B-SimpleRL-Zoo），同时 Pass@1 性能保持竞争力。这表明，通过精心设计的数据策略，RFT 这种相对简单的方法也能够有效地维持和增强模型的探索能力。</li>
</ul>
<h3 id="4-3-提升优化效率：基于-PPL-和位置的优势塑造"><a href="#4-3-提升优化效率：基于-PPL-和位置的优势塑造" class="headerlink" title="4.3 提升优化效率：基于 PPL 和位置的优势塑造"></a><strong>4.3 提升优化效率：基于 PPL 和位置的优势塑造</strong></h3><p>最后，报告将第二部分中关于实例效率（低 PPL）和词元重要性（序列末端）的发现，应用于标准的 RL 算法（GRPO）中，提出了两种简单而有效的<strong>优势塑造 (Advantage Shaping)</strong> 方法。</p>
<ol>
<li><p><strong>基于 PPL 的优势塑造：</strong>这个方法的核心是降低高 PPL 样本的学习权重。对于每个生成的样本 ，首先计算其标准化的对数 PPL 权重 。然后，将原始的词元优势 乘以一个调制因子 ，其中 是一个超参数。这样，PPL 越高的样本，其优势值就被压缩得越多，从而让模型更专注于学习那些低 PPL 的、更“靠谱”的推理路径。</p>
</li>
<li><p><strong>基于位置的优势塑造：</strong>这个方法旨在增强序列末端词元的学习信号。它定义了一个位置奖励 ，这个奖励值随着词元在序列中的相对位置增加而增加。然后，将这个奖励加到原始的优势上，并且其符号与原始优势的符号保持一致。这意味着，对于正样本，越靠后的词元奖励越高；对于负样本，越靠后的词元惩罚越重。</p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8uf7GOCJDwWvM6zZR1wrQgtLtbG0F2Jpx1B06MDLkyKSicghCw4Eh5CQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p>实验结果（如上表和下图）表明，这两种看似简单的修改都带来了显著的性能提升。与 GRPO 基线相比，它们在多个数学推理基准上都取得了更高的平均分和最高分。同时，这两种方法都使得模型在训练后期能够维持更高的熵水平，保留了更强的探索能力。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8sQPy7uEE7kXwqVBcsLTs8zLh03x2oWibMsLnyAvTuG4wiaOHZicBCnQzg/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL88Rica3icR5c8AM0TQtz3R5LwuT8ibnk0Hibu4ecluLl81HkJBpmCqXKunw/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<p>进一步的分析还发现，这两种方法都促使模型生成了更长的、包含更多形式推理和逻辑结构词元的回答。这说明，通过精准地调整奖励信号，可以引导模型进行更深入、更细致的思考过程，最终带来性能的提升。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mmbiz.qpic.cn/sz_mmbiz_png/QFAVf6ySRArlqyK9yMzu7W3gvgLhFtL8LhnDHvuu43KTSBfQnatoPBZu0UeUD0gzGGcV1DiaeLcscjIcQficcSaQ/640?wx_fmt=png&from=appmsg&watermark=1&tp=webp&wxfrom=5&wx_lazy=1" alt="图片"></p>
<h2 id="点评"><a href="#点评" class="headerlink" title="点评"></a><strong>点评</strong></h2><p>论文主要贡献可以总结为以下几点：</p>
<ol>
<li><strong>提出了新的量化指标：</strong> 引入“k-rollout 不可解问题”和“Rollout 分支因子”，与 Pass@k 和熵等传统指标相结合，为量化和比较不同训练范式下 LLM 的能力边界和探索潜力提供了更丰富的工具集。</li>
<li><strong>揭示了 SFT 和 RL 的不同作用：</strong> 通过实验清晰地证明了 SFT 主要通过学习外部数据来扩展探索边界（拓宽），而 RL 则主要通过利用自身生成的数据来锐化已有能力（挖深），并揭示了后者可能会以牺牲探索广度为代价。</li>
<li><strong>进行了细粒度的学习动态分析：</strong> 将 RLVR 训练过程分解为不同阶段、实例和词元层面，揭示了学习信号在不同粒度下的分布规律和作用机制，特别是发现了负样本、低 PPL 样本和序列末端词元在学习中的关键作用。</li>
<li><strong>提出了简单有效的优化方法：</strong> 基于分析洞见，提出了在 RFT 中通过数据选择维持探索能力，以及在 RL 中通过优势塑造提升优化效率的具体方法，并通过实验验证了其有效性。</li>
</ol>
<p>“SFT拓宽，RL挖深”是一个非常棒的总结，但在某些情况下可能过于简化。例如：</p>
<ul>
<li>如果SFT的数据集质量不高、多样性差，也可能导致模型能力收窄（过拟合）。</li>
<li>某些探索性RL算法（如加入好奇心、随机网络蒸馏等机制）也可能旨在扩展模型的能力边界，而不仅仅是利用。</li>
</ul>
<p>论文为了突出核心矛盾，可能忽略了这些中间地带和特殊情况。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Roger-Lv</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/">http://example.com/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Roger-Lv's space</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/SFT/">SFT</a><a class="post-meta__tags" href="/tags/RL/">RL</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/RLVR.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/21/2025-08-21-%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A7%A3%E9%94%81SFT%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BBDFT%E5%A6%82%E4%BD%95%E5%AE%8C%E8%83%9C%E4%BC%A0%E7%BB%9F%E5%BE%AE%E8%B0%83/" title="一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调</div></div><div class="info-2"><div class="info-item-1">一行代码，解锁SFT泛化能力：深度解读DFT如何完胜传统微调转自：https://mp.weixin.qq.com/s/XXGxRk-p5LahtqdYNnbKaA 在大型语言模型 (LLM) 的世界里，如何让模型更好地理解并遵循人类的指令，即所谓的“对齐”，始终是核心议题。目前，主流的技术路线分为两条：监督微调（Supervised Fine-Tuning, SFT）和基于人类反馈的强化学习 （Reinforcement Learning from Human Feedback, RLHF）。 SFT 简单直接，就像教一个学生做题，直接给他看大量的“问题-标准答案”对，让他去模仿。 这种方法易于实现，能让模型快速学会特定任务的“套路”。然而，它的弊病也十分明显——模型容易“死记硬背”，学到的知识很“脆”，泛化能力差，遇到没见过的题型就可能“翻车”。 相比之下，RLHF 更像是请一位教练来指导学生。它不直接给出答案，而是对模型的不同回答给出评分（奖励），让模型在不断的尝试和反馈中，自己探索出更好的策略。但它的问题在于，训练过程极其复杂，需要耗费大量的计算资源，对超参数敏感，且依赖...</div></div></div></a><a class="pagination-related" href="/2025/08/20/2025-08-20-UloRLAn-Ultra-Long-Output-Reinforcement-Learning-Approach-for/" title="UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities</div></div><div class="info-2"><div class="info-item-1">UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities论文链接：https://arxiv.org/pdf/2507.19766 转自：https://zhuanlan.zhihu.com/p/1932380821412638989 得益于Test-time Scaling的成功，大模型的推理能力取得了突破性的进展。为了探索Test-time Scaling的上限，我们尝试通过强化学习来扩展模型输出长度，以提升模型的推理能力。然而，强化学习在处理超长输出时面临两个问题：1) 由于输出长度的长尾分布问题，整体的训练效率低下；2) 超长序列的训练过程中会面临熵崩塌问题。为应对这些挑战，我们对GRPO做了一系列优化，提出了一个名为UloRL的强化学习算法。在Qwen3-30B-A3B的实验表明，通过我们的方法进行强化训练，模型在AIME-2025上由70.9提升到85.1，在BeyondAIME上由50.7提升到...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/21/2025-08-21-%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A7%A3%E9%94%81SFT%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BBDFT%E5%A6%82%E4%BD%95%E5%AE%8C%E8%83%9C%E4%BC%A0%E7%BB%9F%E5%BE%AE%E8%B0%83/" title="一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-21</div><div class="info-item-2">一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调</div></div><div class="info-2"><div class="info-item-1">一行代码，解锁SFT泛化能力：深度解读DFT如何完胜传统微调转自：https://mp.weixin.qq.com/s/XXGxRk-p5LahtqdYNnbKaA 在大型语言模型 (LLM) 的世界里，如何让模型更好地理解并遵循人类的指令，即所谓的“对齐”，始终是核心议题。目前，主流的技术路线分为两条：监督微调（Supervised Fine-Tuning, SFT）和基于人类反馈的强化学习 （Reinforcement Learning from Human Feedback, RLHF）。 SFT 简单直接，就像教一个学生做题，直接给他看大量的“问题-标准答案”对，让他去模仿。 这种方法易于实现，能让模型快速学会特定任务的“套路”。然而，它的弊病也十分明显——模型容易“死记硬背”，学到的知识很“脆”，泛化能力差，遇到没见过的题型就可能“翻车”。 相比之下，RLHF 更像是请一位教练来指导学生。它不直接给出答案，而是对模型的不同回答给出评分（奖励），让模型在不断的尝试和反馈中，自己探索出更好的策略。但它的问题在于，训练过程极其复杂，需要耗费大量的计算资源，对超参数敏感，且依赖...</div></div></div></a><a class="pagination-related" href="/2024/08/06/2024-08-06-WiNGPT2%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/" title="医疗垂直领域大模型WiNGPT2的部署和性能对比"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-06</div><div class="info-item-2">医疗垂直领域大模型WiNGPT2的部署和性能对比</div></div><div class="info-2"><div class="info-item-1">医疗垂直领域大模型WiNGPT2的部署和性能对比WiNGPT2部署和运行123456git clone https://github.com/winninghealth/WiNGPT2.gitcd WiNGPT2mkdir winninghealthcd winninghealthgit clone https://hf-mirror.com/winninghealth/WiNGPT2-7B-Chatcd ..  若没有以下这三个库，则安装gradio、tiktoken、pip install einops flash_attn 123pip install gradiopip install tiktokenpip install einops flash_attn  选一个目录作为cache（前提是这个目录已经存在且有权限读写） 12# 遇到报错 [Errno 13] Permission denied: &#x27;/data/.cache/huggingface/modules/transformers_modules/WiNGPT2-7B-Chat&#x27;expor...</div></div></div></a><a class="pagination-related" href="/2024/08/07/2024-08-07-%E6%B3%95%E5%BE%8B%E5%9E%82%E7%B1%BB%E5%A4%A7%E6%A8%A1%E5%9E%8BDISC-LawGPT%E7%9A%84%E9%83%A8%E7%BD%B2%E8%BF%90%E8%A1%8C%E5%92%8C%E5%AF%B9%E6%AF%94/" title="法律垂类大模型DISC-LawGPT的部署运行和对比"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-07</div><div class="info-item-2">法律垂类大模型DISC-LawGPT的部署运行和对比</div></div><div class="info-2"><div class="info-item-1">法律垂类大模型DISC-LawGPT的部署运行和对比部署和运行123456789101112131415# 部署git clone https://github.com/FudanDISC/DISC-LawLLM.gitcd DISC-LawLLMpip install -r requirements.txtmkdir ShengbinYuecd ShengbinYuegit clone https://hf-mirror.com/ShengbinYue/DISC-LawLLMcd ..mkdir cache# 遇到报错 [Errno 13] Permission denied: &#x27;/data/.cache/huggingface/modules/transformers_modules/DISC-LawLLM&#x27;export HF_HOME=&quot;~/verticalLLM/lzjr/DISC-LawLLM/cache&quot; # 如果遇到CUDA error: out of memory 用 watch -n 0.5 nvidia-smi查看显卡占...</div></div></div></a><a class="pagination-related" href="/2024/08/01/MeChat/" title="垂类心理健康咨询大模型MeChat的部署和性能对比"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-01</div><div class="info-item-2">垂类心理健康咨询大模型MeChat的部署和性能对比</div></div><div class="info-2"><div class="info-item-1">垂类心理健康咨询大模型MeChat的部署和性能对比部署和运行qiuhuachuan&#x2F;smile: SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support (github.com) 1234567891011121314git clone https://github.com/qiuhuachuan/smile.git //如果网络不行就git clont到本地再scp到服务器 data目录可以忽略掉curl --proto &#x27;=https&#x27; --tlsv1.2 -sSf https://sh.rustup.rs | sh //rust complier 然后要重新打开终端 已有rust complier即忽略这条命令RUSTUP_TOOLCHAIN=1.72.0 pip install tokenizers==0.13.2 /*执行上面这条命令，如果不这样做会报错：报错：ERROR: Could not bu...</div></div></div></a><a class="pagination-related" href="/2025/08/15/2025-08-15-ray-accelerate-trainer-lightning-pytorch/" title="ray accelerate trainer lightning pytorch"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-15</div><div class="info-item-2">ray accelerate trainer lightning pytorch</div></div><div class="info-2"><div class="info-item-1">ray、accelerate、trainer、lightning、pytorch转自：https://www.zhihu.com/question/1926849595331318550/answer/1928049512619968205 作者：CodeCrafter链接：https://www.zhihu.com/question/1926849595331318550/answer/1939450608894608104来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 2025 年，纯 PyTorch 是基本功，你必须得会，而且要熟。 它是你的内力，是你理解一切上层框架的基础。 Hugging Face Trainer 是特定领域（尤其是 NLP）的“版本答案”。 如果你就是做微调、做推理，用它，省心省力，快速出活儿。 Lightning 和 Accelerate 是“效率增强器”。 帮你把 PyTorch 代码写得更规范、更工程化，让你从繁琐的样板代码里解放出来，专注于模型本身。 Ray… 这家伙是个“大杀器”，跟前面几个不是一个维度的。 它...</div></div></div></a><a class="pagination-related" href="/2025/08/15/2025-08-15-xpu_timer/" title="xpu_timer"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-15</div><div class="info-item-2">xpu_timer</div></div><div class="info-2"><div class="info-item-1">xpu_timer转自：https://cloud.tencent.com/developer/article/2418684 背景随着大型模型的参数量从十亿量级跃升至万亿级别，其训练规模的急剧扩张不仅引发了集群成本的显著上涨，还对系统稳定性构成了挑战，尤其是机器故障的频发成为不可忽视的问题。对于大规模分布式训练任务而言，可观测性能力成为了排查故障、优化性能的关键所在。所以从事大型模型训练领域的技术人，都会不可避免地面临以下挑战：  训练过程中，性能可能会因网络、计算瓶颈等多种因素而不稳定，出现波动甚至衰退； 分布式训练是多个节点协同工作的，任一节点发生故障（无论是软件、硬件、网卡或 GPU 问题），整个训练流程均需暂停，严重影响训练效率，而且浪费宝贵的 GPU 资源。  但在实际的大模型训练过程中，这些问题是很难排查的，主要原因如下：  训练过程为同步操作，很难通过整体性能指标来排除此时哪些机器出现问题，一个机器慢可以拖慢整体训练速度； 训练性能变慢往往不是训练逻辑&#x2F;框架的问题，通常为环境导致，如果没有训练相关的监控数据，打印 timeline 实际上也没有任何作用，...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Roger-Lv</div><div class="author-info-description">Send a flare and light the way.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">101</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">112</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Roger-Lv"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Roger-Lv" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1150568956@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zhongrenjie-lv-5588a928a/" target="_blank" title="LinkedIn"><i class="iconfont icon-linkedin-fill"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E6%8C%96RLVR%E6%8E%A2%E7%B4%A2%E6%9C%BA%E5%88%B6%EF%BC%9ASFT%E4%B8%93%E6%94%BBPass-k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass-1"><span class="toc-number">1.</span> <span class="toc-text">深挖RLVR探索机制：SFT专攻Pass@k，RL强化Pass@1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E9%87%8F%E5%8C%96%E6%8E%A2%E7%B4%A2%E8%83%BD%E5%8A%9B"><span class="toc-number">1.1.</span> <span class="toc-text">一、 量化探索能力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Pass-k-%E5%BA%A6%E9%87%8F%E4%B8%8E%E2%80%9Ck-rollout-%E4%B8%8D%E5%8F%AF%E8%A7%A3%E9%97%AE%E9%A2%98%E2%80%9D"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 Pass@k 度量与“k-rollout 不可解问题”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%AD%96%E7%95%A5%E7%86%B5%E4%B8%8E%E2%80%9CRollout-%E5%88%86%E6%94%AF%E5%9B%A0%E5%AD%90%E2%80%9D"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 策略熵与“Rollout 分支因子”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%AE%9E%E9%AA%8C%E5%AF%B9%E6%AF%94%EF%BC%9ASFT-vs-RL"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 实验对比：SFT vs. RL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4 外部工具的影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-%E7%86%B5%E4%B8%8E%E6%80%A7%E8%83%BD%E7%9A%84%E7%9B%B8%E4%BA%92%E4%BD%9C%E7%94%A8"><span class="toc-number">1.2.</span> <span class="toc-text">二、 熵与性能的相互作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E9%98%B6%E6%AE%B5%E5%8A%A8%E6%80%81%EF%BC%88Stage-level-Dynamics%EF%BC%89%EF%BC%9A%E4%B8%8A%E5%8D%87%E6%9C%9F%E4%B8%8E%E5%B9%B3%E5%8F%B0%E6%9C%9F"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 阶段动态（Stage-level Dynamics）：上升期与平台期</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AE%9E%E4%BE%8B%E6%95%88%E7%8E%87%EF%BC%88Instance-level-Efficacy%EF%BC%89%EF%BC%9A%E4%BD%8E-PPL-%E6%A0%B7%E6%9C%AC%E6%9B%B4%E5%85%B3%E9%94%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 实例效率（Instance-level Efficacy）：低 PPL 样本更关键</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E8%AF%8D%E5%85%83%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%88Token-level-Significance%EF%BC%89%EF%BC%9A%E5%BA%8F%E5%88%97%E6%9C%AB%E7%AB%AF%E6%9B%B4%E5%85%B3%E9%94%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 词元重要性（Token-level Significance）：序列末端更关键</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E6%8E%A2%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">三、 探索增强的强化学习方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%8E%B0%E6%9C%89%E6%8E%A2%E7%B4%A2%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E5%9B%9E%E9%A1%BE"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 现有探索增强方法回顾</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BB%B4%E6%8C%81%E6%8E%A2%E7%B4%A2%E8%83%BD%E5%8A%9B%EF%BC%9A%E4%BF%9D%E7%95%99-Pass-k-%E7%9A%84-RFT-%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.2.</span> <span class="toc-text">4.2 维持探索能力：保留 Pass@k 的 RFT 策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%8F%90%E5%8D%87%E4%BC%98%E5%8C%96%E6%95%88%E7%8E%87%EF%BC%9A%E5%9F%BA%E4%BA%8E-PPL-%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8A%BF%E5%A1%91%E9%80%A0"><span class="toc-number">1.3.3.</span> <span class="toc-text">4.3 提升优化效率：基于 PPL 和位置的优势塑造</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%82%B9%E8%AF%84"><span class="toc-number">1.4.</span> <span class="toc-text">点评</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/" title="SFT专攻Pass@k，RL强化Pass@1?"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/RLVR.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SFT专攻Pass@k，RL强化Pass@1?"/></a><div class="content"><a class="title" href="/2025/08/21/2025-08-21-SFT%E4%B8%93%E6%94%BBPass@k%EF%BC%8CRL%E5%BC%BA%E5%8C%96Pass@1/" title="SFT专攻Pass@k，RL强化Pass@1?">SFT专攻Pass@k，RL强化Pass@1?</a><time datetime="2025-08-20T16:00:00.000Z" title="发表于 2025-08-21 00:00:00">2025-08-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/21/2025-08-21-%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A7%A3%E9%94%81SFT%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BBDFT%E5%A6%82%E4%BD%95%E5%AE%8C%E8%83%9C%E4%BC%A0%E7%BB%9F%E5%BE%AE%E8%B0%83/" title="一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调"/></a><div class="content"><a class="title" href="/2025/08/21/2025-08-21-%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A7%A3%E9%94%81SFT%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BBDFT%E5%A6%82%E4%BD%95%E5%AE%8C%E8%83%9C%E4%BC%A0%E7%BB%9F%E5%BE%AE%E8%B0%83/" title="一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调">一行代码，解锁SFT泛化能力:深度解读DFT如何完胜传统微调</a><time datetime="2025-08-20T16:00:00.000Z" title="发表于 2025-08-21 00:00:00">2025-08-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/20/2025-08-20-UloRLAn-Ultra-Long-Output-Reinforcement-Learning-Approach-for/" title="UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/LLM.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities"/></a><div class="content"><a class="title" href="/2025/08/20/2025-08-20-UloRLAn-Ultra-Long-Output-Reinforcement-Learning-Approach-for/" title="UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities">UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models’ Reasoning Abilities</a><time datetime="2025-08-19T16:00:00.000Z" title="发表于 2025-08-20 00:00:00">2025-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/18/2025-08-18-%E6%9E%81%E7%AE%80-Megatron-LM-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E5%88%87%E5%88%86%E4%BB%8B%E7%BB%8D/" title="极简 Megatron-LM 模型并行切分介绍"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/Megatron.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="极简 Megatron-LM 模型并行切分介绍"/></a><div class="content"><a class="title" href="/2025/08/18/2025-08-18-%E6%9E%81%E7%AE%80-Megatron-LM-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E5%88%87%E5%88%86%E4%BB%8B%E7%BB%8D/" title="极简 Megatron-LM 模型并行切分介绍">极简 Megatron-LM 模型并行切分介绍</a><time datetime="2025-08-17T16:00:00.000Z" title="发表于 2025-08-18 00:00:00">2025-08-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/18/2025-08-15-Camel%E6%A1%86%E6%9E%B6/" title="Camel框架"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover/camel.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Camel框架"/></a><div class="content"><a class="title" href="/2025/08/18/2025-08-15-Camel%E6%A1%86%E6%9E%B6/" title="Camel框架">Camel框架</a><time datetime="2025-08-17T16:00:00.000Z" title="发表于 2025-08-18 00:00:00">2025-08-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2024 - 2025 By Roger-Lv</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.4.2"></script><script src="/js/main.js?v=5.4.2"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const initValine = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyValine = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const valineConfig = {
      el: '#vcomment',
      appId: 'smA3tZdRGodG2VgnMubBQjLm-gzGzoHsz',
      appKey: 'biCDxj0lSBtZTMie2kNIKErd',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      visitor: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || window.location.pathname
    }

    new Valine(valineConfig)
  }

  const loadValine = async (el, path) => {
    if (typeof Valine === 'function') {
      initValine(el, path)
    } else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js')
      initValine(el, path)
    }
  }

  if (isShuoshuo) {
    'Valine' === 'Valine'
      ? window.shuoshuoComment = { loadComment: loadValine }
      : window.loadOtherComment = loadValine
    return
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><div class="aplayer no-destroy" data-id="8674547170" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.4.2"></script></div></div></body></html>