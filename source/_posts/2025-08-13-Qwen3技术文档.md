---
title: Qwen3技术报告解读
date: 2025-08-14
updated:
tags: [人工智能,强化学习,大模型]
categories: 大模型
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/Qwen.png
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:



---



# Qwen3技术报告解读

转自：https://zhuanlan.zhihu.com/p/1905926139756680880

## 模型架构

Qwen3系列，包括6个[Dense模型](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=Dense模型&zhida_source=entity)，分别是Qwen3-0.6B、Qwen3-1.7B、Qwen3-4B、Qwen3-8B、Qwen3-14B和Qwen3-32B；2个[MoE模型](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=MoE模型&zhida_source=entity)，分别是Qwen3-30B-A3B和Qwen3-235B-A22B。

Qwen3 Dense模型的架构与[Qwen2.5](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=Qwen2.5&zhida_source=entity)相似，包括[GQA](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=GQA&zhida_source=entity)、SwiGLU、[RoPE](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=RoPE&zhida_source=entity)以及RMSNorm with pre-normalization。此外，移除了Qwen2中使用的QKV偏置，并在注意力机制中引入了QK-Norm，以确保Qwen3的稳定训练。

![img](https://pic4.zhimg.com/v2-b2e68e7c0012ed96673769446067a03d_1440w.jpg)

Qwen3 MoE模型采用了细粒度专家分割，共有128个专家，激活8个专家。但与Qwen2.5-MoE不同，Qwen3-MoE去除了共享专家。同时，采用了全局批次负载平衡损失。

![img](https://pic4.zhimg.com/v2-482f8f67fddff87888398db027100cf9_1440w.jpg)

## 预训练

预训练数据共36T Tokens，包含119种语言和方言，涉及代码、STEM、推理任务、书籍、合成数据等。

其中，有部分数据是Qwen2.5-VL模型对大量PDF文档进行[OCR](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=OCR&zhida_source=entity)，再经过Qwen2.5模型进行文本优化，得到的高质量文本数据。

整个预训练分为3个阶段，

- 通用阶段：在30T Tokens上进行训练，最大长度为4096，这个阶段模型基本完成了对语言能力和以版世界知识的全面训练，训练数据覆盖119种语言。
- 推理阶段：为了提高模型推理能力，增加STEM、代码、推理和合成数据的比例，在5T Tokens上进行训练，最大长度为4096，同时在这个阶段加速了学习率的衰减。
- 长上下文阶段：通过高质量的长上下文语料库，以扩展模型的上下文长度，训练最大长度为32768。其中，4096到16384长度数据占比25%，16384到32768长度数据占比75%。与Qwen2.5相同，将RoPE的基础频率从10000增加到1000000，引入YARN和双重块注意力。

通用--->推理--->长上下文



结果这里我就截两个了，详细的结果分析，自己看论文吧。

![img](https://pica.zhimg.com/v2-3a9b188152c82db185ac282648759f86_1440w.jpg)

![img](https://picx.zhimg.com/v2-56733c1e0d4b4e8d4802252ebb47fd8d_1440w.jpg)

## 后训练

![img](https://pica.zhimg.com/v2-e76ab4e43d03cd992d65f70d84740684_1440w.jpg)

Qwen3的后训练流程有两个核心目标：

- 思考控制：融合“非思考”和“思考”两种模式，让用户更灵活地选择模型是否进行推理，并通过指定思考Tokens的预算来控制推理的深度。
- 强到弱蒸馏：利用大型模型的知识，优化小模型的后训练过程，减少小型模型的计算成本。注意：此处蒸馏，涉及数据蒸馏（离线）和传统的output logits 蒸馏（在线）。

![img](https://picx.zhimg.com/v2-341e45400be4ade0d0423327e04db461_1440w.jpg)

### CoT冷启动

CoT冷启动数据集的构建，来自广泛的数学、代码、逻辑推理和一般STEM问题。经过Query和Response两层过滤获得最终数据，

在Query过滤阶段，利用Qwen2.5-72B-Instruct识别并删除不容易验证的query，例如包含多个子问题或常见通用的问题。同时，去除Qwen2.5-72B-Instruct能够直接正确回答，而无需CoT推理的问题。最后还对每个Query进行了领域标注，以保持各领域之间的数据平衡。

在Response过滤阶段，主要利用QwQ-32B对每个问题生成N个候选回答，对于QwQ-32B一直无法生成正确答案的数据，直接进行人工标注。

对于具有正值Pass@N的问题，移除

- 最终答案不正确的
- 存在大量重复回答的
- 明显为猜测而缺乏充分推理的
- 推理内容与总结内容不一致的
- 混用语言或风格转变的
- 可能与潜在验证集过于相似的

获得冷启动数据后，直接[SFT](https://zhida.zhihu.com/search?content_id=257708481&content_type=Article&match_order=1&q=SFT&zhida_source=entity)，为模型灌输基础的推理模式，为后续RL打下基础。

### 推理强化学习

推理强化学习采用GRPO更新模型，并且采用大的Batch Size、每个Query多Rollout结果。

对于推理RL阶段的数据3995个，满足：

- 冷启动阶段未使用过的
- 对于冷启动模型来说是可学习的
- 尽可能是具有挑战性的
- 涵盖广泛的子领域

例如，Qwen3-235B-A22B模型的AIME24分数从70.1增加到85.1，总共进行了170个RL训练步骤。

### 思考模式融合

主要是将“非思考”能力融合到已经具备思考能力的模型中，允许开发者管理和控制推理行为。

为了实现这一点，对推理强化模型进行持续的SFT，并设计了一个聊天模板来融合这两种模式。同时，还发现如果模型可以熟练切换两种模式，那么也可以在不同思考预算下，保持良好的表现。

SFT数据集是结合了“思考”和“非思考”的数据。为了保证该阶段SFT不影响上一阶段的性能，“思考”数据是用第二阶段模型对第一阶段问题进行拒绝采样生成的。“非思考”数据是精心准备的代码、数学、指令遵循、多语言任务、创意写作、问答和角色扮演等数据

为了更好地融合两种模式，使用户能够动态切换模型的思考过程，Qwen3设计了聊天模板。

![img](https://pic3.zhimg.com/v2-ccae6b695c58439772761a21eebb11b8_1440w.jpg)

引入了/think和/no_think标记，对于非思考模式，返回结果会保留一个空的思考块。

当模型学会在非思考和思考模式之间切换，就可以处理基于不完整的思考生成答案，就可以让模型在思考过程中根据预算来强行停止思考过程。即 当模型的思考长度达到定义的阈值时，插入停止思考指令：“考虑到用户的时间限制，我必须根据目前的思考直接给出解决方案。\n</think>.\n\n”。并让模型继续根据其积累的推理生成最终响应。

### 通用强化学习

通用强化学习阶段主要是为了增加各种场景中的能力和稳定性，覆盖20多个任务，每个任务都有制定专门的评分标准。

- 指令遵循：保证模型可以理解和遵循用户指令，包括内容、格式、长度和结构化输出等相关要求
- 格式遵循：是否能够根据/think和/no_think标记切换思考和非思考模式，并在最终输出中正确使用规定的标记，例如，使用<think>和</think>来分隔思考和回复部分
- 偏好对齐：对于开放式查询，偏好对齐提高模型的有用性、参与度和风格
- 代理能力：训练模型通过指定的接口正确调用工具。在RL rollout过程中，允许模型与真实环境执行反馈进行完整的多轮交互，从而提高其在长期决策任务中的性能和稳定性。
- 特定场景能力：设计了针对特定上下文的任务。例如，在RAG任务中，引入奖励信号来引导模型生成准确且上下文相关的回复，减少模型幻觉。

共涉及3种奖励，基于规则的奖励、基于模型的奖励（带参考答案）和基于模型的奖励（无参考答案）。

### 强到弱蒸馏

强到弱蒸馏，针对训练小尺寸模型，5个Dense模型（Qwen3-0.6B、1.7B、4B、8B和14B）和一个MoE模型（Qwen3-30B-A3B）。

蒸馏过程分为两个主要阶段：

- 离线蒸馏策略：将教师模型在/think和/no_think模式下生成的不同的数据，用于学生模型直接SFT训练。有助于学生模型学会基本能力。
- 在线蒸馏策略：学生模型和教师模型在/think或/no_think模式下对相同的提示内容生成结果，将输出logits进行对齐，最小化KL散度。

结果这里我就截两个了，详细的结果分析，自己看论文吧。

![img](https://pica.zhimg.com/v2-0209adf01c23bc1e9d934e94936aa6a4_1440w.jpg)

![img](https://pic2.zhimg.com/v2-5d04471252aebcb297fc1b6c0ce331ed_1440w.jpg)

其实思考融合和通用强化学习，对思考模型在推理榜单上，是有一定的负优化作用，但为了保证整体通用性，Qwen3最后还是进行改操作