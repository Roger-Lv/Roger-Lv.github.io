---
title: AI资源调度
date: 2024-09-20
updated:
tags: [人工智能,容器化,虚拟化,资源调度,AI Infra]
categories: AI Infra
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/并行计算.png
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:



---



# AI资源调度

[云原生 AI 的资源调度和 AI 工作流引擎设计分享_paddleflow-CSDN博客](https://blog.csdn.net/lihui49/article/details/129260286?ops_request_misc=%7B%22request%5Fid%22%3A%2281C8FAB8-41BA-4FDC-A5E5-B7EF5F69A9D0%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=81C8FAB8-41BA-4FDC-A5E5-B7EF5F69A9D0&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-129260286-null-null.142^v100^pc_search_result_base8&utm_term=AI 资源调度&spm=1018.2226.3001.4187)

[解密英伟达NVLink：解锁多GPU计算的无限潜力_nvidia nvlink-CSDN博客](https://blog.csdn.net/njbaige/article/details/137931128?ops_request_misc=%7B%22request%5Fid%22%3A%221C80D102-64F6-4575-BCDD-C84C6BFDB679%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=1C80D102-64F6-4575-BCDD-C84C6BFDB679&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-137931128-null-null.142^v100^pc_search_result_base8&utm_term=NVLINK&spm=1018.2226.3001.4187)

[论文导读：万卡集群训练大模型（by字节跳动）-CSDN博客](https://blog.csdn.net/younger_china/article/details/136484294?ops_request_misc=%7B%22request%5Fid%22%3A%2212B1DA71-F444-49E9-8F5F-9783064D01AF%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=12B1DA71-F444-49E9-8F5F-9783064D01AF&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-136484294-null-null.142^v100^pc_search_result_base8&utm_term=Tor架构感知调度&spm=1018.2226.3001.4187)

https://arxiv.org/pdf/2402.15627.pdf

y = x+MLP(LN(x+Attention(LN(x))))  

这个公式描述的是Transformer模型中的一个自注意力机制（Self-Attention）和前馈神经网络（Feed-Forward Neural Network, MLP）的组合。Transformer模型是自然语言处理领域中广泛使用的一种模型，它在处理序列数据时非常有效。这个公式可以分解为以下几个部分：

1. **LN(x)**：这是Layer Normalization（层归一化）的缩写，它是一种归一化技术，用于稳定训练过程并提高模型性能。它对每个样本的特征进行归一化，而不是对整个批次进行归一化。

2. **Attention(LN(x))**：这是自注意力机制，它允许模型在序列的不同位置关注不同的信息。自注意力机制可以捕捉序列内部的长距离依赖关系，这对于理解文本的上下文非常重要。

3. **MLP(LN(x+Attention(LN(x))))**：这是一个前馈神经网络，它接收层归一化和自注意力机制的输出作为输入。MLP通常包含两个线性变换，中间可能有一个激活函数，如ReLU。这个MLP可以学习从自注意力机制中提取的复杂特征。

4. **y = x + MLP(LN(x+Attention(LN(x))))**：这是Transformer中的残差连接（Residual Connection）。在Transformer中，每个子层（如自注意力层和MLP层）的输出都会与子层的输入相加，然后进行层归一化。这种残差连接有助于避免在深层网络中出现的梯度消失问题，并且可以提高模型的性能。

总结来说，这个公式描述了Transformer模型中的一个关键操作，它结合了层归一化、自注意力机制和前馈神经网络，并通过残差连接来提高模型的稳定性和性能。

[一次讲清模型并行、数据并行、张量并行、流水线并行区别nn.DataParallel[分布式\]-CSDN博客](https://blog.csdn.net/sinat_37574187/article/details/140247471?ops_request_misc=%7B%22request%5Fid%22%3A%22DD298265-F461-4EEF-843D-8118FE1546A2%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=DD298265-F461-4EEF-843D-8118FE1546A2&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-140247471-null-null.142^v100^pc_search_result_base8&utm_term=张量并行和流水线并行&spm=1018.2226.3001.4187)

[【云原生 • Kubernetes】认识 k8s、k8s 架构、核心概念点介绍-CSDN博客](https://blog.csdn.net/weixin_53072519/article/details/125228115?ops_request_misc=%7B%22request%5Fid%22%3A%22D198FBD6-D7CE-4B19-81E2-EF3A930A3A20%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=D198FBD6-D7CE-4B19-81E2-EF3A930A3A20&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-125228115-null-null.142^v100^pc_search_result_base8&utm_term=k8s&spm=1018.2226.3001.4187)

[K8S详解(5万字详细教程)-CSDN博客](https://blog.csdn.net/2301_78183285/article/details/138656873?ops_request_misc=%7B%22request%5Fid%22%3A%22D198FBD6-D7CE-4B19-81E2-EF3A930A3A20%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=D198FBD6-D7CE-4B19-81E2-EF3A930A3A20&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-138656873-null-null.142^v100^pc_search_result_base8&utm_term=k8s&spm=1018.2226.3001.4187)