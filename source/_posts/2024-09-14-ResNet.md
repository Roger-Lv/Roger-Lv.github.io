---
title: ResNet
date: 2024-09-14
updated:
tags: [人工智能,ResNet]
categories: 深度学习
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/ResNet.png
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:

---

# ResNet

视频：[ResNet论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1P3411y7nn/?spm_id_from=pageDriver&vd_source=d2728007bbda9125d177004b9124afc1)

[【深度学习】ResNet网络讲解-CSDN博客](https://blog.csdn.net/weixin_44001371/article/details/134192776?ops_request_misc=%7B%22request%5Fid%22%3A%2274D3B115-BD9B-49A6-9785-A145D2673713%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=74D3B115-BD9B-49A6-9785-A145D2673713&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-134192776-null-null.142^v100^pc_search_result_base8&utm_term=ResNet&spm=1018.2226.3001.4187)

[深度学习基础学习-残差-CSDN博客](https://blog.csdn.net/m0_47146037/article/details/124299668?ops_request_misc=%7B%22request%5Fid%22%3A%22D1D58EE6-5B10-43D5-9E58-B8910DDAE24D%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=D1D58EE6-5B10-43D5-9E58-B8910DDAE24D&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-124299668-null-null.142^v100^pc_search_result_base8&utm_term=残差&spm=1018.2226.3001.4187)

[为什么要使用3×3卷积？& 1*1卷积的作用是什么？& 对ResNet结构的一些理解_3*3卷积-CSDN博客](https://blog.csdn.net/weixin_45928096/article/details/122502882?ops_request_misc=%7B%22request%5Fid%22%3A%22E3D39915-1156-46DA-A754-FE6A0785BEEC%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=E3D39915-1156-46DA-A754-FE6A0785BEEC&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-5-122502882-null-null.142^v100^pc_search_result_base8&utm_term=1*1卷积投影&spm=1018.2226.3001.4187)

[残差、方差、偏差、MSE均方误差、Bagging、Boosting、过拟合欠拟合和交叉验证-CSDN博客](https://blog.csdn.net/qq_37006625/article/details/109474441?ops_request_misc=&request_id=&biz_id=102&utm_term=残差&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-109474441.142^v100^pc_search_result_base8&spm=1018.2226.3001.4187)

[深度学习——残差网络（ResNet）原理讲解+代码（pytroch）_残差神经网络-CSDN博客](https://blog.csdn.net/m0_74055982/article/details/137927190?ops_request_misc=&request_id=&biz_id=102&utm_term=残差神经网络&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137927190.142^v100^pc_search_result_base8&spm=1018.2226.3001.4187) :star:

[快速理解卷积神经网络的输入输出尺寸问题_卷积神经网络输入和输出-CSDN博客](https://blog.csdn.net/weixin_40458355/article/details/79952601?ops_request_misc=%7B%22request%5Fid%22%3A%220E858DEC-19DA-49F1-AEEF-ABB995DE830F%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=0E858DEC-19DA-49F1-AEEF-ABB995DE830F&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-79952601-null-null.142^v100^pc_search_result_base8&utm_term=神经网络中输入输出通道不匹配&spm=1018.2226.3001.4187)

[CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/77471866)

[正态及标准正态分布-CSDN博客](https://blog.csdn.net/AlwaysSpring/article/details/121977514?ops_request_misc=%7B%22request%5Fid%22%3A%22B3A7ED75-F123-4F91-ADBD-E39E86B470C9%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=B3A7ED75-F123-4F91-ADBD-E39E86B470C9&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-121977514-null-null.142^v100^pc_search_result_base8&utm_term=标准正态分布&spm=1018.2226.3001.4187)

[[ 图像分类 \] 经典网络模型4——ResNet 详解与复现-CSDN博客](https://blog.csdn.net/weixin_45084253/article/details/124121400?ops_request_misc=&request_id=&biz_id=102&utm_term=resnet&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-124121400.142^v100^pc_search_result_base8&spm=1018.2226.3001.4187)

![image-20240914102617599.png](https://s2.loli.net/2024/09/14/VqMcyBmWaCK9UQj.png)

现象：更深的网络结构反而训练误差和测试误差都提升了！（梯度消失/梯度爆炸）

这里和overfiiting的区别是：过拟合是训练集上表现好，但测试集表现差，这里的现象是表现的都差，所以不是overfitting

加了ResNet之后，解决这个问题

Batch Normalizazion:具体的过程就是通过方法将该层的特征值分布重新拉回到**标准正态分布**(均值为0方差为1)，**特征值降落在激活函数对于输入较为敏感的区间**，输入的小变化可导致损失函数较大的变化，使得梯度变大，**避免梯度消失**，同时也可**加快收敛**。



**理解：**

可以转换为学习一个残差函数：F（x） = H（x）- x，主要F（x）= 0 就构成了一个恒等变换，而且拟合残差肯定更容易。

F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是:

F(5)′=5.1

引入残差后是：H(5)=5.1，H(5.1)=F(5)+5，F(5)=0.1

这里的F′和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如S输出从5.1变到5.2，映射的输出F′增加了2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。

至于为何shortcut（捷径）的输入是X，而不是X/2或是其他形式。作者的另一篇文章中探讨了这个问题，对以下6种结构（图2）的残差结构进行实验比较，shortcut是X/2的就是第二种，结果发现还是第一种效果好。(实验得出的结果)



输入输出：

1. 填零（zero padding）

2. 1*1卷积投影（在不增加感受野的情况下，让网络加深，进行数据的升维和降维）

   [深度学习基础学习-1x1卷积核的作用（CNN中）_1*1卷积核的作用-CSDN博客](https://blog.csdn.net/m0_47146037/article/details/127769028?ops_request_misc=&request_id=&biz_id=102&utm_term=1*1卷积投影&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-127769028.142^v100^pc_search_result_base8&spm=1018.2226.3001.4187):star

   