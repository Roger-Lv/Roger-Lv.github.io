---
title: Seq2Seq
date: 2024-09-11
updated:
tags: [人工智能,Seq2Seq,Word2Vec]
categories: 大模型
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/LLM.jpeg
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:
---

# Seq2Seq和Word2Vec

## Seq2Seq

[Seq2Seq模型介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/194308943)

## Word2Vec

[深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/114538417)

[一文搞懂One-Hot编码与词嵌入（Embedding）-百度开发者中心 (baidu.com)](https://developer.baidu.com/article/details/3267434)

[如何通俗理解Word2Vec (23年修订版)-CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/102708459) :star:

传统的one-hot 编码仅仅只是将词符号化，不包含任何语义信息。而且词的独热表示（one-hot representation）是高维的，且在高维向量中只有一个维度描述了词的语义 (高到什么程度呢？词典有多大就有多少维，一般至少上万的维度)。所以我们需要解决两个问题：

1. 需要赋予词语义信息
2. 降低维度
   

Word2Vec:最终词向量的维度(与隐含层结点数一致)一般情况下要远远小于词语总数 ![V](https://latex.csdn.net/eq?V) 的大小，所以 **Word2vec 最有价值的是**

- 让不带语义信息的词**带上了语义信息**
- 其次把词语从 one-hot encoder 形式的表示**降维到 Word2vec 形式的表示**



### CBOW（通过上下文推断某个词）

![image-20240911122510111.png](https://s2.loli.net/2024/09/11/sXQKnARJW5rSg4N.png)

1. 将上下文词进行 one-hot 表征作为输入：
   I：        [1,0,0,0]
   drink：     [0,1,0,0]
   coffee：    ？
   everyday： [0,0,0,1]
2. 然后将 one-hot 表征结果[1,0,0,0]、[**0,1,0,0**]、[0,0,0,1]，分别乘以：3×4的输入层到隐藏层的权重矩阵W「这个矩阵也叫嵌入矩阵，可以随机初始化生成」，比如可以是
   1 2 3 0
   1 2 1 2
   -1 1 1 1
3. 将得到的结果向量求平均作为隐藏层向量：[1, 1.67, 0.33]
4. 然后将隐藏层[1, 1.67, 0.33]向量乘以：4×3的隐藏层到输出层的权重矩阵W'「这个矩阵也是嵌入矩阵，也可以初始化得到」，得到输出向量：[4.01, 2.01, 5.00, 3.34]
5. 最后对输出向量[4.01, 2.01, 5.00, 3.34] 做 softmax 激活处理得到实际输出[0.23, 0.03, 0.62, 0.12]，并将其与真实标签[0, 0, 1, 0]做比较，然后基于损失函数做梯度优化训练

### 