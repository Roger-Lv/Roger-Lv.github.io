---
title: Pass@k作为reward可以有效平衡探索与利用
date: 2025-09-01
updated:
tags: [强化学习]
categories: 强化学习
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/RL.jpeg
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:







---

# Pass@k作为reward可以有效平衡探索与利用

https://zhuanlan.zhihu.com/p/1940934363450672938

这篇感觉和https://www.arxiv.org/abs/2504.13837 有点矛盾

主流的基于强化学习的推理模型训练（RLVR）方法通常使用 **Pass@1** 作为优化目标和奖励信号。这导致模型倾向于“保守”地利用已知的有效推理路径，而缺乏探索新路径的动力，容易陷入局部最优，限制了性能提升。

**论文提出的方法：Pass@k 训练** 为了解决上述问题，论文提出了一种新的训练方法——**Pass@k 训练**。

1. **核心思想：** 将广泛用于代码生成评估的 **Pass@k** 指标直接用作 RLVR 训练的奖励信号。Pass@k 衡量的是模型在 k 次尝试内至少成功一次的概率。
2. **奖励机制：** 在训练中，模型为每个问题生成 N 个候选答案。将这些答案分成若干组（每组 k 个，可通过完全采样、自助采样或解析方法构建）。如果某一组内**至少有一个答案正确**，则该组获得正向奖励（例如 1），否则为负向奖励（例如 0）。该组的奖励（或计算出的优势值）会**平均分配给组内的所有 k 个答案**。
3. 优势：
   - **鼓励探索：** 为了提高组的成功率，模型需要生成多样化的答案，覆盖不同的解题思路。
   - **容忍错误：** 即使某个答案本身是错误的，只要它与一个正确答案同组，也能获得正向激励，降低了探索的试错成本。
   - **平衡探索与利用：** 通过分组机制，模型既被鼓励去探索新路径，也会强化那些已被证明有效的路径。

**实现与优化：**

- **朴素实现（完全采样）：** 将生成的答案不重叠地分组。
- **效率优化（自助采样）：** 通过对少量答案进行有放回的重复采样来构建更多组，提高效率和稳定性。
- **极致优化（解析推导）：** 通过数学推导，直接计算出每个正确和错误答案的期望优势值，完全避免了采样和分组过程，更加高效和稳定。

**实验与发现：**

- Pass@k 训练在 Pass@k 指标上持续提升，同时也能显著提升 Pass@1 性能，超越了传统的 Pass@1 训练和一些强大的基线模型（如 Claude 3.7, GPT-4o）。
- 与简单的噪声注入或熵正则化等探索增强方法相比，Pass@k 训练效果更优。
- 实验证明，Pass@k 训练确实提升了模型生成答案的多样性和策略的熵值，即增强了探索能力。
- Pass@k 训练提升了模型在新问题上的泛化能力。
- 对超参数 k 具有鲁棒性，效率问题可通过调整学习率缓解。
- **两阶段训练法效果显著：** 先用 Pass@k 训练进行广泛探索，再切换到 Pass@1 训练进行精调利用，能最大化提升最终的 Pass@1 性能。

**深层机制分析：**

- 论文从**优势函数**（Advantage Function）的角度分析，发现 Pass@k 训练的优势函数形态使其更关注模型当前较难解决的问题，并能自动忽略已经掌握的简单问题，从而更有效地分配训练资源。
- 这引出了一个更广义的概念——**隐式奖励设计**（Implicit Reward Design），即不直接定义奖励，而是根据优化目标直接设计期望的优势函数形态。

**总结：** 该论文指出了 Pass@1 训练在 RLVR 中导致探索不足的问题，并创新性地提出 Pass@k 训练方法。通过将 Pass@k 用作奖励信号，该方法有效鼓励了模型的探索行为，平衡了探索与利用，显著提升了模型的推理能力和泛化性能。论文还深入分析了其成功机制，并提出了隐式奖励设计的新范式。
