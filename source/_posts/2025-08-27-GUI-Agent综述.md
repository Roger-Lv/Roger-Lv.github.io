---
title: GUI Agent综述
date: 2025-08-27
updated:
tags: [LLM,Agent]
categories: Agent
keywords:
description:
top_img: /img/default_top_img.jpg
comments:
cover: /img/cover/agent.png
toc:
toc_number:
toc_style_simple:
copyright:
copyright_author:
copyright_author_href:
copyright_url:
copyright_info:
mathjax:
katex:
aplayer:
highlight_shrink:
aside:
abcjs:



---

## GUI Agent综述

转自：https://zhuanlan.zhihu.com/p/5934506835

最近在基础模型，特别是[大型语言模型](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=大型语言模型&zhida_source=entity)（LLMs）和[多模态大型语言模型](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=多模态大型语言模型&zhida_source=entity)（MLLMs）方面的进步，使得智能体能够执行复杂任务。通过利用（M）LLMs处理和解释[图形用户界面](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=图形用户界面&zhida_source=entity)（GUIs）的能力，这些智能体可以模拟人类交互，如点击和打字，自主执行用户指令。本调查整合了近期关于（M）LLM基础GUI智能体的研究，突出了数据、框架和应用中的关键创新。我们首先讨论代表性的数据集和基准测试。接下来，我们总结了一个统一框架，涵盖了先前研究中使用的基本组件，并附有分类。此外，我们探索了（M）LLM基础GUI智能体的商业应用。基于现有工作，我们识别了几个关键挑战，并提出了未来的研究方向。我们希望本文能激发（M）LLM基础GUI智能体领域的进一步发展。

我们翻译解读最新论文：基于基础模型的图形用户界面智能体，文末有论文信息。

![img](https://pic2.zhimg.com/v2-13075651e969d445e64979069c2d3c8d_1440w.jpg)



## 1 引言

图形用户界面（GUIs）作为人类与数字设备之间的主要交互点。人们每天都在手机和网站上与GUIs互动，一个设计良好的GUI智能体可以显著提升用户体验。因此，关于GUI智能体的研究一直很广泛。然而，传统的基于规则和强化学习方法在需要类似人类交互的任务上挣扎（Gur et al., 2018; Liu et al., 2018），限制了它们的实际应用。近年来，大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的进步将它们在语言理解和认知处理方面的能力提升到了前所未有的水平（OpenAI et al., 2024; Touvron et al., 2023; Yang et al., 2024）。随着自然语言理解能力的提高和推理能力的增强，（M）LLM基础智能体可以有效解释和利用人类语言，制定详细计划，并执行复杂任务。这些突破为AI研究人员提供了新的机会，以应对曾经被认为非常困难的挑战，例如在GUIs中自动化任务。因此，大量研究已经发表，专注于（M）LLM基础GUI智能体，如图1所示，特别是在过去两年中。然而，很少有人努力全面总结和比较这个新兴领域的研究。迫切需要系统回顾，以提供全面理解和激发未来发展。本文提出了一项关于基础模型GUI智能体的全面调查。我们围绕三个关键领域组织调查：数据、框架和应用。首先，我们调查了GUI智能体的可用数据集和基准测试，并将它们列为该领域研究人员的资源。其次，我们回顾了近期关于（M）LLM基础GUI智能体的工作，根据它们的输入模式和学习模式进行分类。最后，我们总结了最新的工业应用，这些应用具有重要的商业潜力。



![img](https://pica.zhimg.com/v2-56a241b03b1c19c43a1c1fd0adc89032_1440w.jpg)



## 2 GUI智能体数据源

最近的研究集中在开发数据集和基准测试，以训练和评估（M）LLM基础GUI智能体的能力。这些研究可以根据它们是否涉及与实际环境的交互，大致分为两类：静态数据集和动态数据集。以下是一些数据集的介绍：

- [Android in the Wild](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=Android+in+the+Wild&zhida_source=entity) (AitW) Rawles et al. (2023) 引入了一个用于移动设备控制的数据集，包含自然语言指令、屏幕截图和任务演示。指令来自人类、语言模型和技术文档。AitW由五个数据集组成：四个用于多步骤任务（GoogleApps、Install、WebShopping、General）和一个用于单步骤任务（Single）。
- Android-In-The-Zoo Zhang et al. (2024b) 引入了一个基准数据集，包含18,643个屏幕-动作对和链式动作推理注释，旨在推进GUI导航智能体研究。
- AndroidControl Li et al. (2024a) 包含15,283个使用Android应用执行的日常任务演示，每个任务实例都附有高级别和低级别人类生成的指令。该数据集可以用于评估模型在训练数据领域内和之外的性能。
- GUI-Odyssey Lu et al. (2024) 引入了一个全面的跨应用导航智能体训练和评估的数据集。该数据集包含7,735个情节，涵盖六种跨应用任务类型、201个不同应用和1,399个应用组合。
- [UGIF](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=UGIF&zhida_source=entity) Venkatesh et al. (2023) 引入了一个全面的多语言、多模态用户界面（UI）本地化数据集，包含4,184个任务，涵盖8种语言。包括查询序列、指令、屏幕截图和人类执行的操作序列，适合多步骤UI操作评估。
- [GUI-WORLD](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=GUI-WORLD&zhida_source=entity) Chen et al. (2024a) 包含12,000多个GUI视频、6种情景类型、8个问题类别和3种格式，适合评估MLLMs在多样化GUI内容上的表现，重点关注动态和顺序元素。
- [PIXELHELP](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=PIXELHELP&zhida_source=entity) Li et al. (2020) 提出了一类新问题，专注于将自然语言指令翻译成移动用户界面上的操作。PIXELHELP引入了三个新数据集：PIXELHELP、ANDROIDHowTO和RICOSCA，总共包含187个多步骤指令，用于模型训练。
- [WebArena](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=WebArena&zhida_source=entity) Zhou et al. (2023) 实现了一个多功能网站，涵盖电子商务、社交论坛、协作软件开发和内容管理，包含812个测试示例，用于落实高级自然语言指令，当前模型如TEXT-BISON-001、GPT3.5和GPT-4的准确率为14.41%，而人类为78.24%。
- [ASSISTGUI](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=ASSISTGUI&zhida_source=entity) Gao et al. (2024) 引入了一个新基准，用于评估模型对Windows上鼠标/键盘的操作。ASSISTGUI包含来自9个软件应用的100个任务（例如，After Effects、MS Word），带有项目文件，以进行准确评估。
- AndroidWorld Rawles et al. (2024) 呈现了一个Android环境，能够为20个真实世界的Android应用中的116个程序化任务提供奖励信号。该环境动态构建任务，用自然语言表达参数，实现无限多的任务变化。
- SPA-Bench Chen et al. (2024b) 提出了一个交互式环境，旨在模拟真实世界条件，评估GUI智能体。这个环境包含340个任务，涉及系统和第三方应用，支持单一应用和跨应用场景，支持英文和中文。

## 3 （M）LLM基础GUI智能体

随着（M）LLMs类似人类的能力，GUI智能体旨在处理各种任务以满足用户需求。为了更好地激发（M）LLMs的能力，GUI智能体的框架应该精心设计。在本节中，我们首先总结了一个系统的构建，从现有工作中精心挑选了一些典型案例，并讨论了它们在不同模块的相关设计。然后，我们给出了GUI智能体的全面分类。智能体的两个关键方面，输入模式和学习模式，用于分类现有工作。从这两个维度，我们包括了当前的主要工作，并帮助新研究人员获得GUI智能体的全景视图。

### 3.1 （M）LLM基础GUI智能体构建

GUI智能体的目标是自动控制设备以完成任务。通常，GUI智能体以用户的查询和设备的用户界面状态作为输入，并提供一系列类似人类的操作来完成任务。如图2所示，我们得出结论，（M）LLM基础GUI智能体的构建包括五个部分：GUI感知器、任务规划器、决策制定者、记忆检索器和执行器。这个构建有很多变体。Wang et al. (2024a) 提出了一个多智能体GUI控制框架，包括一个规划智能体、一个决策智能体和一个反思智能体，以解决移动设备操作任务中的导航挑战，它们具有类似的功能。



![img](https://pic3.zhimg.com/v2-45601b3bc9499b86bf6c3afb0e0486f2_1440w.jpg)



- GUI感知器：为了有效完成任务，GUI智能体需要准确解释用户输入并检测设备UI的变化。
- 任务规划器：GUI智能体应该有效地分解复杂任务，通常采用思维链（CoT）方法。
- 决策制定者：决策制定者负责提供控制设备的下一个操作（s）。
- 执行器：作为GUI智能体和设备之间的链接，执行器将输出映射到相关环境。
- 记忆检索器：记忆检索器被设计为额外的信息源，以帮助智能体更有效地执行任务。

### 3.2 （M）LLM基础GUI智能体分类

如图1所示，我们从不同维度总结了现有工作。因此，本文根据输入模式和学习模式的差异对现有工作进行分类。

#### 3.2.1 不同输入模式的GUI智能体

- 基于LLM的GUI智能体：由于多模态能力有限，早期的GUI智能体（Lee et al., 2023b; Li et al., 2020; Gur et al., 2022; Jiang et al., 2023; Nakano et al., 2022）通常需要GUI感知器将GUI转换为基于文本的输入。
- 基于MLLM的GUI智能体：最近的研究（Shaw et al., 2023; Wang et al., 2021; You et al., 2024; Bai et al., 2021）利用先进的（M）LLMs的多模态能力来提高GUI理解和任务执行能力。

#### 3.2.2 不同学习模式的GUI智能体

- 基于提示的GUI智能体：提示是构建智能体的有效方法，只需额外的计算开销即可。
- 基于SFT的GUI智能体：微调允许LLM适应特定领域并更有效地执行定制任务。

## 4 工业应用（M）LLM基础GUI智能体

- Google Assistant for Android：通过说出像“Hey Google, start a run on Example App”这样的短语，用户可以使用Android的Google Assistant来启动应用、执行任务和访问内容。App Actions通过内置意图（BIIs）增强应用功能，与Google Assistant集成。这使用户能够通过语音查询导航应用并访问功能，Assistant解释这些查询以显示所需的屏幕或小部件。
- Apple Intelligence：在设备和云模型上使用Apple硅片，具有通用基础模型和针对任务（如摘要和语调调整）的专用适配器模型。评估表明，设备上的模型在性能上超越或匹配了Mistral AI、Microsoft和Google的小型模型，而服务器模型超越了OpenAI的GPT-3并与GPT-4相匹配。与ChatGPT等服务不同，Apple在具有自定义硬件的专有服务器上运行其云模型。如果检测到不匹配，系统通过拒绝连接来确保软件完整性。
- [New Bing](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=New+Bing&zhida_source=entity)：Microsoft的搜索引擎旨在为用户提供更直观、高效和全面的搜索体验。利用尖端的人工智能和机器学习技术，New Bing超越了传统的关键词搜索，以理解用户查询背后的上下文和意图。这使它能够提供更相关的结果、个性化建议和增强功能，如会话搜索、图像识别和实时更新。凭借时尚、用户友好的界面和与Microsoft其他服务的深度集成，New Bing旨在重新定义人们在线查找信息的方式，使获取他们需要的知识和洞察力更快、更容易。
- [Microsoft Copilot](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=Microsoft+Copilot&zhida_source=entity)：Microsoft 365应用中的AI工具，用于提高生产力，提供基于GPT的建议、任务自动化和内容生成。通过实时洞察增强工作流程、创造力和决策制定。
- [Anthropic Claude 3.5](https://zhida.zhihu.com/search?content_id=250209277&content_type=Article&match_order=1&q=Anthropic+Claude+3.5&zhida_source=entity)：Claude 3.5的最新版本引入了一项开创性的新功能：计算机使用，允许Claude像人类一样与计算机交互——查看屏幕、移动光标、点击按钮和打字。Asana、Canva、Cognition、DoorDash、Replit和The Browser Company已经开始探索这些可能性，执行需要数十甚至数百步完成的任务。
- AutoGLM：ChatGLM家族的新系列，旨在通过手机和网络平台上的图形用户界面自主完成任务。其Android能力使其能够自主理解用户指令，无需手动输入，使其能够处理如订购外卖、编辑评论、购物和总结文章等复杂任务。
- MagicOS 9.0 YOYO：一个高级助手，具有四个主要功能：自然语言和视觉处理、用户行为和上下文学习、意图识别和决策制定以及无缝应用集成。它了解用户习惯，以自主完成请求，例如通过语音命令订购咖啡，通过导航应用和服务。

## 5 挑战

尽管先前的工作取得了快速发展和激动人心的成就，但（M）LLM基础GUI智能体领域仍处于初期阶段。我们总结了需要解决的几个重大挑战，如下所示：

- 基准与现实之间的差距：现有的数据集和基准测试明显分为静态和动态两类。静态基准测试通常存储执行路径作为序列，目标是预测下一个动作。相比之下，动态基准测试需要在模拟器或真实设备上执行，任务必须完全完成。目前，大多数训练和评估数据都是静态的。然而，由于（M）LLM基础GUI智能体需要解释广泛的环境状态，现有的数据集和基准测试对于实际应用来说是不够的。
- GUI智能体自我进化：自我进化旨在实现GUI智能体的自我闭环。
- 推理效率：人类对GUI的响应时间很敏感。通常，200毫秒以下的延迟是可以接受的，但超过这个阈值的延迟会迅速降低用户体验。对于当前的GUI智能体，推理和通信延迟通常以秒为单位，导致用户满意度差。因此，解决如何最小化这些延迟，或直接在移动设备上部署（M）LLM，是一个紧迫的问题。

## 6 结论

在本文中，我们系统地回顾了（M）LLM基础GUI智能体这一快速发展的研究领域。我们从三个主要角度审视这些研究：数据源、构建和应用。我们还提供了一个详细的分类，将现有研究联系起来，并总结了主要技术。此外，我们提出了几个挑战和潜在的未来方向，用于利用基础模型的GUI智能体。



![img](https://pic4.zhimg.com/v2-ad35c590dcda2d0cb410f3c9f96da439_1440w.jpg)



参考资料

标题：GUI Agents with Foundation Models: A Comprehensive Survey

作者：Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, Ruiming Tang

单位：Huawei Noah’s Ark Lab

标签：人工智能、机器学习、自然语言处理、多模态学习、图形用户界面、智能体

概述：这篇文章是一项关于基于基础模型的图形用户界面智能体的综合调查，涵盖了数据、框架、应用和未来研究方向。

链接：[https://arxiv.org/abs/2411.0489](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2411.04890)